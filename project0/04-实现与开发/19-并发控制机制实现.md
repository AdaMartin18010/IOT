# 并发控制机制实现

## 目录

- [并发控制机制实现](#并发控制机制实现)
  - [目录](#目录)
  - [概述](#概述)
  - [核心实现](#核心实现)
    - [1. 高性能锁机制](#1-高性能锁机制)
    - [2. 无锁数据结构](#2-无锁数据结构)
    - [3. 并发限流机制](#3-并发限流机制)
    - [4. 死锁检测](#4-死锁检测)
  - [配置管理](#配置管理)
  - [性能测试](#性能测试)
  - [部署配置](#部署配置)
    - [Docker](#docker)
  - [总结](#总结)

## 概述

IoT系统中的并发控制包含多种锁机制、无锁数据结构、并发限流、死锁检测等关键技术。

## 核心实现

### 1. 高性能锁机制

```rust
use std::sync::{Arc, Mutex, RwLock};
use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};
use std::time::{Duration, Instant};
use tokio::sync::{Semaphore, RwLock as AsyncRwLock, Mutex as AsyncMutex};
use std::collections::HashMap;

// 自旋锁实现
pub struct SpinLock {
    locked: AtomicBool,
}

impl SpinLock {
    pub fn new() -> Self {
        Self {
            locked: AtomicBool::new(false),
        }
    }

    pub fn lock(&self) -> SpinLockGuard {
        while self.locked.compare_exchange_weak(
            false, true, Ordering::Acquire, Ordering::Relaxed
        ).is_err() {
            // 使用hint避免busy-waiting对CPU缓存的影响
            std::hint::spin_loop();
        }
        SpinLockGuard { lock: self }
    }

    pub fn try_lock(&self) -> Option<SpinLockGuard> {
        if self.locked.compare_exchange(
            false, true, Ordering::Acquire, Ordering::Relaxed
        ).is_ok() {
            Some(SpinLockGuard { lock: self })
        } else {
            None
        }
    }

    fn unlock(&self) {
        self.locked.store(false, Ordering::Release);
    }
}

pub struct SpinLockGuard<'a> {
    lock: &'a SpinLock,
}

impl<'a> Drop for SpinLockGuard<'a> {
    fn drop(&mut self) {
        self.lock.unlock();
    }
}

// 读写自旋锁
pub struct RwSpinLock {
    readers: AtomicUsize,
    writer: AtomicBool,
}

impl RwSpinLock {
    pub fn new() -> Self {
        Self {
            readers: AtomicUsize::new(0),
            writer: AtomicBool::new(false),
        }
    }

    pub fn read(&self) -> RwSpinLockReadGuard {
        loop {
            // 等待写锁释放
            while self.writer.load(Ordering::Acquire) {
                std::hint::spin_loop();
            }

            // 尝试获取读锁
            let current = self.readers.load(Ordering::Acquire);
            if self.readers.compare_exchange_weak(
                current, current + 1, Ordering::AcqRel, Ordering::Relaxed
            ).is_ok() {
                // 再次检查是否有写锁
                if !self.writer.load(Ordering::Acquire) {
                    return RwSpinLockReadGuard { lock: self };
                } else {
                    // 写锁已获取，释放读锁
                    self.readers.fetch_sub(1, Ordering::Release);
                }
            }
        }
    }

    pub fn write(&self) -> RwSpinLockWriteGuard {
        // 获取写锁
        while self.writer.compare_exchange_weak(
            false, true, Ordering::Acquire, Ordering::Relaxed
        ).is_err() {
            std::hint::spin_loop();
        }

        // 等待所有读锁释放
        while self.readers.load(Ordering::Acquire) > 0 {
            std::hint::spin_loop();
        }

        RwSpinLockWriteGuard { lock: self }
    }

    fn unlock_read(&self) {
        self.readers.fetch_sub(1, Ordering::Release);
    }

    fn unlock_write(&self) {
        self.writer.store(false, Ordering::Release);
    }
}

pub struct RwSpinLockReadGuard<'a> {
    lock: &'a RwSpinLock,
}

impl<'a> Drop for RwSpinLockReadGuard<'a> {
    fn drop(&mut self) {
        self.lock.unlock_read();
    }
}

pub struct RwSpinLockWriteGuard<'a> {
    lock: &'a RwSpinLock,
}

impl<'a> Drop for RwSpinLockWriteGuard<'a> {
    fn drop(&mut self) {
        self.lock.unlock_write();
    }
}

// 分段锁实现
pub struct SegmentedLock<T> {
    segments: Vec<RwLock<HashMap<String, T>>>,
    segment_count: usize,
}

impl<T> SegmentedLock<T> {
    pub fn new(segment_count: usize) -> Self {
        let mut segments = Vec::with_capacity(segment_count);
        for _ in 0..segment_count {
            segments.push(RwLock::new(HashMap::new()));
        }

        Self {
            segments,
            segment_count,
        }
    }

    fn get_segment_index(&self, key: &str) -> usize {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        let mut hasher = DefaultHasher::new();
        key.hash(&mut hasher);
        (hasher.finish() as usize) % self.segment_count
    }

    pub fn get(&self, key: &str) -> Option<T> 
    where 
        T: Clone 
    {
        let index = self.get_segment_index(key);
        let segment = self.segments[index].read().unwrap();
        segment.get(key).cloned()
    }

    pub fn insert(&self, key: String, value: T) -> Option<T> {
        let index = self.get_segment_index(&key);
        let mut segment = self.segments[index].write().unwrap();
        segment.insert(key, value)
    }

    pub fn remove(&self, key: &str) -> Option<T> {
        let index = self.get_segment_index(key);
        let mut segment = self.segments[index].write().unwrap();
        segment.remove(key)
    }

    pub fn len(&self) -> usize {
        self.segments.iter()
            .map(|segment| segment.read().unwrap().len())
            .sum()
    }
}
```

### 2. 无锁数据结构

```rust
use std::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};
use std::ptr;

// 无锁队列实现
pub struct LockFreeQueue<T> {
    head: AtomicPtr<Node<T>>,
    tail: AtomicPtr<Node<T>>,
}

struct Node<T> {
    data: Option<T>,
    next: AtomicPtr<Node<T>>,
}

impl<T> Node<T> {
    fn new(data: Option<T>) -> Box<Node<T>> {
        Box::new(Node {
            data,
            next: AtomicPtr::new(ptr::null_mut()),
        })
    }
}

impl<T> LockFreeQueue<T> {
    pub fn new() -> Self {
        let dummy = Box::into_raw(Node::new(None));
        Self {
            head: AtomicPtr::new(dummy),
            tail: AtomicPtr::new(dummy),
        }
    }

    pub fn enqueue(&self, data: T) {
        let new_node = Box::into_raw(Node::new(Some(data)));
        
        loop {
            let tail = self.tail.load(Ordering::Acquire);
            let next = unsafe { (*tail).next.load(Ordering::Acquire) };
            
            if tail == self.tail.load(Ordering::Acquire) {
                if next.is_null() {
                    // 尝试链接新节点
                    if unsafe { (*tail).next.compare_exchange_weak(
                        next, new_node, Ordering::Release, Ordering::Relaxed
                    ).is_ok() } {
                        // 成功链接，更新tail
                        let _ = self.tail.compare_exchange_weak(
                            tail, new_node, Ordering::Release, Ordering::Relaxed
                        );
                        break;
                    }
                } else {
                    // 帮助其他线程更新tail
                    let _ = self.tail.compare_exchange_weak(
                        tail, next, Ordering::Release, Ordering::Relaxed
                    );
                }
            }
        }
    }

    pub fn dequeue(&self) -> Option<T> {
        loop {
            let head = self.head.load(Ordering::Acquire);
            let tail = self.tail.load(Ordering::Acquire);
            let next = unsafe { (*head).next.load(Ordering::Acquire) };
            
            if head == self.head.load(Ordering::Acquire) {
                if head == tail {
                    if next.is_null() {
                        // 队列为空
                        return None;
                    }
                    // 帮助其他线程更新tail
                    let _ = self.tail.compare_exchange_weak(
                        tail, next, Ordering::Release, Ordering::Relaxed
                    );
                } else {
                    if next.is_null() {
                        continue;
                    }
                    
                    // 读取数据
                    let data = unsafe { (*next).data.take() };
                    
                    // 更新head
                    if self.head.compare_exchange_weak(
                        head, next, Ordering::Release, Ordering::Relaxed
                    ).is_ok() {
                        unsafe {
                            Box::from_raw(head);
                        }
                        return data;
                    }
                }
            }
        }
    }
}

unsafe impl<T: Send> Send for LockFreeQueue<T> {}
unsafe impl<T: Send> Sync for LockFreeQueue<T> {}

// 无锁哈希表
pub struct LockFreeHashMap<K, V> {
    buckets: Vec<AtomicPtr<HashNode<K, V>>>,
    size: AtomicUsize,
    capacity: usize,
}

struct HashNode<K, V> {
    key: K,
    value: V,
    hash: u64,
    next: AtomicPtr<HashNode<K, V>>,
}

impl<K, V> LockFreeHashMap<K, V> 
where 
    K: std::hash::Hash + Eq + Clone,
    V: Clone,
{
    pub fn new(capacity: usize) -> Self {
        let mut buckets = Vec::with_capacity(capacity);
        for _ in 0..capacity {
            buckets.push(AtomicPtr::new(ptr::null_mut()));
        }

        Self {
            buckets,
            size: AtomicUsize::new(0),
            capacity,
        }
    }

    fn hash(&self, key: &K) -> u64 {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        let mut hasher = DefaultHasher::new();
        key.hash(&mut hasher);
        hasher.finish()
    }

    fn get_bucket_index(&self, hash: u64) -> usize {
        (hash as usize) % self.capacity
    }

    pub fn insert(&self, key: K, value: V) -> Option<V> {
        let hash = self.hash(&key);
        let bucket_index = self.get_bucket_index(hash);
        
        let new_node = Box::into_raw(Box::new(HashNode {
            key: key.clone(),
            value: value.clone(),
            hash,
            next: AtomicPtr::new(ptr::null_mut()),
        }));

        loop {
            let head = self.buckets[bucket_index].load(Ordering::Acquire);
            
            // 检查是否已存在相同的key
            let mut current = head;
            while !current.is_null() {
                let node = unsafe { &*current };
                if node.hash == hash && node.key == key {
                    // 更新现有节点的值
                    let old_value = node.value.clone();
                    // 注意：这里应该使用原子操作更新值
                    // 简化实现，实际需要更复杂的CAS操作
                    unsafe {
                        Box::from_raw(new_node);
                    }
                    return Some(old_value);
                }
                current = node.next.load(Ordering::Acquire);
            }

            // 链接新节点到链表头部
            unsafe {
                (*new_node).next.store(head, Ordering::Relaxed);
            }
            
            if self.buckets[bucket_index].compare_exchange_weak(
                head, new_node, Ordering::Release, Ordering::Relaxed
            ).is_ok() {
                self.size.fetch_add(1, Ordering::Relaxed);
                return None;
            }
        }
    }

    pub fn get(&self, key: &K) -> Option<V> {
        let hash = self.hash(key);
        let bucket_index = self.get_bucket_index(hash);
        
        let mut current = self.buckets[bucket_index].load(Ordering::Acquire);
        
        while !current.is_null() {
            let node = unsafe { &*current };
            if node.hash == hash && node.key == *key {
                return Some(node.value.clone());
            }
            current = node.next.load(Ordering::Acquire);
        }
        
        None
    }

    pub fn size(&self) -> usize {
        self.size.load(Ordering::Relaxed)
    }
}

unsafe impl<K: Send, V: Send> Send for LockFreeHashMap<K, V> {}
unsafe impl<K: Send, V: Send> Sync for LockFreeHashMap<K, V> {}
```

### 3. 并发限流机制

```rust
use std::time::{Duration, Instant};
use tokio::time::{interval, Interval};

// 令牌桶限流器
pub struct TokenBucket {
    tokens: AtomicUsize,
    capacity: usize,
    refill_rate: usize, // tokens per second
    last_refill: AtomicUsize, // timestamp in milliseconds
}

impl TokenBucket {
    pub fn new(capacity: usize, refill_rate: usize) -> Self {
        Self {
            tokens: AtomicUsize::new(capacity),
            capacity,
            refill_rate,
            last_refill: AtomicUsize::new(
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap()
                    .as_millis() as usize
            ),
        }
    }

    pub fn try_acquire(&self, tokens: usize) -> bool {
        self.refill();
        
        loop {
            let current_tokens = self.tokens.load(Ordering::Acquire);
            if current_tokens >= tokens {
                if self.tokens.compare_exchange_weak(
                    current_tokens, 
                    current_tokens - tokens, 
                    Ordering::Release, 
                    Ordering::Relaxed
                ).is_ok() {
                    return true;
                }
            } else {
                return false;
            }
        }
    }

    pub async fn acquire(&self, tokens: usize) {
        while !self.try_acquire(tokens) {
            tokio::time::sleep(Duration::from_millis(10)).await;
        }
    }

    fn refill(&self) {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_millis() as usize;

        let last = self.last_refill.load(Ordering::Acquire);
        let elapsed = now.saturating_sub(last);
        
        if elapsed >= 1000 { // 至少1秒
            let tokens_to_add = (elapsed / 1000) * self.refill_rate;
            
            if self.last_refill.compare_exchange_weak(
                last, now, Ordering::Release, Ordering::Relaxed
            ).is_ok() {
                loop {
                    let current = self.tokens.load(Ordering::Acquire);
                    let new_tokens = std::cmp::min(current + tokens_to_add, self.capacity);
                    
                    if self.tokens.compare_exchange_weak(
                        current, new_tokens, Ordering::Release, Ordering::Relaxed
                    ).is_ok() {
                        break;
                    }
                }
            }
        }
    }
}

// 滑动窗口限流器
pub struct SlidingWindowLimiter {
    window_size: Duration,
    max_requests: usize,
    requests: AsyncMutex<Vec<Instant>>,
}

impl SlidingWindowLimiter {
    pub fn new(window_size: Duration, max_requests: usize) -> Self {
        Self {
            window_size,
            max_requests,
            requests: AsyncMutex::new(Vec::new()),
        }
    }

    pub async fn try_acquire(&self) -> bool {
        let now = Instant::now();
        let mut requests = self.requests.lock().await;
        
        // 清理过期请求
        let cutoff = now - self.window_size;
        requests.retain(|&timestamp| timestamp > cutoff);
        
        if requests.len() < self.max_requests {
            requests.push(now);
            true
        } else {
            false
        }
    }

    pub async fn acquire(&self) {
        while !self.try_acquire().await {
            tokio::time::sleep(Duration::from_millis(100)).await;
        }
    }
}

// 漏桶限流器
pub struct LeakyBucket {
    capacity: usize,
    leak_rate: usize, // requests per second
    current_level: AtomicUsize,
    last_leak: AtomicUsize, // timestamp in milliseconds
}

impl LeakyBucket {
    pub fn new(capacity: usize, leak_rate: usize) -> Self {
        Self {
            capacity,
            leak_rate,
            current_level: AtomicUsize::new(0),
            last_leak: AtomicUsize::new(
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap()
                    .as_millis() as usize
            ),
        }
    }

    pub fn try_acquire(&self) -> bool {
        self.leak();
        
        loop {
            let current = self.current_level.load(Ordering::Acquire);
            if current < self.capacity {
                if self.current_level.compare_exchange_weak(
                    current, current + 1, Ordering::Release, Ordering::Relaxed
                ).is_ok() {
                    return true;
                }
            } else {
                return false;
            }
        }
    }

    fn leak(&self) {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_millis() as usize;

        let last = self.last_leak.load(Ordering::Acquire);
        let elapsed = now.saturating_sub(last);
        
        if elapsed >= 1000 { // 至少1秒
            let leaks = (elapsed / 1000) * self.leak_rate;
            
            if self.last_leak.compare_exchange_weak(
                last, now, Ordering::Release, Ordering::Relaxed
            ).is_ok() {
                loop {
                    let current = self.current_level.load(Ordering::Acquire);
                    let new_level = current.saturating_sub(leaks);
                    
                    if self.current_level.compare_exchange_weak(
                        current, new_level, Ordering::Release, Ordering::Relaxed
                    ).is_ok() {
                        break;
                    }
                }
            }
        }
    }
}
```

### 4. 死锁检测

```rust
use std::collections::{HashMap, HashSet, VecDeque};
use std::thread::ThreadId;

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct LockId(pub String);

#[derive(Debug, Clone)]
pub struct LockInfo {
    pub id: LockId,
    pub holder: Option<ThreadId>,
    pub waiters: HashSet<ThreadId>,
}

pub struct DeadlockDetector {
    locks: Arc<RwLock<HashMap<LockId, LockInfo>>>,
    wait_for_graph: Arc<RwLock<HashMap<ThreadId, HashSet<ThreadId>>>>,
}

impl DeadlockDetector {
    pub fn new() -> Self {
        Self {
            locks: Arc::new(RwLock::new(HashMap::new())),
            wait_for_graph: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    pub fn register_lock(&self, lock_id: LockId) {
        let mut locks = self.locks.write().unwrap();
        locks.insert(lock_id.clone(), LockInfo {
            id: lock_id,
            holder: None,
            waiters: HashSet::new(),
        });
    }

    pub fn acquire_lock(&self, lock_id: &LockId, thread_id: ThreadId) -> Result<(), DeadlockError> {
        let mut locks = self.locks.write().unwrap();
        let mut wait_for = self.wait_for_graph.write().unwrap();

        if let Some(lock_info) = locks.get_mut(lock_id) {
            if lock_info.holder.is_none() {
                // 锁可用，直接获取
                lock_info.holder = Some(thread_id);
                wait_for.remove(&thread_id); // 移除等待关系
                Ok(())
            } else {
                // 锁被占用，检查死锁
                let holder = lock_info.holder.unwrap();
                
                // 添加等待关系
                wait_for.entry(thread_id).or_insert_with(HashSet::new).insert(holder);
                lock_info.waiters.insert(thread_id);

                // 检查是否形成死锁
                if self.has_cycle(&wait_for, thread_id) {
                    // 清理等待关系
                    wait_for.get_mut(&thread_id).unwrap().remove(&holder);
                    lock_info.waiters.remove(&thread_id);
                    Err(DeadlockError::DeadlockDetected)
                } else {
                    Err(DeadlockError::WouldBlock)
                }
            }
        } else {
            Err(DeadlockError::LockNotFound)
        }
    }

    pub fn release_lock(&self, lock_id: &LockId, thread_id: ThreadId) {
        let mut locks = self.locks.write().unwrap();
        let mut wait_for = self.wait_for_graph.write().unwrap();

        if let Some(lock_info) = locks.get_mut(lock_id) {
            if lock_info.holder == Some(thread_id) {
                lock_info.holder = None;
                
                // 清理等待图中相关的等待关系
                for waiter in &lock_info.waiters {
                    if let Some(waits) = wait_for.get_mut(waiter) {
                        waits.remove(&thread_id);
                    }
                }
                lock_info.waiters.clear();
            }
        }
    }

    fn has_cycle(&self, graph: &HashMap<ThreadId, HashSet<ThreadId>>, start: ThreadId) -> bool {
        let mut visited = HashSet::new();
        let mut rec_stack = HashSet::new();
        self.dfs_cycle_check(graph, start, &mut visited, &mut rec_stack)
    }

    fn dfs_cycle_check(
        &self,
        graph: &HashMap<ThreadId, HashSet<ThreadId>>,
        node: ThreadId,
        visited: &mut HashSet<ThreadId>,
        rec_stack: &mut HashSet<ThreadId>,
    ) -> bool {
        visited.insert(node);
        rec_stack.insert(node);

        if let Some(neighbors) = graph.get(&node) {
            for &neighbor in neighbors {
                if !visited.contains(&neighbor) {
                    if self.dfs_cycle_check(graph, neighbor, visited, rec_stack) {
                        return true;
                    }
                } else if rec_stack.contains(&neighbor) {
                    return true; // 找到环
                }
            }
        }

        rec_stack.remove(&node);
        false
    }

    pub fn get_deadlock_info(&self) -> Vec<Vec<ThreadId>> {
        let graph = self.wait_for_graph.read().unwrap();
        let mut cycles = Vec::new();
        let mut visited = HashSet::new();

        for &thread_id in graph.keys() {
            if !visited.contains(&thread_id) {
                if let Some(cycle) = self.find_cycle(&graph, thread_id, &mut visited) {
                    cycles.push(cycle);
                }
            }
        }

        cycles
    }

    fn find_cycle(
        &self,
        graph: &HashMap<ThreadId, HashSet<ThreadId>>,
        start: ThreadId,
        visited: &mut HashSet<ThreadId>,
    ) -> Option<Vec<ThreadId>> {
        let mut path = Vec::new();
        let mut current = start;
        let mut path_set = HashSet::new();

        loop {
            if path_set.contains(&current) {
                // 找到环，提取环路
                let cycle_start = path.iter().position(|&x| x == current).unwrap();
                return Some(path[cycle_start..].to_vec());
            }

            if visited.contains(&current) {
                break;
            }

            visited.insert(current);
            path.push(current);
            path_set.insert(current);

            if let Some(neighbors) = graph.get(&current) {
                if let Some(&next) = neighbors.iter().next() {
                    current = next;
                } else {
                    break;
                }
            } else {
                break;
            }
        }

        None
    }
}

#[derive(Debug, thiserror::Error)]
pub enum DeadlockError {
    #[error("Deadlock detected")]
    DeadlockDetected,
    #[error("Would block")]
    WouldBlock,
    #[error("Lock not found")]
    LockNotFound,
}
```

## 配置管理

```toml
[concurrency]
max_threads = 32
stack_size_kb = 8192
enable_deadlock_detection = true

[locks]
spin_lock_enabled = true
rw_lock_segments = 16
lock_timeout_ms = 5000

[rate_limiting]
token_bucket_capacity = 1000
token_bucket_refill_rate = 100
sliding_window_size_ms = 60000
sliding_window_max_requests = 1000

[monitoring]
enable_lock_metrics = true
enable_deadlock_alerts = true
```

## 性能测试

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::Arc;
    use std::thread;

    #[test]
    fn test_spin_lock_performance() {
        let lock = Arc::new(SpinLock::new());
        let counter = Arc::new(AtomicUsize::new(0));
        let mut handles = vec![];

        for _ in 0..10 {
            let lock = Arc::clone(&lock);
            let counter = Arc::clone(&counter);
            
            let handle = thread::spawn(move || {
                for _ in 0..1000 {
                    let _guard = lock.lock();
                    counter.fetch_add(1, Ordering::Relaxed);
                }
            });
            
            handles.push(handle);
        }

        for handle in handles {
            handle.join().unwrap();
        }

        assert_eq!(counter.load(Ordering::Relaxed), 10000);
    }

    #[test]
    fn test_lock_free_queue() {
        let queue = Arc::new(LockFreeQueue::new());
        let mut handles = vec![];

        // 生产者线程
        for i in 0..5 {
            let queue = Arc::clone(&queue);
            let handle = thread::spawn(move || {
                for j in 0..100 {
                    queue.enqueue(i * 100 + j);
                }
            });
            handles.push(handle);
        }

        // 消费者线程
        let results = Arc::new(AtomicUsize::new(0));
        for _ in 0..3 {
            let queue = Arc::clone(&queue);
            let results = Arc::clone(&results);
            let handle = thread::spawn(move || {
                loop {
                    if let Some(_) = queue.dequeue() {
                        results.fetch_add(1, Ordering::Relaxed);
                    } else {
                        thread::sleep(Duration::from_millis(1));
                    }
                    
                    if results.load(Ordering::Relaxed) >= 500 {
                        break;
                    }
                }
            });
            handles.push(handle);
        }

        for handle in handles {
            handle.join().unwrap();
        }

        assert_eq!(results.load(Ordering::Relaxed), 500);
    }

    #[tokio::test]
    async fn test_token_bucket() {
        let bucket = TokenBucket::new(10, 5);
        
        // 初始应该有10个令牌
        assert!(bucket.try_acquire(10));
        assert!(!bucket.try_acquire(1));
        
        // 等待令牌补充
        tokio::time::sleep(Duration::from_secs(2)).await;
        assert!(bucket.try_acquire(5));
    }
}
```

## 部署配置

### Docker

```dockerfile
FROM rust:1.70-alpine AS builder
WORKDIR /app
COPY . .
RUN cargo build --release

FROM alpine:latest
WORKDIR /root/
COPY --from=builder /app/target/release/concurrency_system ./
COPY config/concurrency.toml ./config/
EXPOSE 8080
CMD ["./concurrency_system"]
```

## 总结

本并发控制机制实现提供了：

- 高性能自旋锁和读写锁
- 分段锁减少锁竞争
- 无锁数据结构（队列、哈希表）
- 多种限流算法（令牌桶、滑动窗口、漏桶）
- 死锁检测和预防
- 完整的性能测试和监控
