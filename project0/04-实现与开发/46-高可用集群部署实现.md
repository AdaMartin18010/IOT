# IoT语义互操作高可用集群部署实现

## 1. 高可用架构设计

### 1.1 多层高可用架构
```yaml
# k8s/cluster-architecture.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ha-cluster-config
data:
  architecture.yml: |
    high_availability:
      layers:
        infrastructure:
          kubernetes_masters: 3  # 奇数节点保证选举
          worker_nodes: 6        # 最少2个AZ，每AZ至少2节点
          storage_replicas: 3    # 分布式存储3副本
          network_redundancy: true
        
        application:
          semantic_gateway:
            replicas: 5
            anti_affinity: true
            pdb_min_available: 3
          protocol_adapters:
            replicas: 6
            distribution: even_across_az
          cache_cluster:
            replicas: 3
            persistence: true
        
        data:
          postgres_cluster: 3    # 主从复制
          redis_cluster: 6       # 3主3从
          elasticsearch: 3       # 3节点集群
```

### 1.2 集群拓扑定义
```rust
// src/cluster/topology.rs
use std::collections::HashMap;
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ClusterTopology {
    pub availability_zones: Vec<AvailabilityZone>,
    pub node_distribution: NodeDistribution,
    pub network_policies: Vec<NetworkPolicy>,
    pub failure_domains: Vec<FailureDomain>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AvailabilityZone {
    pub name: String,
    pub region: String,
    pub master_nodes: Vec<NodeSpec>,
    pub worker_nodes: Vec<NodeSpec>,
    pub storage_nodes: Vec<NodeSpec>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NodeSpec {
    pub name: String,
    pub cpu_cores: u32,
    pub memory_gb: u32,
    pub storage_gb: u32,
    pub network_bandwidth: String,
    pub labels: HashMap<String, String>,
    pub taints: Vec<NodeTaint>,
}

#[derive(Debug, Clone)]
pub struct ClusterManager {
    topology: ClusterTopology,
    health_checker: HealthChecker,
    failover_controller: FailoverController,
}

impl ClusterManager {
    pub async fn new() -> Result<Self, ClusterError> {
        let topology = Self::load_topology().await?;
        let health_checker = HealthChecker::new(&topology).await?;
        let failover_controller = FailoverController::new(&topology).await?;
        
        Ok(Self {
            topology,
            health_checker,
            failover_controller,
        })
    }
    
    pub async fn ensure_high_availability(&self) -> Result<(), ClusterError> {
        // 检查集群健康状态
        let health_status = self.health_checker.check_cluster_health().await?;
        
        if !health_status.is_healthy() {
            // 触发故障转移
            self.failover_controller.handle_failures(health_status.failures).await?;
        }
        
        // 确保服务分布符合高可用要求
        self.ensure_service_distribution().await?;
        
        Ok(())
    }
    
    async fn ensure_service_distribution(&self) -> Result<(), ClusterError> {
        for service in &self.topology.services {
            let current_distribution = self.get_service_distribution(service).await?;
            let required_distribution = self.calculate_required_distribution(service);
            
            if !self.is_distribution_compliant(&current_distribution, &required_distribution) {
                self.rebalance_service(service, &required_distribution).await?;
            }
        }
        
        Ok(())
    }
}
```

## 2. Kubernetes集群部署

### 2.1 集群初始化配置
```yaml
# k8s/cluster-init/kubeadm-config.yml
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: "10.0.1.10"
  bindPort: 6443
nodeRegistration:
  criSocket: "unix:///var/run/containerd/containerd.sock"
  kubeletExtraArgs:
    node-labels: "node-role.kubernetes.io/master="
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: "v1.28.2"
controlPlaneEndpoint: "k8s-api.iot-cluster.local:6443"
networking:
  serviceSubnet: "10.96.0.0/16"
  podSubnet: "10.244.0.0/16"
  dnsDomain: "cluster.local"
etcd:
  external:
    endpoints:
      - "https://etcd-1.iot-cluster.local:2379"
      - "https://etcd-2.iot-cluster.local:2379"
      - "https://etcd-3.iot-cluster.local:2379"
    caFile: "/etc/etcd/ca.crt"
    certFile: "/etc/etcd/client.crt"
    keyFile: "/etc/etcd/client.key"
apiServer:
  certSANs:
    - "k8s-api.iot-cluster.local"
    - "10.0.1.10"
    - "10.0.1.11"
    - "10.0.1.12"
  extraArgs:
    audit-log-maxage: "30"
    audit-log-maxbackup: "10"
    audit-log-maxsize: "100"
    audit-log-path: "/var/log/audit.log"
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigningRequestApproval,CertificateSubjectRestriction,DefaultIngressClass,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    bind-address: "0.0.0.0"
    secure-port: "10257"
    port: "0"
scheduler:
  extraArgs:
    bind-address: "0.0.0.0"
    secure-port: "10259"
    port: "0"
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
serverTLSBootstrap: true
rotateCertificates: true
```

### 2.2 高可用etcd集群
```yaml
# k8s/etcd/etcd-cluster.yml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: etcd
  namespace: kube-system
spec:
  serviceName: etcd
  replicas: 3
  selector:
    matchLabels:
      app: etcd
  template:
    metadata:
      labels:
        app: etcd
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: etcd
            topologyKey: kubernetes.io/hostname
      containers:
      - name: etcd
        image: k8s.gcr.io/etcd:3.5.9-0
        command:
        - etcd
        - --name=$(POD_NAME)
        - --data-dir=/var/lib/etcd
        - --initial-cluster-state=new
        - --initial-cluster-token=etcd-cluster
        - --initial-cluster=etcd-0=https://etcd-0.etcd:2380,etcd-1=https://etcd-1.etcd:2380,etcd-2=https://etcd-2.etcd:2380
        - --listen-peer-urls=https://0.0.0.0:2380
        - --listen-client-urls=https://0.0.0.0:2379
        - --advertise-client-urls=https://$(POD_NAME).etcd:2379
        - --initial-advertise-peer-urls=https://$(POD_NAME).etcd:2380
        - --client-cert-auth
        - --trusted-ca-file=/etc/etcd/ca.crt
        - --cert-file=/etc/etcd/server.crt
        - --key-file=/etc/etcd/server.key
        - --peer-client-cert-auth
        - --peer-trusted-ca-file=/etc/etcd/ca.crt
        - --peer-cert-file=/etc/etcd/peer.crt
        - --peer-key-file=/etc/etcd/peer.key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        ports:
        - containerPort: 2379
          name: client
        - containerPort: 2380
          name: peer
        volumeMounts:
        - name: etcd-data
          mountPath: /var/lib/etcd
        - name: etcd-certs
          mountPath: /etc/etcd
          readOnly: true
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi
        livenessProbe:
          exec:
            command:
            - etcdctl
            - --cacert=/etc/etcd/ca.crt
            - --cert=/etc/etcd/server.crt
            - --key=/etc/etcd/server.key
            - endpoint
            - health
          initialDelaySeconds: 30
          periodSeconds: 10
      volumes:
      - name: etcd-certs
        secret:
          secretName: etcd-certs
  volumeClaimTemplates:
  - metadata:
      name: etcd-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: "fast-ssd"
      resources:
        requests:
          storage: 50Gi
```

## 3. 服务高可用部署

### 3.1 语义网关高可用部署
```yaml
# k8s/services/semantic-gateway-ha.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: semantic-gateway
  namespace: iot-system
spec:
  replicas: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 2
  selector:
    matchLabels:
      app: semantic-gateway
  template:
    metadata:
      labels:
        app: semantic-gateway
        version: v1
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: semantic-gateway
              topologyKey: kubernetes.io/hostname
          - weight: 50
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: semantic-gateway
              topologyKey: topology.kubernetes.io/zone
      containers:
      - name: semantic-gateway
        image: iot-registry/semantic-gateway:v1.2.0
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: metrics
        env:
        - name: RUST_LOG
          value: "info"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: url
        - name: REDIS_CLUSTER_ENDPOINTS
          value: "redis-cluster:6379"
        - name: ENABLE_METRICS
          value: "true"
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]
      terminationGracePeriodSeconds: 30
      nodeSelector:
        node-type: "compute"
      tolerations:
      - key: "compute-dedicated"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: semantic-gateway-pdb
  namespace: iot-system
spec:
  minAvailable: 3
  selector:
    matchLabels:
      app: semantic-gateway
---
apiVersion: v1
kind: Service
metadata:
  name: semantic-gateway
  namespace: iot-system
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "tcp"
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: semantic-gateway
  sessionAffinity: None
```

### 3.2 数据库高可用集群
```yaml
# k8s/databases/postgres-ha.yml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres-cluster
  namespace: iot-system
spec:
  instances: 3
  
  postgresql:
    parameters:
      max_connections: "200"
      shared_buffers: "256MB"
      effective_cache_size: "1GB"
      maintenance_work_mem: "64MB"
      checkpoint_completion_target: "0.9"
      wal_buffers: "16MB"
      default_statistics_target: "100"
      random_page_cost: "1.1"
      effective_io_concurrency: "200"
      work_mem: "4MB"
      min_wal_size: "1GB"
      max_wal_size: "4GB"
      max_worker_processes: "8"
      max_parallel_workers_per_gather: "4"
      max_parallel_workers: "8"
      max_parallel_maintenance_workers: "4"
  
  bootstrap:
    initdb:
      database: iot_semantic
      owner: iot_user
      secret:
        name: postgres-credentials
  
  storage:
    size: 100Gi
    storageClass: fast-ssd
  
  monitoring:
    enabled: true
    
  backup:
    barmanObjectStore:
      destinationPath: "s3://iot-backups/postgres"
      s3Credentials:
        accessKeyId:
          name: backup-credentials
          key: ACCESS_KEY_ID
        secretAccessKey:
          name: backup-credentials
          key: SECRET_ACCESS_KEY
      wal:
        retention: "7d"
      data:
        retention: "30d"
        
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            cnpg.io/cluster: postgres-cluster
        topologyKey: kubernetes.io/hostname
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-cluster-rw
  namespace: iot-system
spec:
  type: ClusterIP
  ports:
  - port: 5432
    targetPort: 5432
    protocol: TCP
  selector:
    cnpg.io/cluster: postgres-cluster
    role: primary
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-cluster-ro
  namespace: iot-system
spec:
  type: ClusterIP
  ports:
  - port: 5432
    targetPort: 5432
    protocol: TCP
  selector:
    cnpg.io/cluster: postgres-cluster
    role: replica
```

## 4. 负载均衡与服务发现

### 4.1 Istio服务网格配置
```yaml
# k8s/istio/gateway.yml
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: iot-gateway
  namespace: iot-system
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - api.iot-system.local
    tls:
      httpsRedirect: true
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: iot-tls-cert
    hosts:
    - api.iot-system.local
---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: semantic-gateway-vs
  namespace: iot-system
spec:
  hosts:
  - api.iot-system.local
  gateways:
  - iot-gateway
  http:
  - match:
    - uri:
        prefix: /api/v1/semantic
    route:
    - destination:
        host: semantic-gateway
        port:
          number: 80
      weight: 100
    fault:
      delay:
        percentage:
          value: 0.1
        fixedDelay: 5s
    retries:
      attempts: 3
      perTryTimeout: 10s
      retryOn: "5xx,reset,connect-failure,refused-stream"
    timeout: 30s
  - match:
    - uri:
        prefix: /api/v1/protocols
    route:
    - destination:
        host: protocol-adapter
        port:
          number: 80
    circuitBreaker:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: semantic-gateway-dr
  namespace: iot-system
spec:
  host: semantic-gateway
  trafficPolicy:
    loadBalancer:
      simple: LEAST_CONN
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        http2MaxRequests: 100
        maxRequestsPerConnection: 10
        maxRetries: 3
        consecutiveGatewayErrors: 5
        interval: 30s
        baseEjectionTime: 30s
  subsets:
  - name: v1
    labels:
      version: v1
    trafficPolicy:
      portLevelSettings:
      - port:
          number: 80
        loadBalancer:
          consistentHash:
            httpHeaderName: "x-device-id"
```

### 4.2 自定义负载均衡算法
```rust
// src/load_balancer/semantic_aware_lb.rs
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;

#[derive(Debug, Clone)]
pub struct SemanticAwareLoadBalancer {
    backends: Arc<RwLock<Vec<Backend>>>,
    protocol_affinity: Arc<RwLock<HashMap<String, Vec<String>>>>,
    performance_metrics: Arc<RwLock<HashMap<String, PerformanceMetrics>>>,
}

#[derive(Debug, Clone)]
pub struct Backend {
    pub id: String,
    pub endpoint: String,
    pub weight: u32,
    pub health_status: HealthStatus,
    pub supported_protocols: Vec<String>,
    pub current_load: f64,
    pub semantic_cache_hits: u64,
}

#[derive(Debug, Clone)]
pub struct PerformanceMetrics {
    pub avg_response_time: f64,
    pub request_count: u64,
    pub error_rate: f64,
    pub semantic_accuracy: f64,
}

impl SemanticAwareLoadBalancer {
    pub async fn new() -> Self {
        Self {
            backends: Arc::new(RwLock::new(Vec::new())),
            protocol_affinity: Arc::new(RwLock::new(HashMap::new())),
            performance_metrics: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    pub async fn select_backend(&self, request: &SemanticRequest) -> Option<Backend> {
        let backends = self.backends.read().await;
        let metrics = self.performance_metrics.read().await;
        
        // 过滤健康的后端
        let healthy_backends: Vec<&Backend> = backends
            .iter()
            .filter(|b| b.health_status == HealthStatus::Healthy)
            .collect();
        
        if healthy_backends.is_empty() {
            return None;
        }
        
        // 协议亲和性选择
        let protocol_compatible: Vec<&Backend> = healthy_backends
            .iter()
            .filter(|b| b.supported_protocols.contains(&request.protocol))
            .copied()
            .collect();
        
        let candidates = if protocol_compatible.is_empty() {
            &healthy_backends
        } else {
            &protocol_compatible
        };
        
        // 计算最佳后端
        let mut best_backend = None;
        let mut best_score = f64::MIN;
        
        for backend in candidates {
            let score = self.calculate_backend_score(backend, request, &metrics).await;
            if score > best_score {
                best_score = score;
                best_backend = Some((*backend).clone());
            }
        }
        
        best_backend
    }
    
    async fn calculate_backend_score(
        &self,
        backend: &Backend,
        request: &SemanticRequest,
        metrics: &HashMap<String, PerformanceMetrics>,
    ) -> f64 {
        let base_score = match metrics.get(&backend.id) {
            Some(m) => {
                // 响应时间权重 (40%)
                let response_time_score = 1.0 / (1.0 + m.avg_response_time / 1000.0);
                
                // 错误率权重 (30%)
                let error_rate_score = 1.0 - m.error_rate;
                
                // 语义准确性权重 (20%)
                let semantic_score = m.semantic_accuracy;
                
                // 当前负载权重 (10%)
                let load_score = 1.0 - backend.current_load;
                
                response_time_score * 0.4 
                    + error_rate_score * 0.3 
                    + semantic_score * 0.2 
                    + load_score * 0.1
            }
            None => 0.5, // 新后端给予中等分数
        };
        
        // 语义缓存命中率加成
        let cache_bonus = if backend.semantic_cache_hits > 0 {
            0.1 * (backend.semantic_cache_hits as f64).log10()
        } else {
            0.0
        };
        
        // 协议特化加成
        let protocol_bonus = if backend.supported_protocols.contains(&request.protocol) {
            0.2
        } else {
            0.0
        };
        
        base_score + cache_bonus + protocol_bonus
    }
    
    pub async fn update_backend_metrics(&self, backend_id: &str, metrics: PerformanceMetrics) {
        let mut performance_metrics = self.performance_metrics.write().await;
        performance_metrics.insert(backend_id.to_string(), metrics);
    }
    
    pub async fn handle_backend_failure(&self, backend_id: &str) {
        let mut backends = self.backends.write().await;
        if let Some(backend) = backends.iter_mut().find(|b| b.id == backend_id) {
            backend.health_status = HealthStatus::Unhealthy;
            backend.current_load = 0.0;
        }
    }
}
```

## 5. 故障转移与自愈机制

### 5.1 故障检测与恢复控制器
```go
// pkg/controllers/failover_controller.go
package controllers

import (
    "context"
    "time"
    
    "k8s.io/client-go/kubernetes"
    "sigs.k8s.io/controller-runtime/pkg/controller"
    "sigs.k8s.io/controller-runtime/pkg/handler"
    "sigs.k8s.io/controller-runtime/pkg/source"
)

type FailoverController struct {
    client          kubernetes.Interface
    healthChecker   *HealthChecker
    alertManager    *AlertManager
    recoveryActions map[string]RecoveryAction
}

type FailureEvent struct {
    Timestamp   time.Time       `json:"timestamp"`
    Type        FailureType     `json:"type"`
    Severity    SeverityLevel   `json:"severity"`
    Component   string          `json:"component"`
    Details     string          `json:"details"`
    AffectedPods []string       `json:"affected_pods"`
}

type RecoveryAction interface {
    Execute(ctx context.Context, event FailureEvent) error
    CanHandle(event FailureEvent) bool
    Priority() int
}

func (fc *FailoverController) Start(ctx context.Context) error {
    // 启动健康检查监控
    go fc.healthChecker.StartMonitoring(ctx)
    
    // 启动故障事件处理循环
    go fc.processFailureEvents(ctx)
    
    // 启动自愈机制
    go fc.startSelfHealingLoop(ctx)
    
    return nil
}

func (fc *FailoverController) processFailureEvents(ctx context.Context) {
    eventChan := fc.healthChecker.GetFailureEvents()
    
    for {
        select {
        case event := <-eventChan:
            if err := fc.handleFailureEvent(ctx, event); err != nil {
                fc.alertManager.SendAlert(Alert{
                    Level:   "ERROR",
                    Message: fmt.Sprintf("Failed to handle failure event: %v", err),
                    Event:   event,
                })
            }
        case <-ctx.Done():
            return
        }
    }
}

func (fc *FailoverController) handleFailureEvent(ctx context.Context, event FailureEvent) error {
    // 记录故障事件
    log.Printf("Handling failure event: %+v", event)
    
    // 找到合适的恢复动作
    var applicableActions []RecoveryAction
    for _, action := range fc.recoveryActions {
        if action.CanHandle(event) {
            applicableActions = append(applicableActions, action)
        }
    }
    
    // 按优先级排序
    sort.Slice(applicableActions, func(i, j int) bool {
        return applicableActions[i].Priority() > applicableActions[j].Priority()
    })
    
    // 执行恢复动作
    for _, action := range applicableActions {
        if err := action.Execute(ctx, event); err != nil {
            log.Printf("Recovery action failed: %v", err)
            continue
        }
        
        // 验证恢复效果
        if fc.verifyRecovery(ctx, event) {
            log.Printf("Recovery successful for event: %s", event.Component)
            return nil
        }
    }
    
    // 所有恢复动作都失败，发送高级别告警
    fc.alertManager.SendCriticalAlert(Alert{
        Level:   "CRITICAL",
        Message: "All recovery actions failed",
        Event:   event,
    })
    
    return fmt.Errorf("all recovery actions failed for event: %+v", event)
}

// Pod重启恢复动作
type PodRestartAction struct {
    client kubernetes.Interface
}

func (pra *PodRestartAction) Execute(ctx context.Context, event FailureEvent) error {
    for _, podName := range event.AffectedPods {
        if err := pra.restartPod(ctx, podName); err != nil {
            return fmt.Errorf("failed to restart pod %s: %w", podName, err)
        }
    }
    return nil
}

func (pra *PodRestartAction) CanHandle(event FailureEvent) bool {
    return event.Type == FailureTypePodCrash || 
           event.Type == FailureTypePodUnresponsive
}

func (pra *PodRestartAction) Priority() int {
    return 100 // 高优先级
}

// 服务扩容恢复动作
type ScaleUpAction struct {
    client kubernetes.Interface
}

func (sua *ScaleUpAction) Execute(ctx context.Context, event FailureEvent) error {
    deployment := sua.getDeploymentForComponent(event.Component)
    if deployment == nil {
        return fmt.Errorf("no deployment found for component: %s", event.Component)
    }
    
    currentReplicas := *deployment.Spec.Replicas
    newReplicas := currentReplicas + 2 // 增加2个副本
    
    deployment.Spec.Replicas = &newReplicas
    
    _, err := sua.client.AppsV1().
        Deployments(deployment.Namespace).
        Update(ctx, deployment, metav1.UpdateOptions{})
    
    return err
}

func (sua *ScaleUpAction) CanHandle(event FailureEvent) bool {
    return event.Type == FailureTypeHighLoad || 
           event.Type == FailureTypeServiceDegraded
}

func (sua *ScaleUpAction) Priority() int {
    return 80 // 中等优先级
}
```

### 5.2 自愈机制实现
```python
# pkg/healing/self_healing.py
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from dataclasses import dataclass
from kubernetes import client, config

@dataclass
class HealingRule:
    name: str
    condition: str
    action: str
    cooldown: int  # seconds
    max_attempts: int
    enabled: bool = True

class SelfHealingEngine:
    def __init__(self):
        config.load_incluster_config()
        self.k8s_client = client.AppsV1Api()
        self.core_client = client.CoreV1Api()
        self.healing_rules = self.load_healing_rules()
        self.action_history = {}
        self.logger = logging.getLogger(__name__)
    
    async def start_healing_loop(self):
        """启动自愈循环"""
        while True:
            try:
                await self.execute_healing_cycle()
            except Exception as e:
                self.logger.error(f"Healing cycle failed: {e}")
            
            await asyncio.sleep(30)  # 每30秒执行一次
    
    async def execute_healing_cycle(self):
        """执行一次完整的自愈周期"""
        # 收集系统指标
        system_metrics = await self.collect_system_metrics()
        
        # 评估每个治愈规则
        for rule in self.healing_rules:
            if not rule.enabled:
                continue
            
            if await self.evaluate_condition(rule.condition, system_metrics):
                await self.execute_healing_action(rule)
    
    async def collect_system_metrics(self) -> Dict:
        """收集系统指标"""
        metrics = {
            'pods': await self.get_pod_metrics(),
            'services': await self.get_service_metrics(),
            'nodes': await self.get_node_metrics(),
            'resources': await self.get_resource_metrics()
        }
        return metrics
    
    async def get_pod_metrics(self) -> Dict:
        """获取Pod指标"""
        pods = self.core_client.list_namespaced_pod(namespace="iot-system")
        
        pod_metrics = {
            'total': len(pods.items),
            'running': 0,
            'failed': 0,
            'pending': 0,
            'restart_count': 0
        }
        
        for pod in pods.items:
            if pod.status.phase == 'Running':
                pod_metrics['running'] += 1
            elif pod.status.phase == 'Failed':
                pod_metrics['failed'] += 1
            elif pod.status.phase == 'Pending':
                pod_metrics['pending'] += 1
            
            # 统计重启次数
            if pod.status.container_statuses:
                for container in pod.status.container_statuses:
                    pod_metrics['restart_count'] += container.restart_count
        
        return pod_metrics
    
    async def evaluate_condition(self, condition: str, metrics: Dict) -> bool:
        """评估治愈条件"""
        try:
            # 简单的条件表达式评估
            # 例如: "pods.failed > 0 and pods.failed / pods.total > 0.1"
            return eval(condition, {"__builtins__": {}}, metrics)
        except Exception as e:
            self.logger.warning(f"Failed to evaluate condition '{condition}': {e}")
            return False
    
    async def execute_healing_action(self, rule: HealingRule):
        """执行治愈动作"""
        # 检查冷却时间
        if not self.is_action_allowed(rule):
            return
        
        self.logger.info(f"Executing healing action: {rule.name}")
        
        try:
            if rule.action == "restart_failed_pods":
                await self.restart_failed_pods()
            elif rule.action == "scale_up_semantic_gateway":
                await self.scale_up_deployment("semantic-gateway", 2)
            elif rule.action == "clear_memory_pressure":
                await self.handle_memory_pressure()
            elif rule.action == "restart_unhealthy_services":
                await self.restart_unhealthy_services()
            
            # 记录执行历史
            self.record_action_execution(rule)
            
        except Exception as e:
            self.logger.error(f"Healing action '{rule.name}' failed: {e}")
    
    def is_action_allowed(self, rule: HealingRule) -> bool:
        """检查动作是否允许执行（冷却时间和最大尝试次数）"""
        action_key = rule.name
        current_time = datetime.now()
        
        if action_key not in self.action_history:
            return True
        
        history = self.action_history[action_key]
        
        # 检查冷却时间
        if current_time - history['last_execution'] < timedelta(seconds=rule.cooldown):
            return False
        
        # 检查最大尝试次数（在1小时内）
        recent_attempts = [
            t for t in history['attempts'] 
            if current_time - t < timedelta(hours=1)
        ]
        
        return len(recent_attempts) < rule.max_attempts
    
    async def restart_failed_pods(self):
        """重启失败的Pod"""
        pods = self.core_client.list_namespaced_pod(
            namespace="iot-system",
            field_selector="status.phase=Failed"
        )
        
        for pod in pods.items:
            self.logger.info(f"Restarting failed pod: {pod.metadata.name}")
            await self.core_client.delete_namespaced_pod(
                name=pod.metadata.name,
                namespace="iot-system"
            )
    
    async def scale_up_deployment(self, deployment_name: str, additional_replicas: int):
        """扩容部署"""
        deployment = self.k8s_client.read_namespaced_deployment(
            name=deployment_name,
            namespace="iot-system"
        )
        
        current_replicas = deployment.spec.replicas
        new_replicas = current_replicas + additional_replicas
        
        deployment.spec.replicas = new_replicas
        
        self.k8s_client.patch_namespaced_deployment(
            name=deployment_name,
            namespace="iot-system",
            body=deployment
        )
        
        self.logger.info(f"Scaled up {deployment_name} from {current_replicas} to {new_replicas}")
    
    def load_healing_rules(self) -> List[HealingRule]:
        """加载自愈规则"""
        return [
            HealingRule(
                name="restart_failed_pods",
                condition="pods['failed'] > 0",
                action="restart_failed_pods",
                cooldown=300,  # 5分钟
                max_attempts=3
            ),
            HealingRule(
                name="scale_up_on_high_load",
                condition="pods['running'] / pods['total'] < 0.8 and pods['restart_count'] > 10",
                action="scale_up_semantic_gateway",
                cooldown=600,  # 10分钟
                max_attempts=2
            ),
            HealingRule(
                name="memory_pressure_relief",
                condition="resources['memory_usage'] > 0.9",
                action="clear_memory_pressure",
                cooldown=900,  # 15分钟
                max_attempts=1
            )
        ]
```

## 6. 监控与告警

### 6.1 Prometheus监控配置
```yaml
# k8s/monitoring/prometheus-config.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "/etc/prometheus/rules/*.yml"
    
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - alertmanager:9093
    
    scrape_configs:
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https
      
      - job_name: 'semantic-gateway'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_label_app]
          action: keep
          regex: semantic-gateway
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
      
      - job_name: 'protocol-adapters'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_label_component]
          action: keep
          regex: protocol-adapter
  
  rules.yml: |
    groups:
    - name: iot-system-alerts
      rules:
      - alert: SemanticGatewayDown
        expr: up{job="semantic-gateway"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Semantic Gateway is down"
          description: "Semantic Gateway has been down for more than 1 minute"
      
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} for the last 5 minutes"
      
      - alert: HighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected"
          description: "95th percentile latency is {{ $value }}s for the last 10 minutes"
      
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod is crash looping"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is crash looping"
```

这个高可用集群部署实现提供了：

1. **多层高可用架构** - 从基础设施到应用层的完整高可用保证
2. **Kubernetes集群部署** - 生产级别的K8s集群配置和etcd高可用
3. **服务高可用部署** - 语义网关和数据库的高可用部署策略
4. **智能负载均衡** - 语义感知的负载均衡算法
5. **故障转移机制** - 自动故障检测和恢复控制器
6. **自愈系统** - 基于规则的自动修复机制
7. **监控告警** - 全面的Prometheus监控和告警体系

通过这套系统，可以确保IoT语义互操作平台的高可用性和可靠性。 