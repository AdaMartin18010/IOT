# 生产部署与运维自动化实现

## 1. 容器化部署系统

### 1.1 Docker容器编排

```rust
// src/deployment/mod.rs
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use serde::{Deserialize, Serialize};
use docker_api::{Docker, Container, ContainerCreateOptions};

#[derive(Debug)]
pub struct ContainerOrchestrator {
    docker_client: Docker,
    service_registry: Arc<RwLock<HashMap<String, ServiceDefinition>>>,
    deployment_manager: DeploymentManager,
    health_checker: HealthChecker,
    load_balancer: LoadBalancer,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ServiceDefinition {
    pub service_name: String,
    pub image: String,
    pub version: String,
    pub replicas: u32,
    pub resources: ResourceRequirements,
    pub environment: HashMap<String, String>,
    pub ports: Vec<PortMapping>,
    pub volumes: Vec<VolumeMapping>,
    pub health_check: HealthCheckConfig,
    pub scaling_policy: ScalingPolicy,
}

#[derive(Debug, Clone)]
pub struct ResourceRequirements {
    pub cpu_cores: f64,
    pub memory_mb: u64,
    pub storage_gb: u64,
    pub gpu_count: Option<u32>,
}

#[derive(Debug, Clone)]
pub struct PortMapping {
    pub container_port: u16,
    pub host_port: u16,
    pub protocol: String,
}

#[derive(Debug, Clone)]
pub struct VolumeMapping {
    pub host_path: String,
    pub container_path: String,
    pub read_only: bool,
}

#[derive(Debug, Clone)]
pub struct HealthCheckConfig {
    pub endpoint: String,
    pub interval: u64,
    pub timeout: u64,
    pub retries: u32,
    pub start_period: u64,
}

#[derive(Debug, Clone)]
pub struct ScalingPolicy {
    pub min_replicas: u32,
    pub max_replicas: u32,
    pub target_cpu_utilization: f64,
    pub target_memory_utilization: f64,
    pub scale_up_cooldown: u64,
    pub scale_down_cooldown: u64,
}

impl ContainerOrchestrator {
    pub async fn deploy_service(
        &mut self,
        service_def: ServiceDefinition,
    ) -> Result<DeploymentResult, DeploymentError> {
        // 验证服务定义
        self.validate_service_definition(&service_def).await?;
        
        // 拉取镜像
        self.pull_image(&service_def.image).await?;
        
        // 创建网络
        let network_id = self.create_network(&service_def.service_name).await?;
        
        // 部署容器
        let mut containers = Vec::new();
        for i in 0..service_def.replicas {
            let container = self.create_container(&service_def, i, &network_id).await?;
            containers.push(container);
        }
        
        // 启动容器
        for container in &containers {
            self.start_container(container).await?;
        }
        
        // 配置负载均衡
        self.configure_load_balancer(&service_def, &containers).await?;
        
        // 启动健康检查
        self.start_health_checks(&service_def, &containers).await?;
        
        // 注册服务
        self.service_registry.write().await.insert(
            service_def.service_name.clone(),
            service_def,
        );
        
        Ok(DeploymentResult {
            service_name: service_def.service_name,
            containers,
            network_id,
            status: DeploymentStatus::Running,
        })
    }
    
    async fn create_container(
        &self,
        service_def: &ServiceDefinition,
        replica_index: u32,
        network_id: &str,
    ) -> Result<Container, DeploymentError> {
        let container_name = format!("{}-{}", service_def.service_name, replica_index);
        
        // 构建容器创建选项
        let mut create_opts = ContainerCreateOptions::new(&service_def.image);
        create_opts.name(&container_name);
        
        // 设置环境变量
        for (key, value) in &service_def.environment {
            create_opts.env(format!("{}={}", key, value));
        }
        
        // 设置端口映射
        for port_mapping in &service_def.ports {
            create_opts.port_bindings(format!("{}:{}/{}", 
                port_mapping.host_port, 
                port_mapping.container_port, 
                port_mapping.protocol
            ));
        }
        
        // 设置卷映射
        for volume_mapping in &service_def.volumes {
            create_opts.volume_bindings(format!("{}:{}:{}", 
                volume_mapping.host_path,
                volume_mapping.container_path,
                if volume_mapping.read_only { "ro" } else { "rw" }
            ));
        }
        
        // 设置资源限制
        create_opts.memory(service_def.resources.memory_mb * 1024 * 1024);
        create_opts.cpu_quota((service_def.resources.cpu_cores * 100000) as i64);
        create_opts.cpu_period(100000);
        
        // 设置网络
        create_opts.network_mode(network_id);
        
        // 创建容器
        let container = self.docker_client.containers().create(&create_opts).await?;
        
        Ok(container)
    }
    
    async fn start_container(&self, container: &Container) -> Result<(), DeploymentError> {
        container.start().await?;
        
        // 等待容器启动
        tokio::time::sleep(std::time::Duration::from_secs(5)).await;
        
        // 检查容器状态
        let info = container.inspect().await?;
        if !info.state.running.unwrap_or(false) {
            return Err(DeploymentError::ContainerStartFailed);
        }
        
        Ok(())
    }
    
    async fn configure_load_balancer(
        &self,
        service_def: &ServiceDefinition,
        containers: &[Container],
    ) -> Result<(), DeploymentError> {
        let mut endpoints = Vec::new();
        
        for container in containers {
            let info = container.inspect().await?;
            if let Some(network_settings) = info.network_settings {
                if let Some(ip_address) = network_settings.ip_address {
                    for port_mapping in &service_def.ports {
                        endpoints.push(format!("{}:{}", ip_address, port_mapping.container_port));
                    }
                }
            }
        }
        
        self.load_balancer.add_service(&service_def.service_name, endpoints).await?;
        
        Ok(())
    }
    
    async fn start_health_checks(
        &self,
        service_def: &ServiceDefinition,
        containers: &[Container],
    ) -> Result<(), DeploymentError> {
        for container in containers {
            let health_check = HealthCheck {
                container_id: container.id().clone(),
                config: service_def.health_check.clone(),
                service_name: service_def.service_name.clone(),
            };
            
            self.health_checker.add_health_check(health_check).await?;
        }
        
        Ok(())
    }
}
```

### 1.2 Kubernetes部署

```rust
#[derive(Debug)]
pub struct KubernetesDeployer {
    k8s_client: k8s_openapi::api::core::v1::Pod,
    namespace_manager: NamespaceManager,
    service_manager: ServiceManager,
    ingress_manager: IngressManager,
    config_map_manager: ConfigMapManager,
}

#[derive(Debug, Clone)]
pub struct K8sDeployment {
    pub name: String,
    pub namespace: String,
    pub replicas: u32,
    pub containers: Vec<ContainerSpec>,
    pub service: ServiceSpec,
    pub ingress: Option<IngressSpec>,
    pub config_maps: Vec<ConfigMapSpec>,
}

#[derive(Debug, Clone)]
pub struct ContainerSpec {
    pub name: String,
    pub image: String,
    pub ports: Vec<ContainerPort>,
    pub env: Vec<EnvVar>,
    pub resources: ResourceRequirements,
    pub volume_mounts: Vec<VolumeMount>,
}

#[derive(Debug, Clone)]
pub struct ServiceSpec {
    pub name: String,
    pub service_type: ServiceType,
    pub ports: Vec<ServicePort>,
    pub selector: HashMap<String, String>,
}

#[derive(Debug, Clone)]
pub enum ServiceType {
    ClusterIP,
    NodePort,
    LoadBalancer,
}

impl KubernetesDeployer {
    pub async fn deploy_application(
        &mut self,
        deployment: K8sDeployment,
    ) -> Result<K8sDeploymentResult, DeploymentError> {
        // 创建命名空间
        self.namespace_manager.create_namespace(&deployment.namespace).await?;
        
        // 创建ConfigMaps
        for config_map in &deployment.config_maps {
            self.config_map_manager.create_config_map(config_map).await?;
        }
        
        // 创建Deployment
        let deployment_result = self.create_deployment(&deployment).await?;
        
        // 创建Service
        let service_result = self.service_manager.create_service(&deployment.service).await?;
        
        // 创建Ingress（如果指定）
        let ingress_result = if let Some(ingress) = &deployment.ingress {
            Some(self.ingress_manager.create_ingress(ingress).await?)
        } else {
            None
        };
        
        // 等待部署完成
        self.wait_for_deployment_ready(&deployment.name, &deployment.namespace).await?;
        
        Ok(K8sDeploymentResult {
            deployment: deployment_result,
            service: service_result,
            ingress: ingress_result,
            status: DeploymentStatus::Running,
        })
    }
    
    async fn create_deployment(
        &self,
        deployment: &K8sDeployment,
    ) -> Result<k8s_openapi::api::apps::v1::Deployment, DeploymentError> {
        let deployment_spec = k8s_openapi::api::apps::v1::Deployment {
            metadata: k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                name: Some(deployment.name.clone()),
                namespace: Some(deployment.namespace.clone()),
                ..Default::default()
            },
            spec: Some(k8s_openapi::api::apps::v1::DeploymentSpec {
                replicas: Some(deployment.replicas as i32),
                selector: k8s_openapi::apimachinery::pkg::apis::meta::v1::LabelSelector {
                    match_labels: Some(HashMap::from([
                        ("app".to_string(), deployment.name.clone()),
                    ])),
                    ..Default::default()
                },
                template: k8s_openapi::api::core::v1::PodTemplateSpec {
                    metadata: Some(k8s_openapi::apimachinery::pkg::apis::meta::v1::ObjectMeta {
                        labels: Some(HashMap::from([
                            ("app".to_string(), deployment.name.clone()),
                        ])),
                        ..Default::default()
                    }),
                    spec: Some(k8s_openapi::api::core::v1::PodSpec {
                        containers: deployment.containers.iter().map(|container| {
                            k8s_openapi::api::core::v1::Container {
                                name: container.name.clone(),
                                image: Some(container.image.clone()),
                                ports: Some(container.ports.iter().map(|port| {
                                    k8s_openapi::api::core::v1::ContainerPort {
                                        container_port: port.port as i32,
                                        protocol: Some(port.protocol.clone()),
                                        ..Default::default()
                                    }
                                }).collect()),
                                env: Some(container.env.iter().map(|env| {
                                    k8s_openapi::api::core::v1::EnvVar {
                                        name: env.name.clone(),
                                        value: Some(env.value.clone()),
                                        ..Default::default()
                                    }
                                }).collect()),
                                resources: Some(k8s_openapi::api::core::v1::ResourceRequirements {
                                    requests: Some(HashMap::from([
                                        ("cpu".to_string(), format!("{}", container.resources.cpu_cores)),
                                        ("memory".to_string(), format!("{}Mi", container.resources.memory_mb)),
                                    ])),
                                    limits: Some(HashMap::from([
                                        ("cpu".to_string(), format!("{}", container.resources.cpu_cores)),
                                        ("memory".to_string(), format!("{}Mi", container.resources.memory_mb)),
                                    ])),
                                    ..Default::default()
                                }),
                                ..Default::default()
                            }
                        }).collect(),
                        ..Default::default()
                    }),
                },
                ..Default::default()
            }),
            ..Default::default()
        };
        
        // 创建Deployment
        let deployment = self.k8s_client.create(&deployment_spec).await?;
        
        Ok(deployment)
    }
    
    async fn wait_for_deployment_ready(
        &self,
        deployment_name: &str,
        namespace: &str,
    ) -> Result<(), DeploymentError> {
        let mut attempts = 0;
        let max_attempts = 60; // 5分钟超时
        
        while attempts < max_attempts {
            let deployment = self.k8s_client.get(deployment_name, namespace).await?;
            
            if let Some(status) = deployment.status {
                if let Some(ready_replicas) = status.ready_replicas {
                    if let Some(replicas) = status.replicas {
                        if ready_replicas == replicas {
                            return Ok(());
                        }
                    }
                }
            }
            
            tokio::time::sleep(std::time::Duration::from_secs(5)).await;
            attempts += 1;
        }
        
        Err(DeploymentError::DeploymentTimeout)
    }
}
```

## 2. 自动化运维系统

### 2.1 监控与告警

```rust
#[derive(Debug)]
pub struct MonitoringSystem {
    metrics_collector: MetricsCollector,
    alert_manager: AlertManager,
    dashboard_manager: DashboardManager,
    log_aggregator: LogAggregator,
}

#[derive(Debug, Clone)]
pub struct Metric {
    pub name: String,
    pub value: f64,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub labels: HashMap<String, String>,
    pub metric_type: MetricType,
}

#[derive(Debug, Clone)]
pub enum MetricType {
    Counter,
    Gauge,
    Histogram,
    Summary,
}

#[derive(Debug, Clone)]
pub struct AlertRule {
    pub rule_id: String,
    pub name: String,
    pub condition: AlertCondition,
    pub severity: AlertSeverity,
    pub actions: Vec<AlertAction>,
    pub cooldown: u64,
}

#[derive(Debug, Clone)]
pub struct AlertCondition {
    pub metric_name: String,
    pub operator: ComparisonOperator,
    pub threshold: f64,
    pub duration: u64, // 持续时间（秒）
}

#[derive(Debug, Clone)]
pub enum ComparisonOperator {
    GreaterThan,
    LessThan,
    Equal,
    NotEqual,
    GreaterOrEqual,
    LessOrEqual,
}

#[derive(Debug, Clone)]
pub enum AlertSeverity {
    Critical,
    Warning,
    Info,
}

#[derive(Debug, Clone)]
pub struct AlertAction {
    pub action_type: ActionType,
    pub parameters: HashMap<String, String>,
}

#[derive(Debug, Clone)]
pub enum ActionType {
    SendEmail,
    SendSlack,
    SendWebhook,
    ExecuteScript,
    ScaleService,
}

impl MonitoringSystem {
    pub async fn collect_metrics(&mut self) -> Result<Vec<Metric>, MonitoringError> {
        let mut metrics = Vec::new();
        
        // 收集系统指标
        let system_metrics = self.collect_system_metrics().await?;
        metrics.extend(system_metrics);
        
        // 收集应用指标
        let app_metrics = self.collect_application_metrics().await?;
        metrics.extend(app_metrics);
        
        // 收集业务指标
        let business_metrics = self.collect_business_metrics().await?;
        metrics.extend(business_metrics);
        
        // 存储指标
        self.metrics_collector.store_metrics(&metrics).await?;
        
        // 检查告警规则
        self.check_alert_rules(&metrics).await?;
        
        Ok(metrics)
    }
    
    async fn collect_system_metrics(&self) -> Result<Vec<Metric>, MonitoringError> {
        let mut metrics = Vec::new();
        
        // CPU使用率
        let cpu_usage = self.get_cpu_usage().await?;
        metrics.push(Metric {
            name: "system_cpu_usage".to_string(),
            value: cpu_usage,
            timestamp: chrono::Utc::now(),
            labels: HashMap::new(),
            metric_type: MetricType::Gauge,
        });
        
        // 内存使用率
        let memory_usage = self.get_memory_usage().await?;
        metrics.push(Metric {
            name: "system_memory_usage".to_string(),
            value: memory_usage,
            timestamp: chrono::Utc::now(),
            labels: HashMap::new(),
            metric_type: MetricType::Gauge,
        });
        
        // 磁盘使用率
        let disk_usage = self.get_disk_usage().await?;
        metrics.push(Metric {
            name: "system_disk_usage".to_string(),
            value: disk_usage,
            timestamp: chrono::Utc::now(),
            labels: HashMap::new(),
            metric_type: MetricType::Gauge,
        });
        
        // 网络流量
        let network_metrics = self.get_network_metrics().await?;
        metrics.extend(network_metrics);
        
        Ok(metrics)
    }
    
    async fn check_alert_rules(&self, metrics: &[Metric]) -> Result<(), MonitoringError> {
        let alert_rules = self.alert_manager.get_alert_rules().await?;
        
        for rule in alert_rules {
            let matching_metrics: Vec<_> = metrics.iter()
                .filter(|m| m.name == rule.condition.metric_name)
                .collect();
            
            if !matching_metrics.is_empty() {
                let should_trigger = self.evaluate_alert_condition(&rule.condition, matching_metrics).await?;
                
                if should_trigger {
                    self.alert_manager.trigger_alert(&rule, matching_metrics).await?;
                }
            }
        }
        
        Ok(())
    }
    
    async fn evaluate_alert_condition(
        &self,
        condition: &AlertCondition,
        metrics: &[&Metric],
    ) -> Result<bool, MonitoringError> {
        // 检查持续时间窗口内的指标
        let window_start = chrono::Utc::now() - chrono::Duration::seconds(condition.duration as i64);
        
        let window_metrics: Vec<_> = metrics.iter()
            .filter(|m| m.timestamp >= window_start)
            .collect();
        
        if window_metrics.is_empty() {
            return Ok(false);
        }
        
        // 计算平均值
        let avg_value = window_metrics.iter()
            .map(|m| m.value)
            .sum::<f64>() / window_metrics.len() as f64;
        
        // 评估条件
        let result = match condition.operator {
            ComparisonOperator::GreaterThan => avg_value > condition.threshold,
            ComparisonOperator::LessThan => avg_value < condition.threshold,
            ComparisonOperator::Equal => (avg_value - condition.threshold).abs() < 0.001,
            ComparisonOperator::NotEqual => (avg_value - condition.threshold).abs() >= 0.001,
            ComparisonOperator::GreaterOrEqual => avg_value >= condition.threshold,
            ComparisonOperator::LessOrEqual => avg_value <= condition.threshold,
        };
        
        Ok(result)
    }
}
```

### 2.2 自动扩缩容

```rust
#[derive(Debug)]
pub struct AutoScaler {
    scaling_policies: HashMap<String, ScalingPolicy>,
    metrics_analyzer: MetricsAnalyzer,
    deployment_manager: DeploymentManager,
    scaling_history: Vec<ScalingEvent>,
}

#[derive(Debug, Clone)]
pub struct ScalingPolicy {
    pub service_name: String,
    pub min_replicas: u32,
    pub max_replicas: u32,
    pub target_cpu_utilization: f64,
    pub target_memory_utilization: f64,
    pub scale_up_threshold: f64,
    pub scale_down_threshold: f64,
    pub scale_up_cooldown: u64,
    pub scale_down_cooldown: u64,
}

#[derive(Debug, Clone)]
pub struct ScalingEvent {
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub service_name: String,
    pub old_replicas: u32,
    pub new_replicas: u32,
    pub reason: String,
    pub metrics: HashMap<String, f64>,
}

impl AutoScaler {
    pub async fn evaluate_scaling(&mut self) -> Result<Vec<ScalingDecision>, ScalingError> {
        let mut decisions = Vec::new();
        
        for (service_name, policy) in &self.scaling_policies {
            let current_metrics = self.metrics_analyzer.get_service_metrics(service_name).await?;
            
            let decision = self.evaluate_service_scaling(policy, &current_metrics).await?;
            
            if let Some(decision) = decision {
                decisions.push(decision);
            }
        }
        
        Ok(decisions)
    }
    
    async fn evaluate_service_scaling(
        &self,
        policy: &ScalingPolicy,
        metrics: &HashMap<String, f64>,
    ) -> Result<Option<ScalingDecision>, ScalingError> {
        let cpu_utilization = metrics.get("cpu_usage").unwrap_or(&0.0);
        let memory_utilization = metrics.get("memory_usage").unwrap_or(&0.0);
        
        let current_replicas = self.deployment_manager.get_replica_count(&policy.service_name).await?;
        
        // 检查是否需要扩容
        let should_scale_up = cpu_utilization > &policy.scale_up_threshold ||
                             memory_utilization > &policy.scale_up_threshold;
        
        // 检查是否需要缩容
        let should_scale_down = cpu_utilization < &policy.scale_down_threshold &&
                               memory_utilization < &policy.scale_down_threshold;
        
        // 检查冷却期
        let last_scaling = self.get_last_scaling_event(&policy.service_name).await?;
        let now = chrono::Utc::now();
        
        if let Some(last_event) = last_scaling {
            let cooldown_duration = if should_scale_up {
                chrono::Duration::seconds(policy.scale_up_cooldown as i64)
            } else {
                chrono::Duration::seconds(policy.scale_down_cooldown as i64)
            };
            
            if now - last_event.timestamp < cooldown_duration {
                return Ok(None);
            }
        }
        
        // 计算目标副本数
        let target_replicas = if should_scale_up {
            let new_replicas = current_replicas + 1;
            if new_replicas <= policy.max_replicas {
                Some(new_replicas)
            } else {
                None
            }
        } else if should_scale_down {
            let new_replicas = current_replicas.saturating_sub(1);
            if new_replicas >= policy.min_replicas {
                Some(new_replicas)
            } else {
                None
            }
        } else {
            None
        };
        
        if let Some(target_replicas) = target_replicas {
            let reason = if should_scale_up {
                format!("High resource utilization: CPU={:.1}%, Memory={:.1}%", 
                       cpu_utilization * 100.0, memory_utilization * 100.0)
            } else {
                format!("Low resource utilization: CPU={:.1}%, Memory={:.1}%", 
                       cpu_utilization * 100.0, memory_utilization * 100.0)
            };
            
            Ok(Some(ScalingDecision {
                service_name: policy.service_name.clone(),
                current_replicas,
                target_replicas,
                reason,
                metrics: metrics.clone(),
            }))
        } else {
            Ok(None)
        }
    }
    
    pub async fn execute_scaling_decision(
        &mut self,
        decision: ScalingDecision,
    ) -> Result<ScalingResult, ScalingError> {
        // 执行扩缩容
        let result = self.deployment_manager.scale_service(
            &decision.service_name,
            decision.target_replicas,
        ).await?;
        
        // 记录扩缩容事件
        let scaling_event = ScalingEvent {
            timestamp: chrono::Utc::now(),
            service_name: decision.service_name.clone(),
            old_replicas: decision.current_replicas,
            new_replicas: decision.target_replicas,
            reason: decision.reason,
            metrics: decision.metrics,
        };
        
        self.scaling_history.push(scaling_event);
        
        Ok(ScalingResult {
            service_name: decision.service_name,
            old_replicas: decision.current_replicas,
            new_replicas: decision.target_replicas,
            success: result.success,
            message: result.message,
        })
    }
}
```

## 3. 配置和使用示例

### 3.1 部署配置

```yaml
# config/deployment.yaml
deployment:
  environment: "production"
  platform: "kubernetes"
  
  services:
    iot_gateway:
      image: "iot-gateway:latest"
      replicas: 3
      resources:
        cpu_cores: 2.0
        memory_mb: 4096
        storage_gb: 20
      ports:
        - container_port: 8080
          host_port: 8080
          protocol: "tcp"
      environment:
        - DATABASE_URL=postgresql://user:pass@db:5432/iot
        - REDIS_URL=redis://redis:6379
        - LOG_LEVEL=info
      health_check:
        endpoint: "/health"
        interval: 30
        timeout: 5
        retries: 3
      scaling_policy:
        min_replicas: 2
        max_replicas: 10
        target_cpu_utilization: 0.7
        target_memory_utilization: 0.8
        
    semantic_engine:
      image: "semantic-engine:latest"
      replicas: 2
      resources:
        cpu_cores: 4.0
        memory_mb: 8192
        storage_gb: 50
      ports:
        - container_port: 9090
          host_port: 9090
          protocol: "tcp"
      environment:
        - MODEL_PATH=/models
        - BATCH_SIZE=32
        - WORKER_THREADS=8
      volumes:
        - host_path: "/data/models"
          container_path: "/models"
          read_only: true
          
    blockchain_node:
      image: "blockchain-node:latest"
      replicas: 1
      resources:
        cpu_cores: 1.0
        memory_mb: 2048
        storage_gb: 100
      ports:
        - container_port: 8333
          host_port: 8333
          protocol: "tcp"
      volumes:
        - host_path: "/data/blockchain"
          container_path: "/blockchain"
          read_only: false
          
  monitoring:
    prometheus:
      enabled: true
      port: 9090
      
    grafana:
      enabled: true
      port: 3000
      
    alerting:
      enabled: true
      rules:
        - name: "high_cpu_usage"
          condition: "cpu_usage > 80"
          duration: 300
          severity: "warning"
          actions:
            - type: "email"
              recipients: ["admin@example.com"]
            - type: "slack"
              channel: "#alerts"
              
        - name: "service_down"
          condition: "health_check_failed"
          duration: 60
          severity: "critical"
          actions:
            - type: "webhook"
              url: "https://api.example.com/incidents"
```

### 3.2 使用示例

```rust
use crate::deployment::{ContainerOrchestrator, KubernetesDeployer, ServiceDefinition};
use crate::monitoring::{MonitoringSystem, AutoScaler};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // 创建容器编排器
    let mut orchestrator = ContainerOrchestrator::new().await?;
    
    // 定义IoT网关服务
    let gateway_service = ServiceDefinition {
        service_name: "iot-gateway".to_string(),
        image: "iot-gateway:latest".to_string(),
        version: "1.0.0".to_string(),
        replicas: 3,
        resources: ResourceRequirements {
            cpu_cores: 2.0,
            memory_mb: 4096,
            storage_gb: 20,
            gpu_count: None,
        },
        environment: HashMap::from([
            ("DATABASE_URL".to_string(), "postgresql://user:pass@db:5432/iot".to_string()),
            ("REDIS_URL".to_string(), "redis://redis:6379".to_string()),
            ("LOG_LEVEL".to_string(), "info".to_string()),
        ]),
        ports: vec![
            PortMapping {
                container_port: 8080,
                host_port: 8080,
                protocol: "tcp".to_string(),
            },
        ],
        volumes: Vec::new(),
        health_check: HealthCheckConfig {
            endpoint: "/health".to_string(),
            interval: 30,
            timeout: 5,
            retries: 3,
        },
        scaling_policy: ScalingPolicy {
            min_replicas: 2,
            max_replicas: 10,
            target_cpu_utilization: 0.7,
            target_memory_utilization: 0.8,
            scale_up_cooldown: 300,
            scale_down_cooldown: 600,
        },
    };
    
    // 部署服务
    let deployment_result = orchestrator.deploy_service(gateway_service).await?;
    println!("服务部署成功: {:?}", deployment_result);
    
    // 创建监控系统
    let mut monitoring_system = MonitoringSystem::new().await?;
    
    // 启动指标收集
    tokio::spawn(async move {
        loop {
            let metrics = monitoring_system.collect_metrics().await.unwrap_or_else(|e| {
                eprintln!("指标收集失败: {:?}", e);
                Vec::new()
            });
            
            println!("收集到 {} 个指标", metrics.len());
            
            tokio::time::sleep(std::time::Duration::from_secs(60)).await;
        }
    });
    
    // 创建自动扩缩容器
    let mut auto_scaler = AutoScaler::new().await?;
    
    // 添加扩缩容策略
    auto_scaler.add_scaling_policy(ScalingPolicy {
        service_name: "iot-gateway".to_string(),
        min_replicas: 2,
        max_replicas: 10,
        target_cpu_utilization: 0.7,
        target_memory_utilization: 0.8,
        scale_up_threshold: 0.8,
        scale_down_threshold: 0.3,
        scale_up_cooldown: 300,
        scale_down_cooldown: 600,
    }).await?;
    
    // 启动自动扩缩容
    tokio::spawn(async move {
        loop {
            let decisions = auto_scaler.evaluate_scaling().await.unwrap_or_else(|e| {
                eprintln!("扩缩容评估失败: {:?}", e);
                Vec::new()
            });
            
            for decision in decisions {
                println!("执行扩缩容决策: {:?}", decision);
                let result = auto_scaler.execute_scaling_decision(decision).await.unwrap_or_else(|e| {
                    eprintln!("扩缩容执行失败: {:?}", e);
                    ScalingResult {
                        service_name: "unknown".to_string(),
                        old_replicas: 0,
                        new_replicas: 0,
                        success: false,
                        message: e.to_string(),
                    }
                });
                
                println!("扩缩容结果: {:?}", result);
            }
            
            tokio::time::sleep(std::time::Duration::from_secs(300)).await;
        }
    });
    
    // 保持运行
    tokio::signal::ctrl_c().await?;
    println!("正在关闭部署系统...");
    
    Ok(())
}
```

这个生产部署与运维自动化实现提供了完整的部署和运维功能，包括：

- 容器化部署与编排
- Kubernetes部署管理
- 监控与告警系统
- 自动扩缩容
- 健康检查与故障恢复
- 完整的配置和使用示例

支持生产环境的高可用部署和自动化运维管理。
