# 缓存系统优化实现

## 概述

IoT系统的缓存优化包含多级缓存架构、智能缓存策略、分布式缓存一致性等关键技术。

## 核心实现

### 1. 多级缓存架构

```rust
use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use std::time::{Duration, Instant};
use tokio::sync::RwLock as AsyncRwLock;
use serde::{Serialize, Deserialize};

#[derive(Debug, Clone)]
pub struct CacheEntry<T> {
    pub value: T,
    pub created_at: Instant,
    pub last_accessed: Instant,
    pub access_count: u64,
    pub ttl: Option<Duration>,
}

pub trait Cache<K, V>: Send + Sync {
    async fn get(&self, key: &K) -> Option<V>;
    async fn put(&self, key: K, value: V, ttl: Option<Duration>);
    async fn remove(&self, key: &K) -> Option<V>;
    async fn clear(&self);
    async fn size(&self) -> usize;
}

// L1缓存：内存缓存
pub struct MemoryCache<K, V> {
    storage: Arc<AsyncRwLock<HashMap<K, CacheEntry<V>>>>,
    max_size: usize,
    default_ttl: Option<Duration>,
}

impl<K, V> MemoryCache<K, V>
where
    K: std::hash::Hash + Eq + Clone + Send + Sync,
    V: Clone + Send + Sync,
{
    pub fn new(max_size: usize, default_ttl: Option<Duration>) -> Self {
        Self {
            storage: Arc::new(AsyncRwLock::new(HashMap::new())),
            max_size,
            default_ttl,
        }
    }

    async fn evict_if_needed(&self) {
        let mut storage = self.storage.write().await;
        
        if storage.len() >= self.max_size {
            // LRU淘汰策略
            let mut entries: Vec<_> = storage.iter().collect();
            entries.sort_by_key(|(_, entry)| entry.last_accessed);
            
            let evict_count = storage.len() - self.max_size + 1;
            for i in 0..evict_count {
                if let Some((key, _)) = entries.get(i) {
                    storage.remove(*key);
                }
            }
        }
    }

    async fn cleanup_expired(&self) {
        let mut storage = self.storage.write().await;
        let now = Instant::now();
        
        storage.retain(|_, entry| {
            if let Some(ttl) = entry.ttl {
                now.duration_since(entry.created_at) < ttl
            } else {
                true
            }
        });
    }
}

impl<K, V> Cache<K, V> for MemoryCache<K, V>
where
    K: std::hash::Hash + Eq + Clone + Send + Sync,
    V: Clone + Send + Sync,
{
    async fn get(&self, key: &K) -> Option<V> {
        self.cleanup_expired().await;
        
        let mut storage = self.storage.write().await;
        if let Some(entry) = storage.get_mut(key) {
            entry.last_accessed = Instant::now();
            entry.access_count += 1;
            Some(entry.value.clone())
        } else {
            None
        }
    }

    async fn put(&self, key: K, value: V, ttl: Option<Duration>) {
        self.evict_if_needed().await;
        
        let entry = CacheEntry {
            value,
            created_at: Instant::now(),
            last_accessed: Instant::now(),
            access_count: 0,
            ttl: ttl.or(self.default_ttl),
        };

        let mut storage = self.storage.write().await;
        storage.insert(key, entry);
    }

    async fn remove(&self, key: &K) -> Option<V> {
        let mut storage = self.storage.write().await;
        storage.remove(key).map(|entry| entry.value)
    }

    async fn clear(&self) {
        let mut storage = self.storage.write().await;
        storage.clear();
    }

    async fn size(&self) -> usize {
        let storage = self.storage.read().await;
        storage.len()
    }
}

// L2缓存：Redis分布式缓存
pub struct RedisCache {
    client: redis::Client,
    key_prefix: String,
}

impl RedisCache {
    pub fn new(redis_url: &str, key_prefix: String) -> Result<Self, redis::RedisError> {
        let client = redis::Client::open(redis_url)?;
        Ok(Self {
            client,
            key_prefix,
        })
    }

    fn format_key(&self, key: &str) -> String {
        format!("{}:{}", self.key_prefix, key)
    }
}

impl Cache<String, String> for RedisCache {
    async fn get(&self, key: &String) -> Option<String> {
        let mut conn = self.client.get_async_connection().await.ok()?;
        let formatted_key = self.format_key(key);
        
        redis::cmd("GET")
            .arg(&formatted_key)
            .query_async(&mut conn)
            .await
            .ok()
    }

    async fn put(&self, key: String, value: String, ttl: Option<Duration>) {
        if let Ok(mut conn) = self.client.get_async_connection().await {
            let formatted_key = self.format_key(&key);
            
            match ttl {
                Some(duration) => {
                    let _ = redis::cmd("SETEX")
                        .arg(&formatted_key)
                        .arg(duration.as_secs())
                        .arg(&value)
                        .query_async::<_, ()>(&mut conn)
                        .await;
                }
                None => {
                    let _ = redis::cmd("SET")
                        .arg(&formatted_key)
                        .arg(&value)
                        .query_async::<_, ()>(&mut conn)
                        .await;
                }
            }
        }
    }

    async fn remove(&self, key: &String) -> Option<String> {
        let mut conn = self.client.get_async_connection().await.ok()?;
        let formatted_key = self.format_key(key);
        
        let value: Option<String> = redis::cmd("GET")
            .arg(&formatted_key)
            .query_async(&mut conn)
            .await
            .ok()?;

        redis::cmd("DEL")
            .arg(&formatted_key)
            .query_async::<_, ()>(&mut conn)
            .await
            .ok()?;

        value
    }

    async fn clear(&self) {
        if let Ok(mut conn) = self.client.get_async_connection().await {
            let pattern = format!("{}:*", self.key_prefix);
            let keys: Vec<String> = redis::cmd("KEYS")
                .arg(&pattern)
                .query_async(&mut conn)
                .await
                .unwrap_or_default();

            if !keys.is_empty() {
                let _ = redis::cmd("DEL")
                    .arg(&keys)
                    .query_async::<_, ()>(&mut conn)
                    .await;
            }
        }
    }

    async fn size(&self) -> usize {
        if let Ok(mut conn) = self.client.get_async_connection().await {
            let pattern = format!("{}:*", self.key_prefix);
            let keys: Vec<String> = redis::cmd("KEYS")
                .arg(&pattern)
                .query_async(&mut conn)
                .await
                .unwrap_or_default();
            keys.len()
        } else {
            0
        }
    }
}

// 多级缓存管理器
pub struct MultiLevelCache<K, V> {
    l1_cache: Arc<dyn Cache<K, V>>,
    l2_cache: Option<Arc<dyn Cache<K, V>>>,
    l3_cache: Option<Arc<dyn Cache<K, V>>>,
    write_through: bool,
    write_behind: bool,
}

impl<K, V> MultiLevelCache<K, V>
where
    K: Clone + Send + Sync,
    V: Clone + Send + Sync,
{
    pub fn new(
        l1_cache: Arc<dyn Cache<K, V>>,
        l2_cache: Option<Arc<dyn Cache<K, V>>>,
        l3_cache: Option<Arc<dyn Cache<K, V>>>,
    ) -> Self {
        Self {
            l1_cache,
            l2_cache,
            l3_cache,
            write_through: true,
            write_behind: false,
        }
    }

    pub async fn get(&self, key: &K) -> Option<V> {
        // L1缓存查找
        if let Some(value) = self.l1_cache.get(key).await {
            return Some(value);
        }

        // L2缓存查找
        if let Some(l2) = &self.l2_cache {
            if let Some(value) = l2.get(key).await {
                // 回填L1缓存
                self.l1_cache.put(key.clone(), value.clone(), None).await;
                return Some(value);
            }
        }

        // L3缓存查找
        if let Some(l3) = &self.l3_cache {
            if let Some(value) = l3.get(key).await {
                // 回填L1和L2缓存
                self.l1_cache.put(key.clone(), value.clone(), None).await;
                if let Some(l2) = &self.l2_cache {
                    l2.put(key.clone(), value.clone(), None).await;
                }
                return Some(value);
            }
        }

        None
    }

    pub async fn put(&self, key: K, value: V, ttl: Option<Duration>) {
        // 写入L1缓存
        self.l1_cache.put(key.clone(), value.clone(), ttl).await;

        if self.write_through {
            // 同步写入其他级别缓存
            if let Some(l2) = &self.l2_cache {
                l2.put(key.clone(), value.clone(), ttl).await;
            }
            if let Some(l3) = &self.l3_cache {
                l3.put(key.clone(), value.clone(), ttl).await;
            }
        } else if self.write_behind {
            // 异步写入其他级别缓存
            let l2_cache = self.l2_cache.clone();
            let l3_cache = self.l3_cache.clone();
            let key_clone = key.clone();
            let value_clone = value.clone();
            
            tokio::spawn(async move {
                if let Some(l2) = l2_cache {
                    l2.put(key_clone.clone(), value_clone.clone(), ttl).await;
                }
                if let Some(l3) = l3_cache {
                    l3.put(key_clone, value_clone, ttl).await;
                }
            });
        }
    }

    pub async fn remove(&self, key: &K) -> Option<V> {
        let mut result = None;

        // 从所有级别删除
        result = result.or(self.l1_cache.remove(key).await);
        
        if let Some(l2) = &self.l2_cache {
            result = result.or(l2.remove(key).await);
        }
        
        if let Some(l3) = &self.l3_cache {
            result = result.or(l3.remove(key).await);
        }

        result
    }
}
```

### 2. 智能缓存策略

```rust
#[derive(Debug, Clone)]
pub enum EvictionPolicy {
    LRU,    // Least Recently Used
    LFU,    // Least Frequently Used
    FIFO,   // First In First Out
    Random,
    TTL,    // Time To Live
}

#[derive(Debug, Clone)]
pub struct CachePolicy {
    pub eviction_policy: EvictionPolicy,
    pub max_size: usize,
    pub default_ttl: Option<Duration>,
    pub cleanup_interval: Duration,
    pub hit_ratio_threshold: f64,
}

pub struct AdaptiveCache<K, V> {
    memory_cache: Arc<MemoryCache<K, V>>,
    policy: CachePolicy,
    metrics: Arc<CacheMetrics>,
    last_cleanup: Instant,
}

impl<K, V> AdaptiveCache<K, V>
where
    K: std::hash::Hash + Eq + Clone + Send + Sync,
    V: Clone + Send + Sync,
{
    pub fn new(policy: CachePolicy) -> Self {
        Self {
            memory_cache: Arc::new(MemoryCache::new(policy.max_size, policy.default_ttl)),
            policy,
            metrics: Arc::new(CacheMetrics::new()),
            last_cleanup: Instant::now(),
        }
    }

    pub async fn get(&self, key: &K) -> Option<V> {
        let result = self.memory_cache.get(key).await;
        
        match &result {
            Some(_) => self.metrics.record_hit(),
            None => self.metrics.record_miss(),
        }

        // 自适应策略调整
        self.adapt_policy().await;

        result
    }

    pub async fn put(&self, key: K, value: V) {
        self.memory_cache.put(key, value, self.policy.default_ttl).await;
        self.cleanup_if_needed().await;
    }

    async fn adapt_policy(&self) {
        let hit_ratio = self.metrics.hit_ratio();
        
        // 如果命中率低于阈值，调整策略
        if hit_ratio < self.policy.hit_ratio_threshold {
            log::info!("Hit ratio {} below threshold {}, adapting cache policy", 
                      hit_ratio, self.policy.hit_ratio_threshold);
            
            // 可以动态调整缓存大小、TTL等
            // 这里简化实现
        }
    }

    async fn cleanup_if_needed(&self) {
        if self.last_cleanup.elapsed() >= self.policy.cleanup_interval {
            // 执行清理逻辑
            tokio::spawn(async move {
                // 清理过期条目
            });
        }
    }
}

pub struct CacheMetrics {
    hits: Arc<std::sync::atomic::AtomicU64>,
    misses: Arc<std::sync::atomic::AtomicU64>,
    evictions: Arc<std::sync::atomic::AtomicU64>,
}

impl CacheMetrics {
    pub fn new() -> Self {
        Self {
            hits: Arc::new(std::sync::atomic::AtomicU64::new(0)),
            misses: Arc::new(std::sync::atomic::AtomicU64::new(0)),
            evictions: Arc::new(std::sync::atomic::AtomicU64::new(0)),
        }
    }

    pub fn record_hit(&self) {
        self.hits.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
    }

    pub fn record_miss(&self) {
        self.misses.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
    }

    pub fn record_eviction(&self) {
        self.evictions.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
    }

    pub fn hit_ratio(&self) -> f64 {
        let hits = self.hits.load(std::sync::atomic::Ordering::Relaxed);
        let misses = self.misses.load(std::sync::atomic::Ordering::Relaxed);
        let total = hits + misses;
        
        if total == 0 {
            0.0
        } else {
            hits as f64 / total as f64
        }
    }
}
```

### 3. 分布式缓存一致性

```rust
use std::collections::HashSet;
use tokio::sync::broadcast;

#[derive(Debug, Clone)]
pub enum CacheEvent {
    Put { key: String, value: String },
    Remove { key: String },
    Clear,
}

pub struct DistributedCacheManager {
    local_cache: Arc<dyn Cache<String, String>>,
    event_sender: broadcast::Sender<CacheEvent>,
    node_id: String,
    consistency_level: ConsistencyLevel,
}

#[derive(Debug, Clone)]
pub enum ConsistencyLevel {
    Eventual,   // 最终一致性
    Strong,     // 强一致性
    Weak,       // 弱一致性
}

impl DistributedCacheManager {
    pub fn new(
        local_cache: Arc<dyn Cache<String, String>>,
        node_id: String,
        consistency_level: ConsistencyLevel,
    ) -> Self {
        let (event_sender, _) = broadcast::channel(1000);
        
        Self {
            local_cache,
            event_sender,
            node_id,
            consistency_level,
        }
    }

    pub async fn get(&self, key: &str) -> Option<String> {
        self.local_cache.get(&key.to_string()).await
    }

    pub async fn put(&self, key: String, value: String) {
        match self.consistency_level {
            ConsistencyLevel::Strong => {
                // 强一致性：先更新其他节点，再更新本地
                self.broadcast_update(&key, &value).await;
                self.local_cache.put(key, value, None).await;
            }
            ConsistencyLevel::Eventual => {
                // 最终一致性：先更新本地，再异步通知其他节点
                self.local_cache.put(key.clone(), value.clone(), None).await;
                let _ = self.event_sender.send(CacheEvent::Put { key, value });
            }
            ConsistencyLevel::Weak => {
                // 弱一致性：只更新本地
                self.local_cache.put(key, value, None).await;
            }
        }
    }

    pub async fn remove(&self, key: &str) -> Option<String> {
        let result = self.local_cache.remove(&key.to_string()).await;
        
        match self.consistency_level {
            ConsistencyLevel::Strong | ConsistencyLevel::Eventual => {
                let _ = self.event_sender.send(CacheEvent::Remove { 
                    key: key.to_string() 
                });
            }
            ConsistencyLevel::Weak => {}
        }

        result
    }

    async fn broadcast_update(&self, key: &str, value: &str) {
        // 实现分布式更新逻辑
        // 可以使用Redis发布订阅、消息队列等
        log::info!("Broadcasting cache update: {} = {}", key, value);
    }

    pub fn subscribe_to_events(&self) -> broadcast::Receiver<CacheEvent> {
        self.event_sender.subscribe()
    }

    pub async fn handle_remote_event(&self, event: CacheEvent) {
        match event {
            CacheEvent::Put { key, value } => {
                self.local_cache.put(key, value, None).await;
            }
            CacheEvent::Remove { key } => {
                self.local_cache.remove(&key).await;
            }
            CacheEvent::Clear => {
                self.local_cache.clear().await;
            }
        }
    }
}

// 缓存同步器
pub struct CacheSynchronizer {
    managers: Vec<Arc<DistributedCacheManager>>,
    sync_interval: Duration,
}

impl CacheSynchronizer {
    pub fn new(sync_interval: Duration) -> Self {
        Self {
            managers: Vec::new(),
            sync_interval,
        }
    }

    pub fn add_manager(&mut self, manager: Arc<DistributedCacheManager>) {
        self.managers.push(manager);
    }

    pub async fn start_sync(&self) {
        let managers = self.managers.clone();
        let interval = self.sync_interval;

        tokio::spawn(async move {
            let mut interval_timer = tokio::time::interval(interval);
            
            loop {
                interval_timer.tick().await;
                
                for manager in &managers {
                    let mut receiver = manager.subscribe_to_events();
                    
                    while let Ok(event) = receiver.try_recv() {
                        // 将事件转发给其他管理器
                        for other_manager in &managers {
                            if !Arc::ptr_eq(manager, other_manager) {
                                other_manager.handle_remote_event(event.clone()).await;
                            }
                        }
                    }
                }
            }
        });
    }
}
```

## 配置管理

```toml
[cache]
enabled = true
default_ttl_seconds = 3600
cleanup_interval_seconds = 300

[l1_cache]
type = "memory"
max_size = 10000
eviction_policy = "LRU"

[l2_cache]
type = "redis"
url = "redis://localhost:6379"
key_prefix = "iot_cache"
pool_size = 10

[l3_cache]
type = "disk"
path = "/var/cache/iot"
max_size_mb = 1024

[distributed]
consistency_level = "eventual"
sync_interval_seconds = 60
node_id = "node-001"
```

## 性能基准测试

```rust
#[cfg(test)]
mod benchmarks {
    use super::*;
    use criterion::{black_box, Criterion};

    fn benchmark_cache_operations(c: &mut Criterion) {
        let rt = tokio::runtime::Runtime::new().unwrap();
        
        c.bench_function("memory_cache_get", |b| {
            let cache = MemoryCache::new(1000, None);
            
            b.to_async(&rt).iter(|| async {
                let key = "test_key".to_string();
                black_box(cache.get(&key).await);
            });
        });

        c.bench_function("memory_cache_put", |b| {
            let cache = MemoryCache::new(1000, None);
            
            b.to_async(&rt).iter(|| async {
                let key = format!("key_{}", rand::random::<u32>());
                let value = "test_value".to_string();
                black_box(cache.put(key, value, None).await);
            });
        });
    }
}
```

## 监控指标

```rust
use prometheus::{Counter, Histogram, Gauge};

pub struct CacheMonitoring {
    cache_hits: Counter,
    cache_misses: Counter,
    cache_evictions: Counter,
    cache_size: Gauge,
    operation_duration: Histogram,
}

impl CacheMonitoring {
    pub fn new() -> Self {
        Self {
            cache_hits: Counter::new("cache_hits_total", "Total cache hits").unwrap(),
            cache_misses: Counter::new("cache_misses_total", "Total cache misses").unwrap(),
            cache_evictions: Counter::new("cache_evictions_total", "Total cache evictions").unwrap(),
            cache_size: Gauge::new("cache_size", "Current cache size").unwrap(),
            operation_duration: Histogram::new("cache_operation_duration_seconds", "Cache operation duration").unwrap(),
        }
    }

    pub fn record_hit(&self) {
        self.cache_hits.inc();
    }

    pub fn record_miss(&self) {
        self.cache_misses.inc();
    }

    pub fn record_eviction(&self) {
        self.cache_evictions.inc();
    }

    pub fn update_size(&self, size: f64) {
        self.cache_size.set(size);
    }

    pub fn record_operation_duration(&self, duration: Duration) {
        self.operation_duration.observe(duration.as_secs_f64());
    }
}
```

## 部署配置

### Docker

```dockerfile
FROM rust:1.70-alpine AS builder
WORKDIR /app
COPY . .
RUN cargo build --release

FROM alpine:latest
RUN apk --no-cache add ca-certificates redis
WORKDIR /root/
COPY --from=builder /app/target/release/cache_system ./
COPY config/cache.toml ./config/
EXPOSE 8080
CMD ["./cache_system"]
```

### Kubernetes

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cache-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: cache-system
  template:
    metadata:
      labels:
        app: cache-system
    spec:
      containers:
      - name: cache-system
        image: iot/cache-system:latest
        env:
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        - name: NODE_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
spec:
  selector:
    app: redis
  ports:
  - port: 6379
```

## 总结

本缓存系统优化实现提供了：

- 多级缓存架构（内存+Redis+磁盘）
- 智能缓存策略和自适应调优
- 分布式缓存一致性保证
- 完整的监控和指标收集
- 高性能的并发访问支持
