# 数据流处理实现

## 1. 数据流处理架构

### 1.1 流处理核心概念

```rust
use std::collections::HashMap;
use serde::{Deserialize, Serialize};
use chrono::{DateTime, Utc};
use tokio::sync::mpsc;
use std::time::Duration;

/// 数据流事件
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StreamEvent {
    pub id: String,
    pub timestamp: DateTime<Utc>,
    pub source: String,
    pub event_type: String,
    pub data: serde_json::Value,
    pub metadata: HashMap<String, String>,
}

/// 流处理窗口
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessingWindow {
    pub start_time: DateTime<Utc>,
    pub end_time: DateTime<Utc>,
    pub events: Vec<StreamEvent>,
    pub window_type: WindowType,
}

/// 窗口类型
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum WindowType {
    Tumbling(Duration),    // 翻滚窗口
    Sliding(Duration),     // 滑动窗口
    Session(Duration),     // 会话窗口
    Global,               // 全局窗口
}

/// 流处理结果
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessingResult {
    pub window_id: String,
    pub timestamp: DateTime<Utc>,
    pub result_type: String,
    pub data: serde_json::Value,
    pub metadata: HashMap<String, String>,
}

/// 流处理器接口
#[async_trait::async_trait]
pub trait StreamProcessor: Send + Sync {
    async fn process(&self, events: &[StreamEvent]) -> Result<Vec<ProcessingResult>, Box<dyn std::error::Error>>;
    fn get_window_type(&self) -> WindowType;
    fn get_name(&self) -> &str;
}

/// 流处理管道
pub struct StreamPipeline {
    processors: Vec<Box<dyn StreamProcessor>>,
    event_sender: mpsc::Sender<StreamEvent>,
    event_receiver: mpsc::Receiver<StreamEvent>,
    result_sender: mpsc::Sender<ProcessingResult>,
    result_receiver: mpsc::Receiver<ProcessingResult>,
    windows: HashMap<String, ProcessingWindow>,
}

impl StreamPipeline {
    pub fn new() -> Self {
        let (event_tx, event_rx) = mpsc::channel(10000);
        let (result_tx, result_rx) = mpsc::channel(1000);
        
        Self {
            processors: Vec::new(),
            event_sender: event_tx,
            event_receiver: event_rx,
            result_sender: result_tx,
            result_receiver: result_rx,
            windows: HashMap::new(),
        }
    }

    /// 添加处理器
    pub fn add_processor(&mut self, processor: Box<dyn StreamProcessor>) {
        self.processors.push(processor);
    }

    /// 发送事件
    pub async fn send_event(&self, event: StreamEvent) -> Result<(), Box<dyn std::error::Error>> {
        self.event_sender.send(event).await
            .map_err(|e| format!("Failed to send event: {}", e).into())
    }

    /// 启动处理管道
    pub async fn start(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        let mut event_receiver = self.event_receiver.clone();
        let result_sender = self.result_sender.clone();
        let processors = self.processors.clone();
        
        tokio::spawn(async move {
            let mut windows = HashMap::new();
            
            while let Some(event) = event_receiver.recv().await {
                // 为每个处理器创建或更新窗口
                for processor in &processors {
                    let window_id = Self::get_window_id(processor.get_name(), &processor.get_window_type(), &event.timestamp);
                    
                    let window = windows.entry(window_id.clone()).or_insert_with(|| {
                        let (start_time, end_time) = Self::calculate_window_bounds(&processor.get_window_type(), &event.timestamp);
                        ProcessingWindow {
                            start_time,
                            end_time,
                            events: Vec::new(),
                            window_type: processor.get_window_type(),
                        }
                    });
                    
                    window.events.push(event.clone());
                    
                    // 检查窗口是否应该触发处理
                    if Self::should_trigger_processing(&window, &event.timestamp) {
                        if let Ok(results) = processor.process(&window.events).await {
                            for result in results {
                                if let Err(e) = result_sender.send(result).await {
                                    eprintln!("Failed to send processing result: {}", e);
                                }
                            }
                        }
                        
                        // 清理窗口
                        window.events.clear();
                    }
                }
            }
        });
        
        Ok(())
    }

    /// 获取处理结果
    pub async fn get_result(&mut self) -> Option<ProcessingResult> {
        self.result_receiver.recv().await
    }

    /// 获取窗口ID
    fn get_window_id(processor_name: &str, window_type: &WindowType, timestamp: &DateTime<Utc>) -> String {
        match window_type {
            WindowType::Tumbling(duration) => {
                let window_start = timestamp.timestamp() - (timestamp.timestamp() % duration.as_secs() as i64);
                format!("{}_{}", processor_name, window_start)
            }
            WindowType::Sliding(duration) => {
                let window_start = timestamp.timestamp() - (timestamp.timestamp() % duration.as_secs() as i64);
                format!("{}_{}", processor_name, window_start)
            }
            WindowType::Session(_) => {
                format!("{}_{}", processor_name, timestamp.timestamp())
            }
            WindowType::Global => {
                format!("{}_global", processor_name)
            }
        }
    }

    /// 计算窗口边界
    fn calculate_window_bounds(window_type: &WindowType, timestamp: &DateTime<Utc>) -> (DateTime<Utc>, DateTime<Utc>) {
        match window_type {
            WindowType::Tumbling(duration) => {
                let window_start = timestamp.timestamp() - (timestamp.timestamp() % duration.as_secs() as i64);
                let start_time = DateTime::from_timestamp(window_start, 0).unwrap_or(*timestamp);
                let end_time = start_time + chrono::Duration::seconds(duration.as_secs() as i64);
                (start_time, end_time)
            }
            WindowType::Sliding(duration) => {
                let window_start = timestamp.timestamp() - (timestamp.timestamp() % duration.as_secs() as i64);
                let start_time = DateTime::from_timestamp(window_start, 0).unwrap_or(*timestamp);
                let end_time = start_time + chrono::Duration::seconds(duration.as_secs() as i64);
                (start_time, end_time)
            }
            WindowType::Session(_) => {
                (*timestamp, *timestamp)
            }
            WindowType::Global => {
                (DateTime::from_timestamp(0, 0).unwrap(), *timestamp)
            }
        }
    }

    /// 检查是否应该触发处理
    fn should_trigger_processing(window: &ProcessingWindow, current_time: &DateTime<Utc>) -> bool {
        match window.window_type {
            WindowType::Tumbling(_) => {
                current_time >= &window.end_time
            }
            WindowType::Sliding(_) => {
                current_time >= &window.end_time
            }
            WindowType::Session(duration) => {
                if let Some(last_event) = window.events.last() {
                    (current_time.timestamp() - last_event.timestamp.timestamp()) > duration.as_secs() as i64
                } else {
                    false
                }
            }
            WindowType::Global => {
                window.events.len() >= 100 // 每100个事件处理一次
            }
        }
    }
}
```

## 2. 流处理器实现

### 2.1 聚合处理器

```rust
/// 聚合处理器
pub struct AggregationProcessor {
    name: String,
    window_type: WindowType,
    aggregation_type: AggregationType,
    field_name: String,
}

/// 聚合类型
#[derive(Debug, Clone)]
pub enum AggregationType {
    Count,
    Sum,
    Average,
    Min,
    Max,
    Distinct,
}

impl AggregationProcessor {
    pub fn new(name: String, window_type: WindowType, aggregation_type: AggregationType, field_name: String) -> Self {
        Self {
            name,
            window_type,
            aggregation_type,
            field_name,
        }
    }
}

#[async_trait::async_trait]
impl StreamProcessor for AggregationProcessor {
    async fn process(&self, events: &[StreamEvent]) -> Result<Vec<ProcessingResult>, Box<dyn std::error::Error>> {
        let mut result = ProcessingResult {
            window_id: format!("{}_{}", self.name, events.first().map(|e| e.timestamp.timestamp()).unwrap_or(0)),
            timestamp: Utc::now(),
            result_type: "aggregation".to_string(),
            data: serde_json::Value::Null,
            metadata: HashMap::new(),
        };

        match self.aggregation_type {
            AggregationType::Count => {
                result.data = serde_json::json!({
                    "count": events.len(),
                    "field": self.field_name
                });
            }
            AggregationType::Sum => {
                let sum: f64 = events.iter()
                    .filter_map(|event| {
                        event.data.get(&self.field_name)
                            .and_then(|v| v.as_f64())
                    })
                    .sum();
                
                result.data = serde_json::json!({
                    "sum": sum,
                    "field": self.field_name
                });
            }
            AggregationType::Average => {
                let values: Vec<f64> = events.iter()
                    .filter_map(|event| {
                        event.data.get(&self.field_name)
                            .and_then(|v| v.as_f64())
                    })
                    .collect();
                
                let average = if values.is_empty() {
                    0.0
                } else {
                    values.iter().sum::<f64>() / values.len() as f64
                };
                
                result.data = serde_json::json!({
                    "average": average,
                    "field": self.field_name,
                    "count": values.len()
                });
            }
            AggregationType::Min => {
                let min = events.iter()
                    .filter_map(|event| {
                        event.data.get(&self.field_name)
                            .and_then(|v| v.as_f64())
                    })
                    .min_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));
                
                result.data = serde_json::json!({
                    "min": min.unwrap_or(0.0),
                    "field": self.field_name
                });
            }
            AggregationType::Max => {
                let max = events.iter()
                    .filter_map(|event| {
                        event.data.get(&self.field_name)
                            .and_then(|v| v.as_f64())
                    })
                    .max_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));
                
                result.data = serde_json::json!({
                    "max": max.unwrap_or(0.0),
                    "field": self.field_name
                });
            }
            AggregationType::Distinct => {
                let distinct_values: std::collections::HashSet<String> = events.iter()
                    .filter_map(|event| {
                        event.data.get(&self.field_name)
                            .and_then(|v| v.as_str())
                            .map(|s| s.to_string())
                    })
                    .collect();
                
                result.data = serde_json::json!({
                    "distinct_count": distinct_values.len(),
                    "distinct_values": distinct_values.into_iter().collect::<Vec<String>>(),
                    "field": self.field_name
                });
            }
        }

        result.metadata.insert("processor".to_string(), self.name.clone());
        result.metadata.insert("aggregation_type".to_string(), format!("{:?}", self.aggregation_type));
        
        Ok(vec![result])
    }

    fn get_window_type(&self) -> WindowType {
        self.window_type.clone()
    }

    fn get_name(&self) -> &str {
        &self.name
    }
}
```

### 2.2 过滤处理器

```rust
/// 过滤处理器
pub struct FilterProcessor {
    name: String,
    window_type: WindowType,
    filter_condition: FilterCondition,
}

/// 过滤条件
#[derive(Debug, Clone)]
pub enum FilterCondition {
    FieldEquals(String, serde_json::Value),
    FieldGreaterThan(String, f64),
    FieldLessThan(String, f64),
    FieldContains(String, String),
    FieldRegex(String, String),
    Custom(String), // 自定义条件表达式
}

impl FilterProcessor {
    pub fn new(name: String, window_type: WindowType, filter_condition: FilterCondition) -> Self {
        Self {
            name,
            window_type,
            filter_condition,
        }
    }

    fn evaluate_condition(&self, event: &StreamEvent) -> bool {
        match &self.filter_condition {
            FilterCondition::FieldEquals(field, value) => {
                event.data.get(field) == Some(value)
            }
            FilterCondition::FieldGreaterThan(field, threshold) => {
                event.data.get(field)
                    .and_then(|v| v.as_f64())
                    .map(|v| v > *threshold)
                    .unwrap_or(false)
            }
            FilterCondition::FieldLessThan(field, threshold) => {
                event.data.get(field)
                    .and_then(|v| v.as_f64())
                    .map(|v| v < *threshold)
                    .unwrap_or(false)
            }
            FilterCondition::FieldContains(field, substring) => {
                event.data.get(field)
                    .and_then(|v| v.as_str())
                    .map(|s| s.contains(substring))
                    .unwrap_or(false)
            }
            FilterCondition::FieldRegex(field, pattern) => {
                // 简化的正则匹配，实际应该使用regex crate
                event.data.get(field)
                    .and_then(|v| v.as_str())
                    .map(|s| s.contains(pattern))
                    .unwrap_or(false)
            }
            FilterCondition::Custom(_) => {
                // 自定义条件评估
                true
            }
        }
    }
}

#[async_trait::async_trait]
impl StreamProcessor for FilterProcessor {
    async fn process(&self, events: &[StreamEvent]) -> Result<Vec<ProcessingResult>, Box<dyn std::error::Error>> {
        let filtered_events: Vec<StreamEvent> = events.iter()
            .filter(|event| self.evaluate_condition(event))
            .cloned()
            .collect();

        let result = ProcessingResult {
            window_id: format!("{}_{}", self.name, events.first().map(|e| e.timestamp.timestamp()).unwrap_or(0)),
            timestamp: Utc::now(),
            result_type: "filtered_events".to_string(),
            data: serde_json::json!({
                "original_count": events.len(),
                "filtered_count": filtered_events.len(),
                "events": filtered_events.iter().map(|e| {
                    serde_json::json!({
                        "id": e.id,
                        "timestamp": e.timestamp.to_rfc3339(),
                        "data": e.data
                    })
                }).collect::<Vec<_>>()
            }),
            metadata: {
                let mut metadata = HashMap::new();
                metadata.insert("processor".to_string(), self.name.clone());
                metadata.insert("filter_condition".to_string(), format!("{:?}", self.filter_condition));
                metadata
            },
        };

        Ok(vec![result])
    }

    fn get_window_type(&self) -> WindowType {
        self.window_type.clone()
    }

    fn get_name(&self) -> &str {
        &self.name
    }
}
```

### 2.3 模式检测处理器

```rust
/// 模式检测处理器
pub struct PatternDetectionProcessor {
    name: String,
    window_type: WindowType,
    pattern_type: PatternType,
    threshold: f64,
}

/// 模式类型
#[derive(Debug, Clone)]
pub enum PatternType {
    Anomaly,      // 异常检测
    Trend,        // 趋势检测
    Seasonality,  // 季节性检测
    Spike,        // 峰值检测
}

impl PatternDetectionProcessor {
    pub fn new(name: String, window_type: WindowType, pattern_type: PatternType, threshold: f64) -> Self {
        Self {
            name,
            window_type,
            pattern_type,
            threshold,
        }
    }

    fn detect_anomaly(&self, values: &[f64]) -> Vec<bool> {
        if values.len() < 3 {
            return vec![false; values.len()];
        }

        let mean = values.iter().sum::<f64>() / values.len() as f64;
        let variance = values.iter()
            .map(|v| (v - mean).powi(2))
            .sum::<f64>() / values.len() as f64;
        let std_dev = variance.sqrt();

        values.iter()
            .map(|v| (v - mean).abs() > self.threshold * std_dev)
            .collect()
    }

    fn detect_trend(&self, values: &[f64]) -> Option<String> {
        if values.len() < 3 {
            return None;
        }

        let n = values.len() as f64;
        let sum_x: f64 = (0..values.len()).map(|i| i as f64).sum();
        let sum_y: f64 = values.iter().sum();
        let sum_xy: f64 = values.iter().enumerate().map(|(i, v)| i as f64 * v).sum();
        let sum_x2: f64 = (0..values.len()).map(|i| (i as f64).powi(2)).sum();

        let slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x.powi(2));

        if slope.abs() < self.threshold {
            Some("stable".to_string())
        } else if slope > 0.0 {
            Some("increasing".to_string())
        } else {
            Some("decreasing".to_string())
        }
    }

    fn detect_spike(&self, values: &[f64]) -> Vec<bool> {
        if values.len() < 3 {
            return vec![false; values.len()];
        }

        let mut spikes = vec![false; values.len()];
        
        for i in 1..values.len() - 1 {
            let prev = values[i - 1];
            let curr = values[i];
            let next = values[i + 1];
            
            let avg_neighbors = (prev + next) / 2.0;
            let deviation = (curr - avg_neighbors).abs() / avg_neighbors;
            
            spikes[i] = deviation > self.threshold;
        }

        spikes
    }
}

#[async_trait::async_trait]
impl StreamProcessor for PatternDetectionProcessor {
    async fn process(&self, events: &[StreamEvent]) -> Result<Vec<ProcessingResult>, Box<dyn std::error::Error>> {
        // 提取数值字段（假设所有事件都有相同的数值字段）
        let values: Vec<f64> = events.iter()
            .filter_map(|event| {
                // 尝试从事件数据中提取数值
                if let Some(value) = event.data.as_object() {
                    value.values().next()
                        .and_then(|v| v.as_f64())
                } else {
                    event.data.as_f64()
                }
            })
            .collect();

        if values.is_empty() {
            return Ok(vec![]);
        }

        let result = match self.pattern_type {
            PatternType::Anomaly => {
                let anomalies = self.detect_anomaly(&values);
                serde_json::json!({
                    "pattern_type": "anomaly",
                    "anomalies": anomalies,
                    "values": values,
                    "threshold": self.threshold
                })
            }
            PatternType::Trend => {
                let trend = self.detect_trend(&values);
                serde_json::json!({
                    "pattern_type": "trend",
                    "trend": trend,
                    "values": values,
                    "threshold": self.threshold
                })
            }
            PatternType::Spike => {
                let spikes = self.detect_spike(&values);
                serde_json::json!({
                    "pattern_type": "spike",
                    "spikes": spikes,
                    "values": values,
                    "threshold": self.threshold
                })
            }
            PatternType::Seasonality => {
                // 简化的季节性检测
                serde_json::json!({
                    "pattern_type": "seasonality",
                    "values": values,
                    "threshold": self.threshold,
                    "note": "Seasonality detection requires longer time series"
                })
            }
        };

        let processing_result = ProcessingResult {
            window_id: format!("{}_{}", self.name, events.first().map(|e| e.timestamp.timestamp()).unwrap_or(0)),
            timestamp: Utc::now(),
            result_type: "pattern_detection".to_string(),
            data: result,
            metadata: {
                let mut metadata = HashMap::new();
                metadata.insert("processor".to_string(), self.name.clone());
                metadata.insert("pattern_type".to_string(), format!("{:?}", self.pattern_type));
                metadata
            },
        };

        Ok(vec![processing_result])
    }

    fn get_window_type(&self) -> WindowType {
        self.window_type.clone()
    }

    fn get_name(&self) -> &str {
        &self.name
    }
}
```

## 3. 数据流连接器

### 3.1 Kafka连接器

```rust
use rdkafka::consumer::{Consumer, StreamConsumer};
use rdkafka::producer::{FutureProducer, FutureRecord};
use rdkafka::ClientConfig;

/// Kafka流连接器
pub struct KafkaStreamConnector {
    consumer: StreamConsumer,
    producer: FutureProducer,
    topic: String,
}

impl KafkaStreamConnector {
    pub async fn new(bootstrap_servers: &str, topic: String) -> Result<Self, Box<dyn std::error::Error>> {
        let consumer: StreamConsumer = ClientConfig::new()
            .set("group.id", "iot_stream_processor")
            .set("bootstrap.servers", bootstrap_servers)
            .set("enable.partition.eof", "false")
            .set("session.timeout.ms", "6000")
            .set("enable.auto.commit", "true")
            .create()?;

        let producer: FutureProducer = ClientConfig::new()
            .set("bootstrap.servers", bootstrap_servers)
            .set("message.timeout.ms", "5000")
            .create()?;

        consumer.subscribe(&[&topic])?;

        Ok(Self {
            consumer,
            producer,
            topic,
        })
    }

    /// 从Kafka消费事件
    pub async fn consume_events(&self) -> Result<Vec<StreamEvent>, Box<dyn std::error::Error>> {
        let mut events = Vec::new();
        
        // 这里应该实现实际的Kafka消费逻辑
        // 简化实现，实际应该使用rdkafka的异步消费API
        
        Ok(events)
    }

    /// 发送结果到Kafka
    pub async fn send_result(&self, result: &ProcessingResult) -> Result<(), Box<dyn std::error::Error>> {
        let payload = serde_json::to_string(result)?;
        
        let record = FutureRecord::to(&self.topic)
            .payload(payload.as_bytes())
            .key(&result.window_id);

        self.producer.send(record, Duration::from_secs(5)).await
            .map_err(|(e, _)| format!("Failed to send to Kafka: {}", e).into())?;

        Ok(())
    }
}
```

### 3.2 MQTT连接器

```rust
use paho_mqtt as mqtt;

/// MQTT流连接器
pub struct MqttStreamConnector {
    client: mqtt::AsyncClient,
    topic: String,
}

impl MqttStreamConnector {
    pub async fn new(broker_url: &str, topic: String) -> Result<Self, Box<dyn std::error::Error>> {
        let create_opts = mqtt::CreateOptionsBuilder::new()
            .server_uri(broker_url)
            .client_id("iot_stream_processor")
            .finalize();

        let client = mqtt::AsyncClient::new(create_opts)?;

        let conn_opts = mqtt::ConnectOptionsBuilder::new()
            .keep_alive_interval(Duration::from_secs(20))
            .clean_session(true)
            .finalize();

        client.connect(conn_opts).await?;

        Ok(Self {
            client,
            topic,
        })
    }

    /// 从MQTT订阅事件
    pub async fn subscribe_events(&self) -> Result<(), Box<dyn std::error::Error>> {
        self.client.subscribe(&self.topic, 1).await?;
        Ok(())
    }

    /// 发送结果到MQTT
    pub async fn send_result(&self, result: &ProcessingResult) -> Result<(), Box<dyn std::error::Error>> {
        let payload = serde_json::to_string(result)?;
        
        let message = mqtt::MessageBuilder::new()
            .topic(&self.topic)
            .payload(payload.as_bytes())
            .qos(1)
            .finalize();

        self.client.publish(message).await?;
        Ok(())
    }
}
```

## 4. 应用示例

### 4.1 IoT传感器数据流处理

```rust
use crate::stream_processing::{StreamPipeline, StreamEvent, ProcessingResult};
use crate::processors::{AggregationProcessor, FilterProcessor, PatternDetectionProcessor};
use crate::connectors::{KafkaStreamConnector, MqttStreamConnector};

async fn iot_sensor_stream_processing() -> Result<(), Box<dyn std::error::Error>> {
    // 创建流处理管道
    let mut pipeline = StreamPipeline::new();
    
    // 添加聚合处理器 - 计算温度平均值
    let temp_aggregator = Box::new(AggregationProcessor::new(
        "temperature_avg".to_string(),
        WindowType::Tumbling(Duration::from_secs(60)), // 1分钟翻滚窗口
        AggregationType::Average,
        "temperature".to_string(),
    ));
    pipeline.add_processor(temp_aggregator);
    
    // 添加过滤处理器 - 过滤异常温度值
    let temp_filter = Box::new(FilterProcessor::new(
        "temperature_filter".to_string(),
        WindowType::Sliding(Duration::from_secs(300)), // 5分钟滑动窗口
        FilterCondition::FieldBetween("temperature".to_string(), -50.0, 100.0),
    ));
    pipeline.add_processor(temp_filter);
    
    // 添加模式检测处理器 - 检测温度异常
    let anomaly_detector = Box::new(PatternDetectionProcessor::new(
        "temperature_anomaly".to_string(),
        WindowType::Tumbling(Duration::from_secs(300)), // 5分钟窗口
        PatternType::Anomaly,
        2.0, // 2个标准差阈值
    ));
    pipeline.add_processor(anomaly_detector);
    
    // 启动处理管道
    pipeline.start().await?;
    
    // 创建Kafka连接器
    let kafka_connector = KafkaStreamConnector::new(
        "localhost:9092",
        "iot_sensor_data".to_string(),
    ).await?;
    
    // 创建MQTT连接器用于结果输出
    let mqtt_connector = MqttStreamConnector::new(
        "tcp://localhost:1883",
        "iot_processing_results".to_string(),
    ).await?;
    
    // 启动事件消费循环
    tokio::spawn(async move {
        loop {
            match kafka_connector.consume_events().await {
                Ok(events) => {
                    for event in events {
                        if let Err(e) = pipeline.send_event(event).await {
                            eprintln!("Failed to send event to pipeline: {}", e);
                        }
                    }
                }
                Err(e) => {
                    eprintln!("Failed to consume events from Kafka: {}", e);
                }
            }
            
            tokio::time::sleep(Duration::from_secs(1)).await;
        }
    });
    
    // 启动结果处理循环
    tokio::spawn(async move {
        loop {
            if let Some(result) = pipeline.get_result().await {
                // 发送结果到MQTT
                if let Err(e) = mqtt_connector.send_result(&result).await {
                    eprintln!("Failed to send result to MQTT: {}", e);
                }
                
                // 打印结果
                println!("Processing result: {:?}", result);
            }
            
            tokio::time::sleep(Duration::from_millis(100)).await;
        }
    });
    
    // 模拟传感器数据
    let mut sensor_id = 1;
    loop {
        let temperature = 20.0 + (sensor_id as f64 * 0.1) + (rand::random::<f64>() - 0.5) * 10.0;
        let humidity = 50.0 + (rand::random::<f64>() - 0.5) * 20.0;
        
        let event = StreamEvent {
            id: format!("sensor_{}", sensor_id),
            timestamp: Utc::now(),
            source: format!("sensor_{}", sensor_id),
            event_type: "sensor_reading".to_string(),
            data: serde_json::json!({
                "temperature": temperature,
                "humidity": humidity,
                "sensor_id": sensor_id
            }),
            metadata: {
                let mut metadata = HashMap::new();
                metadata.insert("location".to_string(), "building_a".to_string());
                metadata.insert("sensor_type".to_string(), "environmental".to_string());
                metadata
            },
        };
        
        if let Err(e) = pipeline.send_event(event).await {
            eprintln!("Failed to send sensor event: {}", e);
        }
        
        sensor_id = (sensor_id % 10) + 1; // 循环使用10个传感器
        tokio::time::sleep(Duration::from_secs(5)).await;
    }
}
```

## 5. 总结

本实现提供了：

1. **完整的流处理架构** - 支持多种窗口类型和处理模式
2. **多种处理器类型** - 聚合、过滤、模式检测
3. **实时数据连接器** - Kafka和MQTT支持
4. **可扩展的管道设计** - 支持复杂的处理流程
5. **实际应用示例** - IoT传感器数据流处理

这个数据流处理系统为IoT平台提供了强大的实时数据处理能力，支持复杂的数据分析和模式检测。
