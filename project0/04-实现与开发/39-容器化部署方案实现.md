# 容器化部署方案实现

## 1. Docker容器化架构

### 1.1 基础镜像构建

```dockerfile
# Rust应用基础镜像
FROM rust:1.75-slim as builder

# 设置工作目录
WORKDIR /app

# 复制依赖文件
COPY Cargo.toml Cargo.lock ./
COPY src ./src

# 构建应用
RUN cargo build --release

# 运行时镜像
FROM debian:bookworm-slim

# 安装运行时依赖
RUN apt-get update && apt-get install -y \
    ca-certificates \
    libssl3 \
    && rm -rf /var/lib/apt/lists/*

# 创建应用用户
RUN useradd -r -s /bin/false iot-user

# 复制二进制文件
COPY --from=builder /app/target/release/iot-gateway /usr/local/bin/

# 设置权限
RUN chown iot-user:iot-user /usr/local/bin/iot-gateway

# 切换到应用用户
USER iot-user

# 暴露端口
EXPOSE 8080 8443 9090

# 健康检查
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# 启动命令
CMD ["iot-gateway"]
```

### 1.2 多阶段构建优化

```dockerfile
# IoT语义网关多阶段构建
FROM rust:1.75-alpine as rust-builder
WORKDIR /app
COPY gateway/ ./
RUN apk add --no-cache musl-dev openssl-dev
RUN cargo build --release --target x86_64-unknown-linux-musl

# Node.js前端构建
FROM node:18-alpine as frontend-builder
WORKDIR /app
COPY frontend/package*.json ./
RUN npm ci --only=production
COPY frontend/ ./
RUN npm run build

# Go微服务构建
FROM golang:1.21-alpine as go-builder
WORKDIR /app
COPY services/go.mod services/go.sum ./
RUN go mod download
COPY services/ ./
RUN CGO_ENABLED=0 GOOS=linux go build -o service ./cmd/main.go

# 最终运行镜像
FROM alpine:3.18
RUN apk --no-cache add ca-certificates tzdata
WORKDIR /root/

# 复制构建产物
COPY --from=rust-builder /app/target/x86_64-unknown-linux-musl/release/iot-gateway ./
COPY --from=frontend-builder /app/dist ./static/
COPY --from=go-builder /app/service ./

# 配置文件
COPY configs/ ./configs/

# 启动脚本
COPY scripts/entrypoint.sh ./
RUN chmod +x entrypoint.sh

EXPOSE 8080 8443 9090 9091

CMD ["./entrypoint.sh"]
```

### 1.3 Docker Compose编排

```yaml
# docker-compose.yml
version: '3.8'

services:
  # IoT语义网关
  iot-gateway:
    build:
      context: .
      dockerfile: Dockerfile.gateway
    ports:
      - "8080:8080"
      - "8443:8443"
    environment:
      - RUST_LOG=info
      - DATABASE_URL=postgresql://iot:password@postgres:5432/iot_db
      - REDIS_URL=redis://redis:6379
      - KAFKA_BROKERS=kafka:9092
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy
    volumes:
      - ./configs:/app/configs:ro
      - iot-logs:/app/logs
    networks:
      - iot-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # 设备管理服务
  device-manager:
    build:
      context: ./services/device-manager
      dockerfile: Dockerfile
    ports:
      - "8081:8080"
    environment:
      - DATABASE_URL=postgresql://iot:password@postgres:5432/device_db
      - MQTT_BROKER=mqtt://mosquitto:1883
    depends_on:
      - postgres
      - mosquitto
    networks:
      - iot-network
    restart: unless-stopped

  # 数据处理服务
  data-processor:
    build:
      context: ./services/data-processor
      dockerfile: Dockerfile
    environment:
      - KAFKA_BROKERS=kafka:9092
      - INFLUXDB_URL=http://influxdb:8086
      - INFLUXDB_TOKEN=${INFLUXDB_TOKEN}
    depends_on:
      - kafka
      - influxdb
    networks:
      - iot-network
    deploy:
      replicas: 3
    restart: unless-stopped

  # PostgreSQL数据库
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: iot_db
      POSTGRES_USER: iot
      POSTGRES_PASSWORD: password
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d:ro
    networks:
      - iot-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U iot -d iot_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Redis缓存
  redis:
    image: redis:7-alpine
    command: redis-server --requirepass password
    volumes:
      - redis-data:/data
    networks:
      - iot-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  # InfluxDB时序数据库
  influxdb:
    image: influxdb:2.7-alpine
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: admin
      DOCKER_INFLUXDB_INIT_PASSWORD: password
      DOCKER_INFLUXDB_INIT_ORG: iot-org
      DOCKER_INFLUXDB_INIT_BUCKET: iot-data
    volumes:
      - influxdb-data:/var/lib/influxdb2
    networks:
      - iot-network
    restart: unless-stopped

  # Kafka消息队列
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
    volumes:
      - kafka-data:/var/lib/kafka/data
    networks:
      - iot-network
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Zookeeper
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
    networks:
      - iot-network
    restart: unless-stopped

  # MQTT Broker
  mosquitto:
    image: eclipse-mosquitto:2.0
    ports:
      - "1883:1883"
      - "9001:9001"
    volumes:
      - ./mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf:ro
      - mosquitto-data:/mosquitto/data
    networks:
      - iot-network
    restart: unless-stopped

  # 监控服务
  prometheus:
    image: prom/prometheus:v2.45.0
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    networks:
      - iot-network
    restart: unless-stopped

  grafana:
    image: grafana/grafana:10.0.0
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
    networks:
      - iot-network
    restart: unless-stopped

volumes:
  postgres-data:
  redis-data:
  influxdb-data:
  kafka-data:
  zookeeper-data:
  mosquitto-data:
  prometheus-data:
  grafana-data:
  iot-logs:

networks:
  iot-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

## 2. Kubernetes部署架构

### 2.1 命名空间和资源配置

```yaml
# namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: iot-platform
  labels:
    name: iot-platform
    environment: production

---
# resource-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: iot-platform-quota
  namespace: iot-platform
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    persistentvolumeclaims: "10"
    services: "20"
    secrets: "10"
    configmaps: "20"

---
# limit-range.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: iot-platform-limits
  namespace: iot-platform
spec:
  limits:
  - default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
    type: Container
```

### 2.2 ConfigMap和Secret配置

```yaml
# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: iot-gateway-config
  namespace: iot-platform
data:
  gateway.toml: |
    [server]
    host = "0.0.0.0"
    port = 8080
    
    [database]
    host = "postgres-service"
    port = 5432
    database = "iot_db"
    
    [redis]
    host = "redis-service"
    port = 6379
    
    [kafka]
    brokers = ["kafka-service:9092"]
    
    [logging]
    level = "info"
    format = "json"

  prometheus.yml: |
    global:
      scrape_interval: 15s
    
    scrape_configs:
      - job_name: 'iot-gateway'
        static_configs:
          - targets: ['iot-gateway-service:9090']
      
      - job_name: 'device-manager'
        static_configs:
          - targets: ['device-manager-service:9090']

---
# secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: iot-platform-secrets
  namespace: iot-platform
type: Opaque
data:
  postgres-password: cGFzc3dvcmQ=  # base64 encoded 'password'
  redis-password: cGFzc3dvcmQ=
  jwt-secret: bXktand0LXNlY3JldC1rZXk=
  influxdb-token: bXktaW5mbHV4ZGItdG9rZW4=
```

### 2.3 持久化存储配置

```yaml
# storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: iot-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  fsType: ext4
  encrypted: "true"
allowVolumeExpansion: true
reclaimPolicy: Retain

---
# postgres-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: iot-platform
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: iot-ssd
  resources:
    requests:
      storage: 100Gi

---
# influxdb-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: influxdb-pvc
  namespace: iot-platform
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: iot-ssd
  resources:
    requests:
      storage: 500Gi
```

### 2.4 应用部署配置

```yaml
# iot-gateway-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: iot-gateway
  namespace: iot-platform
  labels:
    app: iot-gateway
    version: v1.0.0
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: iot-gateway
  template:
    metadata:
      labels:
        app: iot-gateway
        version: v1.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: iot-gateway-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
      containers:
      - name: iot-gateway
        image: iot-platform/gateway:v1.0.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        - containerPort: 8443
          name: https
          protocol: TCP
        - containerPort: 9090
          name: metrics
          protocol: TCP
        env:
        - name: RUST_LOG
          value: "info"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: iot-platform-secrets
              key: database-url
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: iot-platform-secrets
              key: redis-password
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
          readOnly: true
        - name: logs-volume
          mountPath: /app/logs
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 1Gi
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
      volumes:
      - name: config-volume
        configMap:
          name: iot-gateway-config
      - name: logs-volume
        emptyDir: {}
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations:
      - key: "iot-workload"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"

---
# iot-gateway-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: iot-gateway-service
  namespace: iot-platform
  labels:
    app: iot-gateway
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
    protocol: TCP
    name: http
  - port: 8443
    targetPort: 8443
    protocol: TCP
    name: https
  - port: 9090
    targetPort: 9090
    protocol: TCP
    name: metrics
  selector:
    app: iot-gateway

---
# iot-gateway-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: iot-gateway-hpa
  namespace: iot-platform
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: iot-gateway
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
```

## 3. 服务网格架构

### 3.1 Istio服务网格配置

```yaml
# istio-gateway.yaml
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: iot-platform-gateway
  namespace: iot-platform
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - iot-platform.example.com
    tls:
      httpsRedirect: true
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: iot-platform-tls
    hosts:
    - iot-platform.example.com

---
# virtual-service.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: iot-platform-vs
  namespace: iot-platform
spec:
  hosts:
  - iot-platform.example.com
  gateways:
  - iot-platform-gateway
  http:
  - match:
    - uri:
        prefix: /api/v1/devices
    route:
    - destination:
        host: device-manager-service
        port:
          number: 8080
      weight: 100
    fault:
      delay:
        percentage:
          value: 0.1
        fixedDelay: 5s
    retries:
      attempts: 3
      perTryTimeout: 2s
  - match:
    - uri:
        prefix: /api/v1/gateway
    route:
    - destination:
        host: iot-gateway-service
        port:
          number: 8080
      weight: 100
    timeout: 30s

---
# destination-rule.yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: iot-gateway-dr
  namespace: iot-platform
spec:
  host: iot-gateway-service
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 10
    loadBalancer:
      simple: LEAST_CONN
    outlierDetection:
      consecutiveErrors: 3
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
  subsets:
  - name: v1
    labels:
      version: v1.0.0
    trafficPolicy:
      circuitBreaker:
        maxConnections: 50
        maxPendingRequests: 25
        maxRetries: 3

---
# service-entry.yaml
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata:
  name: external-api
  namespace: iot-platform
spec:
  hosts:
  - external-api.example.com
  ports:
  - number: 443
    name: https
    protocol: HTTPS
  location: MESH_EXTERNAL
  resolution: DNS

---
# authorization-policy.yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: iot-platform-authz
  namespace: iot-platform
spec:
  selector:
    matchLabels:
      app: iot-gateway
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/iot-platform/sa/device-manager-sa"]
  - to:
    - operation:
        methods: ["GET", "POST"]
        paths: ["/api/v1/*"]
  - when:
    - key: request.headers[authorization]
      values: ["Bearer *"]
```

### 3.2 安全策略配置

```yaml
# peer-authentication.yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: iot-platform
spec:
  mtls:
    mode: STRICT

---
# request-authentication.yaml
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: jwt-auth
  namespace: iot-platform
spec:
  selector:
    matchLabels:
      app: iot-gateway
  jwtRules:
  - issuer: "https://auth.iot-platform.example.com"
    jwksUri: "https://auth.iot-platform.example.com/.well-known/jwks.json"
    audiences:
    - "iot-platform-api"
    forwardOriginalToken: true

---
# network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: iot-platform-netpol
  namespace: iot-platform
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: istio-system
    - namespaceSelector:
        matchLabels:
          name: iot-platform
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
  - to:
    - namespaceSelector:
        matchLabels:
          name: iot-platform
  - to: []
    ports:
    - protocol: TCP
      port: 443
```

## 4. CI/CD流水线

### 4.1 GitLab CI配置

```yaml
# .gitlab-ci.yml
stages:
  - test
  - build
  - security
  - deploy
  - verify

variables:
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: "/certs"
  REGISTRY: registry.gitlab.com/iot-platform
  KUBE_NAMESPACE: iot-platform

# 测试阶段
test:rust:
  stage: test
  image: rust:1.75-slim
  before_script:
    - apt-get update && apt-get install -y pkg-config libssl-dev
    - cargo --version
  script:
    - cargo test --verbose
    - cargo clippy -- -D warnings
    - cargo fmt --check
  coverage: '/Coverage: \d+\.\d+/'
  artifacts:
    reports:
      junit: target/test-results.xml
      coverage_report:
        coverage_format: cobertura
        path: target/coverage.xml

test:integration:
  stage: test
  image: docker/compose:latest
  services:
    - docker:dind
  script:
    - docker-compose -f docker-compose.test.yml up --build --abort-on-container-exit
    - docker-compose -f docker-compose.test.yml down

# 构建阶段
build:docker:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  before_script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
    - docker build -t $REGISTRY/iot-gateway:$CI_COMMIT_SHA .
    - docker build -t $REGISTRY/iot-gateway:latest .
    - docker push $REGISTRY/iot-gateway:$CI_COMMIT_SHA
    - docker push $REGISTRY/iot-gateway:latest
  only:
    - main
    - develop

# 安全扫描
security:container:
  stage: security
  image: aquasec/trivy:latest
  script:
    - trivy image --exit-code 1 --severity HIGH,CRITICAL $REGISTRY/iot-gateway:$CI_COMMIT_SHA
  allow_failure: true

security:sast:
  stage: security
  image: securecodewarrior/gitlab-sast:latest
  script:
    - sast-scan --language rust --output sast-report.json
  artifacts:
    reports:
      sast: sast-report.json

# 部署阶段
deploy:staging:
  stage: deploy
  image: bitnami/kubectl:latest
  environment:
    name: staging
    url: https://staging.iot-platform.example.com
  before_script:
    - kubectl config use-context $KUBE_CONTEXT_STAGING
  script:
    - kubectl set image deployment/iot-gateway iot-gateway=$REGISTRY/iot-gateway:$CI_COMMIT_SHA -n $KUBE_NAMESPACE
    - kubectl rollout status deployment/iot-gateway -n $KUBE_NAMESPACE --timeout=300s
  only:
    - develop

deploy:production:
  stage: deploy
  image: bitnami/kubectl:latest
  environment:
    name: production
    url: https://iot-platform.example.com
  before_script:
    - kubectl config use-context $KUBE_CONTEXT_PROD
  script:
    - kubectl set image deployment/iot-gateway iot-gateway=$REGISTRY/iot-gateway:$CI_COMMIT_SHA -n $KUBE_NAMESPACE
    - kubectl rollout status deployment/iot-gateway -n $KUBE_NAMESPACE --timeout=600s
  when: manual
  only:
    - main

# 验证阶段
verify:health:
  stage: verify
  image: curlimages/curl:latest
  script:
    - curl -f https://iot-platform.example.com/health
    - curl -f https://iot-platform.example.com/ready
  retry:
    max: 3
    when: script_failure
```

### 4.2 Helm Chart配置

```yaml
# Chart.yaml
apiVersion: v2
name: iot-platform
description: IoT Platform Helm Chart
version: 1.0.0
appVersion: "1.0.0"
dependencies:
  - name: postgresql
    version: 12.x.x
    repository: https://charts.bitnami.com/bitnami
  - name: redis
    version: 17.x.x
    repository: https://charts.bitnami.com/bitnami
  - name: kafka
    version: 22.x.x
    repository: https://charts.bitnami.com/bitnami

---
# values.yaml
global:
  imageRegistry: registry.gitlab.com/iot-platform
  imagePullSecrets:
    - name: gitlab-registry

iotGateway:
  image:
    repository: iot-gateway
    tag: "1.0.0"
    pullPolicy: IfNotPresent
  
  replicaCount: 3
  
  service:
    type: ClusterIP
    port: 8080
    targetPort: 8080
  
  ingress:
    enabled: true
    className: "nginx"
    annotations:
      cert-manager.io/cluster-issuer: "letsencrypt-prod"
      nginx.ingress.kubernetes.io/rate-limit: "100"
    hosts:
      - host: iot-platform.example.com
        paths:
          - path: /
            pathType: Prefix
    tls:
      - secretName: iot-platform-tls
        hosts:
          - iot-platform.example.com
  
  resources:
    limits:
      cpu: 1000m
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 256Mi
  
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80

postgresql:
  enabled: true
  auth:
    postgresPassword: "secure-password"
    database: "iot_db"
  primary:
    persistence:
      enabled: true
      size: 100Gi
      storageClass: "iot-ssd"

redis:
  enabled: true
  auth:
    enabled: true
    password: "secure-password"
  master:
    persistence:
      enabled: true
      size: 10Gi
      storageClass: "iot-ssd"

kafka:
  enabled: true
  persistence:
    enabled: true
    size: 100Gi
    storageClass: "iot-ssd"
  zookeeper:
    persistence:
      enabled: true
      size: 10Gi
      storageClass: "iot-ssd"

monitoring:
  prometheus:
    enabled: true
  grafana:
    enabled: true
    adminPassword: "admin-password"
```

## 5. 监控和日志

### 5.1 Prometheus监控配置

```yaml
# prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: iot-platform
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "/etc/prometheus/rules/*.yml"
    
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - alertmanager:9093
    
    scrape_configs:
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https
      
      - job_name: 'iot-platform-services'
        kubernetes_sd_configs:
        - role: pod
          namespaces:
            names:
            - iot-platform
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__

  alerts.yml: |
    groups:
    - name: iot-platform
      rules:
      - alert: HighCPUUsage
        expr: cpu_usage_percent > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 80% for more than 5 minutes"
      
      - alert: HighMemoryUsage
        expr: memory_usage_percent > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is above 85% for more than 5 minutes"
      
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service is down"
          description: "Service {{ $labels.instance }} is down"
```

### 5.2 日志聚合配置

```yaml
# fluentd-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: iot-platform
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>
    
    <filter kubernetes.**>
      @type kubernetes_metadata
    </filter>
    
    <filter kubernetes.**>
      @type record_transformer
      <record>
        hostname ${hostname}
        tag ${tag}
        namespace ${record["kubernetes"]["namespace_name"]}
        pod_name ${record["kubernetes"]["pod_name"]}
        container_name ${record["kubernetes"]["container_name"]}
      </record>
    </filter>
    
    <match kubernetes.**>
      @type elasticsearch
      host elasticsearch.logging.svc.cluster.local
      port 9200
      index_name iot-platform-logs
      type_name _doc
      include_tag_key true
      tag_key @log_name
      flush_interval 1s
    </match>

---
# fluentd-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: iot-platform
spec:
  selector:
    matchLabels:
      name: fluentd
  template:
    metadata:
      labels:
        name: fluentd
    spec:
      serviceAccount: fluentd
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.logging.svc.cluster.local"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config-volume
          mountPath: /fluentd/etc
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config-volume
        configMap:
          name: fluentd-config
```

## 6. 备份和恢复

### 6.1 数据备份策略

```bash
#!/bin/bash
# backup-script.sh

set -e

# 配置变量
NAMESPACE="iot-platform"
BACKUP_BUCKET="s3://iot-platform-backups"
DATE=$(date +%Y%m%d_%H%M%S)

# PostgreSQL备份
echo "Starting PostgreSQL backup..."
kubectl exec -n $NAMESPACE deployment/postgres -- pg_dump -U iot iot_db > postgres_backup_$DATE.sql
aws s3 cp postgres_backup_$DATE.sql $BACKUP_BUCKET/postgres/
rm postgres_backup_$DATE.sql

# InfluxDB备份
echo "Starting InfluxDB backup..."
kubectl exec -n $NAMESPACE deployment/influxdb -- influx backup /tmp/influx_backup_$DATE
kubectl cp $NAMESPACE/influxdb-pod:/tmp/influx_backup_$DATE ./influx_backup_$DATE
tar -czf influx_backup_$DATE.tar.gz influx_backup_$DATE
aws s3 cp influx_backup_$DATE.tar.gz $BACKUP_BUCKET/influxdb/
rm -rf influx_backup_$DATE influx_backup_$DATE.tar.gz

# Kubernetes配置备份
echo "Starting Kubernetes config backup..."
kubectl get all,pvc,configmap,secret -n $NAMESPACE -o yaml > k8s_config_$DATE.yaml
aws s3 cp k8s_config_$DATE.yaml $BACKUP_BUCKET/k8s/
rm k8s_config_$DATE.yaml

# 清理旧备份（保留30天）
aws s3 ls $BACKUP_BUCKET/ --recursive | awk '$1 < "'$(date -d '30 days ago' +%Y-%m-%d)'" {print $4}' | xargs -I {} aws s3 rm s3://iot-platform-backups/{}

echo "Backup completed successfully"
```

### 6.2 灾难恢复计划

```yaml
# disaster-recovery-plan.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: disaster-recovery-plan
  namespace: iot-platform
data:
  recovery-steps.md: |
    # IoT Platform Disaster Recovery Plan
    
    ## Recovery Time Objectives (RTO)
    - Critical services: 15 minutes
    - Non-critical services: 1 hour
    - Full system recovery: 4 hours
    
    ## Recovery Point Objectives (RPO)
    - Database: 5 minutes
    - Configuration: 1 hour
    - Logs: 15 minutes
    
    ## Recovery Procedures
    
    ### 1. Assess Damage
    ```bash
    kubectl get nodes
    kubectl get pods -n iot-platform
    kubectl get pvc -n iot-platform
    ```
    
    ### 2. Restore Data
    ```bash
    # Restore PostgreSQL
    kubectl exec -n iot-platform deployment/postgres -- psql -U iot -d iot_db < postgres_backup_latest.sql
    
    # Restore InfluxDB
    kubectl exec -n iot-platform deployment/influxdb -- influx restore /tmp/influx_backup_latest
    ```
    
    ### 3. Redeploy Services
    ```bash
    helm upgrade --install iot-platform ./helm-chart -n iot-platform
    ```
    
    ### 4. Verify Recovery
    ```bash
    curl -f https://iot-platform.example.com/health
    kubectl get pods -n iot-platform
    ```

  backup-cron.yaml: |
    apiVersion: batch/v1
    kind: CronJob
    metadata:
      name: backup-job
      namespace: iot-platform
    spec:
      schedule: "0 2 * * *"  # Daily at 2 AM
      jobTemplate:
        spec:
          template:
            spec:
              containers:
              - name: backup
                image: iot-platform/backup:latest
                command:
                - /bin/bash
                - -c
                - |
                  /scripts/backup-script.sh
                env:
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      name: aws-credentials
                      key: access-key-id
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      name: aws-credentials
                      key: secret-access-key
                volumeMounts:
                - name: backup-scripts
                  mountPath: /scripts
              volumes:
              - name: backup-scripts
                configMap:
                  name: backup-scripts
              restartPolicy: OnFailure
```

这个容器化部署方案实现提供了：

1. **Docker容器化** - 多阶段构建、优化镜像、健康检查
2. **Kubernetes编排** - 完整的K8s资源配置和自动扩缩容
3. **服务网格** - Istio流量管理、安全策略、可观测性
4. **CI/CD流水线** - 自动化测试、构建、部署和验证
5. **监控日志** - Prometheus监控、Fluentd日志聚合
6. **备份恢复** - 自动化备份策略和灾难恢复计划

系统设计注重高可用性、可扩展性和运维自动化，能够支持大规模IoT平台的生产环境部署。
