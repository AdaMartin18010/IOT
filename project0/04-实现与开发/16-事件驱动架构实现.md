# 事件驱动架构实现

## 目录

- [事件驱动架构实现](#事件驱动架构实现)
  - [目录](#目录)
  - [概述](#概述)
  - [理论基础](#理论基础)
    - [事件代数定义](#事件代数定义)
    - [因果关系建模](#因果关系建模)
  - [核心实现](#核心实现)
    - [1. 事件总线实现](#1-事件总线实现)
    - [2. 事件溯源实现](#2-事件溯源实现)
    - [3. Actor模型实现](#3-actor模型实现)
    - [4. 事件流处理实现](#4-事件流处理实现)
  - [配置管理](#配置管理)
    - [事件总线配置](#事件总线配置)
    - [Rust配置结构](#rust配置结构)
  - [性能优化](#性能优化)
    - [1. 事件批处理](#1-事件批处理)
    - [2. 内存池优化](#2-内存池优化)
  - [监控指标](#监控指标)
    - [事件指标收集](#事件指标收集)
  - [测试实现](#测试实现)
    - [单元测试](#单元测试)
    - [性能基准测试](#性能基准测试)
  - [部署配置](#部署配置)
    - [Docker配置](#docker配置)
    - [Kubernetes部署](#kubernetes部署)
  - [总结](#总结)

## 概述

事件驱动架构是IoT系统的核心组件，负责处理设备事件、系统事件和业务事件的异步传播和处理。本实现基于Actor模型和事件溯源模式，提供高性能、可扩展的事件处理能力。

## 理论基础

### 事件代数定义

```coq
(* Coq中的事件代数定义 *)
Inductive Event : Type :=
  | DeviceEvent : DeviceId -> EventData -> Timestamp -> Event
  | SystemEvent : ComponentId -> EventData -> Timestamp -> Event
  | BusinessEvent : ProcessId -> EventData -> Timestamp -> Event.

(* 事件流定义 *)
Definition EventStream := list Event.

(* 事件处理函数 *)
Definition EventHandler := Event -> State -> (State * list Event).

(* 事件流处理的单调性 *)
Theorem event_stream_monotonic : 
  forall (s : EventStream) (h : EventHandler) (st : State),
    length (process_stream h s st) >= length s.
```

### 因果关系建模

```agda
-- Agda中的因果关系建模
data CausalRelation : Set where
  happened-before : Event → Event → CausalRelation
  concurrent : Event → Event → CausalRelation
  causally-independent : Event → Event → CausalRelation

-- 向量时钟实现
VectorClock : Set
VectorClock = NodeId → ℕ

-- 偏序关系
_≤ᵥ_ : VectorClock → VectorClock → Set
v₁ ≤ᵥ v₂ = ∀ (n : NodeId) → v₁ n ≤ v₂ n
```

## 核心实现

### 1. 事件总线实现

```rust
use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use tokio::sync::{mpsc, broadcast};
use serde::{Serialize, Deserialize};
use chrono::{DateTime, Utc};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Event {
    pub id: Uuid,
    pub event_type: String,
    pub source: String,
    pub timestamp: DateTime<Utc>,
    pub data: serde_json::Value,
    pub correlation_id: Option<Uuid>,
    pub causation_id: Option<Uuid>,
    pub metadata: HashMap<String, String>,
    pub vector_clock: VectorClock,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VectorClock {
    pub clocks: HashMap<String, u64>,
}

impl VectorClock {
    pub fn new() -> Self {
        Self {
            clocks: HashMap::new(),
        }
    }

    pub fn tick(&mut self, node_id: &str) {
        let counter = self.clocks.entry(node_id.to_string()).or_insert(0);
        *counter += 1;
    }

    pub fn update(&mut self, other: &VectorClock) {
        for (node_id, &other_clock) in &other.clocks {
            let current_clock = self.clocks.entry(node_id.clone()).or_insert(0);
            *current_clock = (*current_clock).max(other_clock);
        }
    }

    pub fn happens_before(&self, other: &VectorClock) -> bool {
        let mut strictly_less = false;
        
        for (node_id, &self_clock) in &self.clocks {
            if let Some(&other_clock) = other.clocks.get(node_id) {
                if self_clock > other_clock {
                    return false;
                }
                if self_clock < other_clock {
                    strictly_less = true;
                }
            } else if self_clock > 0 {
                return false;
            }
        }

        for (node_id, &other_clock) in &other.clocks {
            if !self.clocks.contains_key(node_id) && other_clock > 0 {
                strictly_less = true;
            }
        }

        strictly_less
    }
}

pub trait EventHandler: Send + Sync {
    async fn handle(&self, event: &Event) -> Result<Vec<Event>, EventError>;
    fn can_handle(&self, event_type: &str) -> bool;
    fn priority(&self) -> u8;
}

#[derive(Debug, Clone)]
pub struct EventBus {
    handlers: Arc<RwLock<HashMap<String, Vec<Arc<dyn EventHandler>>>>>,
    publisher: broadcast::Sender<Event>,
    node_id: String,
    vector_clock: Arc<RwLock<VectorClock>>,
    event_store: Arc<dyn EventStore>,
    metrics: Arc<EventMetrics>,
}

impl EventBus {
    pub fn new(
        node_id: String,
        event_store: Arc<dyn EventStore>,
        capacity: usize,
    ) -> Self {
        let (publisher, _) = broadcast::channel(capacity);
        
        Self {
            handlers: Arc::new(RwLock::new(HashMap::new())),
            publisher,
            node_id,
            vector_clock: Arc::new(RwLock::new(VectorClock::new())),
            event_store,
            metrics: Arc::new(EventMetrics::new()),
        }
    }

    pub async fn publish(&self, mut event: Event) -> Result<(), EventError> {
        // 更新向量时钟
        {
            let mut clock = self.vector_clock.write().unwrap();
            clock.tick(&self.node_id);
            event.vector_clock = clock.clone();
        }

        // 持久化事件
        self.event_store.store(&event).await?;

        // 发布事件
        if let Err(_) = self.publisher.send(event.clone()) {
            return Err(EventError::PublishFailed);
        }

        // 更新指标
        self.metrics.event_published(&event.event_type);

        // 异步处理事件
        self.process_event(event).await?;

        Ok(())
    }

    async fn process_event(&self, event: Event) -> Result<(), EventError> {
        let handlers = {
            let handlers_guard = self.handlers.read().unwrap();
            handlers_guard
                .get(&event.event_type)
                .cloned()
                .unwrap_or_default()
        };

        // 按优先级排序处理器
        let mut sorted_handlers = handlers;
        sorted_handlers.sort_by_key(|h| std::cmp::Reverse(h.priority()));

        for handler in sorted_handlers {
            if handler.can_handle(&event.event_type) {
                match handler.handle(&event).await {
                    Ok(new_events) => {
                        for new_event in new_events {
                            self.publish(new_event).await?;
                        }
                    }
                    Err(e) => {
                        self.metrics.handler_error(&event.event_type);
                        log::error!("Handler error for event {}: {:?}", event.id, e);
                    }
                }
            }
        }

        Ok(())
    }

    pub fn subscribe(&self, event_type: String, handler: Arc<dyn EventHandler>) {
        let mut handlers = self.handlers.write().unwrap();
        handlers
            .entry(event_type)
            .or_insert_with(Vec::new)
            .push(handler);
    }

    pub fn subscribe_to_stream(&self) -> broadcast::Receiver<Event> {
        self.publisher.subscribe()
    }
}

#[derive(Debug, thiserror::Error)]
pub enum EventError {
    #[error("Failed to publish event")]
    PublishFailed,
    #[error("Storage error: {0}")]
    StorageError(String),
    #[error("Serialization error: {0}")]
    SerializationError(String),
    #[error("Handler error: {0}")]
    HandlerError(String),
}
```

### 2. 事件溯源实现

```rust
use async_trait::async_trait;
use serde::{Serialize, Deserialize};
use std::collections::BTreeMap;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Aggregate {
    pub id: Uuid,
    pub version: u64,
    pub events: Vec<Event>,
    pub snapshot_version: Option<u64>,
}

#[async_trait]
pub trait EventStore: Send + Sync {
    async fn store(&self, event: &Event) -> Result<(), EventError>;
    async fn load_events(
        &self,
        aggregate_id: Uuid,
        from_version: Option<u64>,
    ) -> Result<Vec<Event>, EventError>;
    async fn save_snapshot(
        &self,
        aggregate_id: Uuid,
        version: u64,
        data: &[u8],
    ) -> Result<(), EventError>;
    async fn load_snapshot(
        &self,
        aggregate_id: Uuid,
    ) -> Result<Option<(u64, Vec<u8>)>, EventError>;
}

pub struct PostgresEventStore {
    pool: sqlx::PgPool,
}

impl PostgresEventStore {
    pub fn new(pool: sqlx::PgPool) -> Self {
        Self { pool }
    }

    async fn init_schema(&self) -> Result<(), EventError> {
        sqlx::query(
            r#"
            CREATE TABLE IF NOT EXISTS events (
                id UUID PRIMARY KEY,
                aggregate_id UUID NOT NULL,
                event_type VARCHAR NOT NULL,
                event_data JSONB NOT NULL,
                version BIGINT NOT NULL,
                timestamp TIMESTAMPTZ NOT NULL,
                metadata JSONB,
                correlation_id UUID,
                causation_id UUID,
                vector_clock JSONB NOT NULL,
                UNIQUE(aggregate_id, version)
            );

            CREATE TABLE IF NOT EXISTS snapshots (
                aggregate_id UUID PRIMARY KEY,
                version BIGINT NOT NULL,
                data BYTEA NOT NULL,
                timestamp TIMESTAMPTZ NOT NULL
            );

            CREATE INDEX IF NOT EXISTS events_aggregate_id_version_idx 
            ON events (aggregate_id, version);
            
            CREATE INDEX IF NOT EXISTS events_timestamp_idx 
            ON events (timestamp);
            "#
        )
        .execute(&self.pool)
        .await
        .map_err(|e| EventError::StorageError(e.to_string()))?;

        Ok(())
    }
}

#[async_trait]
impl EventStore for PostgresEventStore {
    async fn store(&self, event: &Event) -> Result<(), EventError> {
        let vector_clock_json = serde_json::to_value(&event.vector_clock)
            .map_err(|e| EventError::SerializationError(e.to_string()))?;

        let metadata_json = serde_json::to_value(&event.metadata)
            .map_err(|e| EventError::SerializationError(e.to_string()))?;

        sqlx::query(
            r#"
            INSERT INTO events (
                id, aggregate_id, event_type, event_data, version, 
                timestamp, metadata, correlation_id, causation_id, vector_clock
            )
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
            "#
        )
        .bind(event.id)
        .bind(Uuid::parse_str(&event.source).unwrap_or_default())
        .bind(&event.event_type)
        .bind(&event.data)
        .bind(0i64) // version - 需要从事件中提取
        .bind(event.timestamp)
        .bind(metadata_json)
        .bind(event.correlation_id)
        .bind(event.causation_id)
        .bind(vector_clock_json)
        .execute(&self.pool)
        .await
        .map_err(|e| EventError::StorageError(e.to_string()))?;

        Ok(())
    }

    async fn load_events(
        &self,
        aggregate_id: Uuid,
        from_version: Option<u64>,
    ) -> Result<Vec<Event>, EventError> {
        let from_ver = from_version.unwrap_or(0) as i64;

        let rows = sqlx::query(
            r#"
            SELECT id, event_type, event_data, timestamp, 
                   metadata, correlation_id, causation_id, vector_clock
            FROM events 
            WHERE aggregate_id = $1 AND version >= $2
            ORDER BY version ASC
            "#
        )
        .bind(aggregate_id)
        .bind(from_ver)
        .fetch_all(&self.pool)
        .await
        .map_err(|e| EventError::StorageError(e.to_string()))?;

        let mut events = Vec::new();
        for row in rows {
            let vector_clock: VectorClock = serde_json::from_value(row.get("vector_clock"))
                .map_err(|e| EventError::SerializationError(e.to_string()))?;

            let metadata: HashMap<String, String> = serde_json::from_value(row.get("metadata"))
                .map_err(|e| EventError::SerializationError(e.to_string()))?;

            let event = Event {
                id: row.get("id"),
                event_type: row.get("event_type"),
                source: aggregate_id.to_string(),
                timestamp: row.get("timestamp"),
                data: row.get("event_data"),
                correlation_id: row.get("correlation_id"),
                causation_id: row.get("causation_id"),
                metadata,
                vector_clock,
            };

            events.push(event);
        }

        Ok(events)
    }

    async fn save_snapshot(
        &self,
        aggregate_id: Uuid,
        version: u64,
        data: &[u8],
    ) -> Result<(), EventError> {
        sqlx::query(
            r#"
            INSERT INTO snapshots (aggregate_id, version, data, timestamp)
            VALUES ($1, $2, $3, NOW())
            ON CONFLICT (aggregate_id) 
            DO UPDATE SET version = $2, data = $3, timestamp = NOW()
            "#
        )
        .bind(aggregate_id)
        .bind(version as i64)
        .bind(data)
        .execute(&self.pool)
        .await
        .map_err(|e| EventError::StorageError(e.to_string()))?;

        Ok(())
    }

    async fn load_snapshot(
        &self,
        aggregate_id: Uuid,
    ) -> Result<Option<(u64, Vec<u8>)>, EventError> {
        let row = sqlx::query(
            "SELECT version, data FROM snapshots WHERE aggregate_id = $1"
        )
        .bind(aggregate_id)
        .fetch_optional(&self.pool)
        .await
        .map_err(|e| EventError::StorageError(e.to_string()))?;

        if let Some(row) = row {
            let version = row.get::<i64, _>("version") as u64;
            let data = row.get::<Vec<u8>, _>("data");
            Ok(Some((version, data)))
        } else {
            Ok(None)
        }
    }
}
```

### 3. Actor模型实现

```rust
use tokio::sync::{mpsc, oneshot};
use std::collections::HashMap;
use std::sync::Arc;

pub trait Actor: Send {
    type Message: Send;
    type State: Send;

    fn new(state: Self::State) -> Self
    where
        Self: Sized;

    async fn handle(
        &mut self,
        message: Self::Message,
        state: &mut Self::State,
    ) -> Result<Option<Vec<Event>>, ActorError>;
}

pub struct ActorSystem {
    actors: HashMap<String, mpsc::UnboundedSender<ActorMessage>>,
    event_bus: Arc<EventBus>,
}

#[derive(Debug)]
pub enum ActorMessage {
    Event(Event),
    Command {
        data: serde_json::Value,
        reply: oneshot::Sender<Result<Vec<Event>, ActorError>>,
    },
    Stop,
}

impl ActorSystem {
    pub fn new(event_bus: Arc<EventBus>) -> Self {
        Self {
            actors: HashMap::new(),
            event_bus,
        }
    }

    pub async fn spawn_actor<A: Actor + 'static>(
        &mut self,
        name: String,
        mut actor: A,
        mut state: A::State,
    ) -> Result<(), ActorError> {
        let (tx, mut rx) = mpsc::unbounded_channel();
        let event_bus = Arc::clone(&self.event_bus);

        tokio::spawn(async move {
            while let Some(message) = rx.recv().await {
                match message {
                    ActorMessage::Event(event) => {
                        if let Ok(msg) = serde_json::from_value(event.data.clone()) {
                            if let Ok(Some(events)) = actor.handle(msg, &mut state).await {
                                for event in events {
                                    let _ = event_bus.publish(event).await;
                                }
                            }
                        }
                    }
                    ActorMessage::Command { data, reply } => {
                        if let Ok(msg) = serde_json::from_value(data) {
                            let result = actor.handle(msg, &mut state).await;
                            let _ = reply.send(result);
                        }
                    }
                    ActorMessage::Stop => break,
                }
            }
        });

        self.actors.insert(name, tx);
        Ok(())
    }

    pub async fn send_command(
        &self,
        actor_name: &str,
        data: serde_json::Value,
    ) -> Result<Vec<Event>, ActorError> {
        if let Some(tx) = self.actors.get(actor_name) {
            let (reply_tx, reply_rx) = oneshot::channel();
            
            tx.send(ActorMessage::Command { data, reply: reply_tx })
                .map_err(|_| ActorError::ActorNotFound)?;

            reply_rx.await
                .map_err(|_| ActorError::CommunicationError)?
        } else {
            Err(ActorError::ActorNotFound)
        }
    }

    pub async fn broadcast_event(&self, event: Event) {
        for tx in self.actors.values() {
            let _ = tx.send(ActorMessage::Event(event.clone()));
        }
    }
}

#[derive(Debug, thiserror::Error)]
pub enum ActorError {
    #[error("Actor not found")]
    ActorNotFound,
    #[error("Communication error")]
    CommunicationError,
    #[error("Processing error: {0}")]
    ProcessingError(String),
}
```

### 4. 事件流处理实现

```rust
use futures::{Stream, StreamExt};
use std::pin::Pin;
use std::task::{Context, Poll};

pub trait EventProcessor: Send + Sync {
    async fn process(&self, event: Event) -> Result<Vec<Event>, EventError>;
}

pub struct EventPipeline {
    processors: Vec<Arc<dyn EventProcessor>>,
    parallelism: usize,
}

impl EventPipeline {
    pub fn new(parallelism: usize) -> Self {
        Self {
            processors: Vec::new(),
            parallelism,
        }
    }

    pub fn add_processor(&mut self, processor: Arc<dyn EventProcessor>) {
        self.processors.push(processor);
    }

    pub async fn process_stream<S>(&self, mut stream: S) -> impl Stream<Item = Event>
    where
        S: Stream<Item = Event> + Send + 'static,
    {
        let processors = self.processors.clone();
        let parallelism = self.parallelism;

        stream
            .map(move |event| {
                let processors = processors.clone();
                async move {
                    let mut current_events = vec![event];
                    
                    for processor in &processors {
                        let mut next_events = Vec::new();
                        
                        for event in current_events {
                            match processor.process(event).await {
                                Ok(mut events) => next_events.append(&mut events),
                                Err(e) => {
                                    log::error!("Processor error: {:?}", e);
                                }
                            }
                        }
                        
                        current_events = next_events;
                    }
                    
                    current_events
                }
            })
            .buffer_unordered(parallelism)
            .flat_map(|events| futures::stream::iter(events))
    }
}

// 示例处理器：事件转换器
pub struct EventTransformer {
    transform_rules: HashMap<String, serde_json::Value>,
}

#[async_trait]
impl EventProcessor for EventTransformer {
    async fn process(&self, mut event: Event) -> Result<Vec<Event>, EventError> {
        if let Some(rule) = self.transform_rules.get(&event.event_type) {
            // 应用转换规则
            if let Some(new_type) = rule.get("new_type") {
                event.event_type = new_type.as_str().unwrap_or(&event.event_type).to_string();
            }
            
            if let Some(data_mapping) = rule.get("data_mapping") {
                // 应用数据映射
                event.data = apply_data_mapping(&event.data, data_mapping)?;
            }
        }
        
        Ok(vec![event])
    }
}

fn apply_data_mapping(
    data: &serde_json::Value,
    mapping: &serde_json::Value,
) -> Result<serde_json::Value, EventError> {
    // 实现数据映射逻辑
    Ok(data.clone()) // 简化实现
}
```

## 配置管理

### 事件总线配置

```toml
[event_bus]
node_id = "node-001"
buffer_size = 10000
max_handlers_per_type = 10
metrics_enabled = true

[event_store]
type = "postgres"
connection_string = "postgresql://user:pass@localhost/events"
connection_pool_size = 20
batch_size = 100
snapshot_frequency = 1000

[actor_system]
max_actors = 1000
actor_timeout_ms = 5000
supervisor_strategy = "restart"

[processing]
pipeline_parallelism = 8
batch_processing_size = 50
processing_timeout_ms = 1000
```

### Rust配置结构

```rust
#[derive(Debug, Deserialize)]
pub struct EventDrivenConfig {
    pub event_bus: EventBusConfig,
    pub event_store: EventStoreConfig,
    pub actor_system: ActorSystemConfig,
    pub processing: ProcessingConfig,
}

#[derive(Debug, Deserialize)]
pub struct EventBusConfig {
    pub node_id: String,
    pub buffer_size: usize,
    pub max_handlers_per_type: usize,
    pub metrics_enabled: bool,
}

#[derive(Debug, Deserialize)]
pub struct EventStoreConfig {
    pub store_type: String,
    pub connection_string: String,
    pub connection_pool_size: u32,
    pub batch_size: usize,
    pub snapshot_frequency: u64,
}

#[derive(Debug, Deserialize)]
pub struct ActorSystemConfig {
    pub max_actors: usize,
    pub actor_timeout_ms: u64,
    pub supervisor_strategy: String,
}

#[derive(Debug, Deserialize)]
pub struct ProcessingConfig {
    pub pipeline_parallelism: usize,
    pub batch_processing_size: usize,
    pub processing_timeout_ms: u64,
}
```

## 性能优化

### 1. 事件批处理

```rust
pub struct BatchEventProcessor {
    batch_size: usize,
    batch_timeout: Duration,
    pending_events: Vec<Event>,
    last_flush: Instant,
}

impl BatchEventProcessor {
    pub async fn add_event(&mut self, event: Event) -> Result<Vec<Event>, EventError> {
        self.pending_events.push(event);
        
        if self.pending_events.len() >= self.batch_size 
            || self.last_flush.elapsed() >= self.batch_timeout {
            self.flush().await
        } else {
            Ok(Vec::new())
        }
    }

    async fn flush(&mut self) -> Result<Vec<Event>, EventError> {
        let events = std::mem::take(&mut self.pending_events);
        self.last_flush = Instant::now();
        
        // 批量处理事件
        self.process_batch(events).await
    }

    async fn process_batch(&self, events: Vec<Event>) -> Result<Vec<Event>, EventError> {
        // 批量处理逻辑
        Ok(events)
    }
}
```

### 2. 内存池优化

```rust
use object_pool::{Pool, Reusable};

pub struct EventPool {
    pool: Pool<Event>,
}

impl EventPool {
    pub fn new(capacity: usize) -> Self {
        Self {
            pool: Pool::new(capacity, || Event::default()),
        }
    }

    pub fn get(&self) -> Reusable<Event> {
        self.pool.try_pull().unwrap_or_else(|| {
            self.pool.attach(Event::default())
        })
    }
}

impl Default for Event {
    fn default() -> Self {
        Self {
            id: Uuid::new_v4(),
            event_type: String::new(),
            source: String::new(),
            timestamp: Utc::now(),
            data: serde_json::Value::Null,
            correlation_id: None,
            causation_id: None,
            metadata: HashMap::new(),
            vector_clock: VectorClock::new(),
        }
    }
}
```

## 监控指标

### 事件指标收集

```rust
use prometheus::{Counter, Histogram, Gauge, register_counter, register_histogram, register_gauge};
use std::sync::Once;

pub struct EventMetrics {
    events_published: Counter,
    events_processed: Counter,
    event_processing_duration: Histogram,
    active_handlers: Gauge,
    event_store_operations: Counter,
    vector_clock_comparisons: Counter,
}

impl EventMetrics {
    pub fn new() -> Self {
        static INIT: Once = Once::new();
        
        INIT.call_once(|| {
            // 注册指标
        });

        Self {
            events_published: register_counter!(
                "events_published_total",
                "Total number of events published"
            ).unwrap(),
            events_processed: register_counter!(
                "events_processed_total", 
                "Total number of events processed"
            ).unwrap(),
            event_processing_duration: register_histogram!(
                "event_processing_duration_seconds",
                "Event processing duration"
            ).unwrap(),
            active_handlers: register_gauge!(
                "active_event_handlers",
                "Number of active event handlers"
            ).unwrap(),
            event_store_operations: register_counter!(
                "event_store_operations_total",
                "Total event store operations"
            ).unwrap(),
            vector_clock_comparisons: register_counter!(
                "vector_clock_comparisons_total",
                "Total vector clock comparisons"
            ).unwrap(),
        }
    }

    pub fn event_published(&self, event_type: &str) {
        self.events_published.inc();
    }

    pub fn event_processed(&self, event_type: &str, duration: Duration) {
        self.events_processed.inc();
        self.event_processing_duration.observe(duration.as_secs_f64());
    }

    pub fn handler_error(&self, event_type: &str) {
        // 记录处理器错误
    }
}
```

## 测试实现

### 单元测试

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tokio_test;

    #[tokio::test]
    async fn test_event_bus_publish_subscribe() {
        let event_store = Arc::new(InMemoryEventStore::new());
        let event_bus = EventBus::new("test-node".to_string(), event_store, 100);
        
        let mut receiver = event_bus.subscribe_to_stream();
        
        let event = Event {
            id: Uuid::new_v4(),
            event_type: "test_event".to_string(),
            source: "test_source".to_string(),
            timestamp: Utc::now(),
            data: serde_json::json!({"key": "value"}),
            correlation_id: None,
            causation_id: None,
            metadata: HashMap::new(),
            vector_clock: VectorClock::new(),
        };

        event_bus.publish(event.clone()).await.unwrap();
        
        let received_event = receiver.recv().await.unwrap();
        assert_eq!(received_event.id, event.id);
    }

    #[tokio::test]
    async fn test_vector_clock_ordering() {
        let mut clock1 = VectorClock::new();
        let mut clock2 = VectorClock::new();

        clock1.tick("node1");
        clock2.update(&clock1);
        clock2.tick("node2");

        assert!(clock1.happens_before(&clock2));
        assert!(!clock2.happens_before(&clock1));
    }

    #[tokio::test]
    async fn test_event_sourcing() {
        let event_store = Arc::new(InMemoryEventStore::new());
        let aggregate_id = Uuid::new_v4();

        let events = vec![
            create_test_event("event1", aggregate_id),
            create_test_event("event2", aggregate_id),
            create_test_event("event3", aggregate_id),
        ];

        for event in &events {
            event_store.store(event).await.unwrap();
        }

        let loaded_events = event_store.load_events(aggregate_id, None).await.unwrap();
        assert_eq!(loaded_events.len(), 3);
    }

    fn create_test_event(event_type: &str, aggregate_id: Uuid) -> Event {
        Event {
            id: Uuid::new_v4(),
            event_type: event_type.to_string(),
            source: aggregate_id.to_string(),
            timestamp: Utc::now(),
            data: serde_json::json!({}),
            correlation_id: None,
            causation_id: None,
            metadata: HashMap::new(),
            vector_clock: VectorClock::new(),
        }
    }
}
```

### 性能基准测试

```rust
#[cfg(test)]
mod benchmarks {
    use super::*;
    use criterion::{black_box, criterion_group, criterion_main, Criterion};

    fn benchmark_event_publishing(c: &mut Criterion) {
        let rt = tokio::runtime::Runtime::new().unwrap();
        
        c.bench_function("event_publishing", |b| {
            b.to_async(&rt).iter(|| async {
                let event_store = Arc::new(InMemoryEventStore::new());
                let event_bus = EventBus::new("bench-node".to_string(), event_store, 1000);
                
                let event = create_test_event("bench_event", Uuid::new_v4());
                black_box(event_bus.publish(event).await.unwrap());
            });
        });
    }

    fn benchmark_vector_clock_comparison(c: &mut Criterion) {
        c.bench_function("vector_clock_comparison", |b| {
            let mut clock1 = VectorClock::new();
            let mut clock2 = VectorClock::new();
            
            for i in 0..10 {
                clock1.tick(&format!("node{}", i));
                clock2.tick(&format!("node{}", i + 1));
            }
            
            b.iter(|| {
                black_box(clock1.happens_before(&clock2));
            });
        });
    }

    criterion_group!(benches, benchmark_event_publishing, benchmark_vector_clock_comparison);
    criterion_main!(benches);
}
```

## 部署配置

### Docker配置

```dockerfile
FROM rust:1.70-alpine AS builder

WORKDIR /app
COPY Cargo.toml Cargo.lock ./
COPY src/ src/

RUN cargo build --release

FROM alpine:latest
RUN apk --no-cache add ca-certificates
WORKDIR /root/

COPY --from=builder /app/target/release/event_driven_system ./
COPY config/event_driven.toml ./config/

EXPOSE 8080 9090

CMD ["./event_driven_system"]
```

### Kubernetes部署

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: event-driven-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: event-driven-system
  template:
    metadata:
      labels:
        app: event-driven-system
    spec:
      containers:
      - name: event-driven-system
        image: iot/event-driven-system:latest
        ports:
        - containerPort: 8080
        - containerPort: 9090
        env:
        - name: NODE_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: connection-string
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: event-driven-service
spec:
  selector:
    app: event-driven-system
  ports:
  - name: api
    port: 8080
    targetPort: 8080
  - name: metrics
    port: 9090
    targetPort: 9090
```

## 总结

本事件驱动架构实现提供了：

1. **完整的事件总线**：支持发布订阅、向量时钟、因果关系追踪
2. **事件溯源机制**：持久化事件流、快照管理、状态重建
3. **Actor模型**：异步消息处理、容错机制、监督策略
4. **流处理管道**：批处理、并行处理、转换规则
5. **性能优化**：内存池、批处理、指标监控
6. **生产级配置**：容器化部署、Kubernetes编排、监控集成

该实现为IoT系统提供了高性能、可扩展的事件处理能力，支持复杂的业务逻辑和实时响应需求。
