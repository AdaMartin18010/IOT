# 物联网数据分析实现

## 概述

物联网数据分析系统提供实时和历史数据分析能力，包括数据聚合、统计分析、异常检测和预测分析。

## 核心架构

### 1. 数据分析引擎

```rust
// 数据分析引擎核心结构
pub struct DataAnalysisEngine {
    processors: HashMap<String, Box<dyn DataProcessor>>,
    aggregators: HashMap<String, Box<dyn DataAggregator>>,
    analyzers: HashMap<String, Box<dyn DataAnalyzer>>,
    cache: Arc<RedisCache>,
}

// 数据处理器接口
pub trait DataProcessor: Send + Sync {
    fn process(&self, data: &SensorData) -> Result<ProcessedData, AnalysisError>;
    fn get_name(&self) -> &str;
}

// 数据聚合器接口
pub trait DataAggregator: Send + Sync {
    fn aggregate(&self, data_points: &[SensorData], window: TimeWindow) -> Result<AggregatedData, AnalysisError>;
    fn get_aggregation_type(&self) -> AggregationType;
}

// 数据分析器接口
pub trait DataAnalyzer: Send + Sync {
    fn analyze(&self, data: &ProcessedData) -> Result<AnalysisResult, AnalysisError>;
    fn get_analysis_type(&self) -> AnalysisType;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessedData {
    pub id: String,
    pub device_id: String,
    pub timestamp: DateTime<Utc>,
    pub values: HashMap<String, f64>,
    pub metadata: HashMap<String, String>,
    pub quality_score: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AggregatedData {
    pub id: String,
    pub device_id: String,
    pub window_start: DateTime<Utc>,
    pub window_end: DateTime<Utc>,
    pub aggregation_type: AggregationType,
    pub metrics: HashMap<String, AggregatedMetric>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AggregatedMetric {
    pub count: u64,
    pub sum: f64,
    pub min: f64,
    pub max: f64,
    pub mean: f64,
    pub std_dev: f64,
    pub median: f64,
    pub percentile_95: f64,
    pub percentile_99: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AggregationType {
    Average,
    Sum,
    Count,
    Min,
    Max,
    StandardDeviation,
    Percentile(f64),
    Custom(String),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalysisResult {
    pub id: String,
    pub analysis_type: AnalysisType,
    pub timestamp: DateTime<Utc>,
    pub insights: Vec<Insight>,
    pub anomalies: Vec<Anomaly>,
    pub predictions: Vec<Prediction>,
    pub confidence: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AnalysisType {
    TrendAnalysis,
    AnomalyDetection,
    PatternRecognition,
    PredictiveAnalysis,
    CorrelationAnalysis,
    StatisticalAnalysis,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Insight {
    pub id: String,
    pub insight_type: InsightType,
    pub description: String,
    pub value: f64,
    pub threshold: Option<f64>,
    pub severity: Severity,
    pub timestamp: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum InsightType {
    TrendUp,
    TrendDown,
    Spike,
    Drop,
    Correlation,
    Outlier,
    Seasonal,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Anomaly {
    pub id: String,
    pub anomaly_type: AnomalyType,
    pub description: String,
    pub value: f64,
    pub expected_value: f64,
    pub deviation: f64,
    pub severity: Severity,
    pub timestamp: DateTime<Utc>,
    pub confidence: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AnomalyType {
    PointAnomaly,
    ContextualAnomaly,
    CollectiveAnomaly,
    TemporalAnomaly,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Prediction {
    pub id: String,
    pub target_metric: String,
    pub predicted_value: f64,
    pub confidence_interval: (f64, f64),
    pub prediction_horizon: Duration,
    pub timestamp: DateTime<Utc>,
    pub model_info: ModelInfo,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelInfo {
    pub model_type: String,
    pub model_version: String,
    pub training_data_size: u64,
    pub accuracy: f64,
    pub last_trained: DateTime<Utc>,
}
```

### 2. 数据处理器实现

```rust
// 数据清洗处理器
pub struct DataCleaningProcessor {
    name: String,
    quality_threshold: f64,
}

impl DataProcessor for DataCleaningProcessor {
    fn process(&self, data: &SensorData) -> Result<ProcessedData, AnalysisError> {
        let mut processed_values = HashMap::new();
        let mut metadata = HashMap::new();
        
        // 数据质量检查
        let quality_score = self.calculate_quality_score(data);
        
        if quality_score < self.quality_threshold {
            return Err(AnalysisError::LowQualityData(quality_score));
        }
        
        // 异常值检测和清洗
        for (key, value) in &data.values {
            if let Ok(num_value) = value.parse::<f64>() {
                if self.is_valid_value(num_value) {
                    processed_values.insert(key.clone(), num_value);
                } else {
                    metadata.insert(format!("{}_cleaned", key), "outlier_removed".to_string());
                }
            }
        }
        
        Ok(ProcessedData {
            id: Uuid::new_v4().to_string(),
            device_id: data.device_id.clone(),
            timestamp: data.timestamp,
            values: processed_values,
            metadata,
            quality_score,
        })
    }
    
    fn get_name(&self) -> &str {
        &self.name
    }
}

impl DataCleaningProcessor {
    fn calculate_quality_score(&self, data: &SensorData) -> f64 {
        let mut score = 1.0;
        
        // 检查数据完整性
        if data.values.is_empty() {
            score *= 0.5;
        }
        
        // 检查时间戳有效性
        if data.timestamp > Utc::now() {
            score *= 0.8;
        }
        
        score
    }
    
    fn is_valid_value(&self, value: f64) -> bool {
        !value.is_nan() && !value.is_infinite() && value.abs() < 1e6
    }
}

// 数据标准化处理器
pub struct DataNormalizationProcessor {
    name: String,
    normalization_method: NormalizationMethod,
}

#[derive(Debug, Clone)]
pub enum NormalizationMethod {
    MinMax,
    ZScore,
    Robust,
    Decimal,
}

impl DataProcessor for DataNormalizationProcessor {
    fn process(&self, data: &SensorData) -> Result<ProcessedData, AnalysisError> {
        let mut normalized_values = HashMap::new();
        
        for (key, value) in &data.values {
            if let Ok(num_value) = value.parse::<f64>() {
                let normalized_value = match self.normalization_method {
                    NormalizationMethod::MinMax => self.min_max_normalize(num_value),
                    NormalizationMethod::ZScore => self.z_score_normalize(num_value),
                    NormalizationMethod::Robust => self.robust_normalize(num_value),
                    NormalizationMethod::Decimal => self.decimal_normalize(num_value),
                };
                normalized_values.insert(key.clone(), normalized_value);
            }
        }
        
        Ok(ProcessedData {
            id: Uuid::new_v4().to_string(),
            device_id: data.device_id.clone(),
            timestamp: data.timestamp,
            values: normalized_values,
            metadata: HashMap::new(),
            quality_score: 1.0,
        })
    }
    
    fn get_name(&self) -> &str {
        &self.name
    }
}

impl DataNormalizationProcessor {
    fn min_max_normalize(&self, value: f64) -> f64 {
        // 简化的Min-Max标准化
        (value - 0.0) / (100.0 - 0.0)
    }
    
    fn z_score_normalize(&self, value: f64) -> f64 {
        // 简化的Z-Score标准化
        (value - 50.0) / 10.0
    }
    
    fn robust_normalize(&self, value: f64) -> f64 {
        // 简化的Robust标准化
        (value - 50.0) / 15.0
    }
    
    fn decimal_normalize(&self, value: f64) -> f64 {
        // 十进制标准化
        value / 100.0
    }
}
```

### 3. 数据聚合器实现

```rust
// 时间窗口聚合器
pub struct TimeWindowAggregator {
    aggregation_type: AggregationType,
    window_size: Duration,
}

impl DataAggregator for TimeWindowAggregator {
    fn aggregate(&self, data_points: &[SensorData], window: TimeWindow) -> Result<AggregatedData, AnalysisError> {
        if data_points.is_empty() {
            return Err(AnalysisError::NoDataPoints);
        }
        
        let mut metrics = HashMap::new();
        
        for (metric_name, _) in &data_points[0].values {
            let values: Vec<f64> = data_points
                .iter()
                .filter_map(|dp| dp.values.get(metric_name)?.parse::<f64>().ok())
                .collect();
            
            if !values.is_empty() {
                let aggregated_metric = self.calculate_aggregated_metric(&values);
                metrics.insert(metric_name.clone(), aggregated_metric);
            }
        }
        
        Ok(AggregatedData {
            id: Uuid::new_v4().to_string(),
            device_id: data_points[0].device_id.clone(),
            window_start: window.start,
            window_end: window.end,
            aggregation_type: self.aggregation_type.clone(),
            metrics,
        })
    }
    
    fn get_aggregation_type(&self) -> AggregationType {
        self.aggregation_type.clone()
    }
}

impl TimeWindowAggregator {
    fn calculate_aggregated_metric(&self, values: &[f64]) -> AggregatedMetric {
        let count = values.len() as u64;
        let sum: f64 = values.iter().sum();
        let min = values.iter().fold(f64::INFINITY, |a, &b| a.min(b));
        let max = values.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
        let mean = sum / count as f64;
        
        let variance: f64 = values.iter()
            .map(|v| (v - mean).powi(2))
            .sum::<f64>() / count as f64;
        let std_dev = variance.sqrt();
        
        let mut sorted_values = values.to_vec();
        sorted_values.sort_by(|a, b| a.partial_cmp(b).unwrap());
        let median = if count % 2 == 0 {
            (sorted_values[count as usize / 2 - 1] + sorted_values[count as usize / 2]) / 2.0
        } else {
            sorted_values[count as usize / 2]
        };
        
        let percentile_95_idx = (count as f64 * 0.95) as usize;
        let percentile_95 = sorted_values[percentile_95_idx.min(sorted_values.len() - 1)];
        
        let percentile_99_idx = (count as f64 * 0.99) as usize;
        let percentile_99 = sorted_values[percentile_99_idx.min(sorted_values.len() - 1)];
        
        AggregatedMetric {
            count,
            sum,
            min,
            max,
            mean,
            std_dev,
            median,
            percentile_95,
            percentile_99,
        }
    }
}
```

### 4. 数据分析器实现

```rust
// 异常检测分析器
pub struct AnomalyDetectionAnalyzer {
    detection_method: AnomalyDetectionMethod,
    threshold: f64,
}

#[derive(Debug, Clone)]
pub enum AnomalyDetectionMethod {
    Statistical,
    IsolationForest,
    LocalOutlierFactor,
    OneClassSVM,
}

impl DataAnalyzer for AnomalyDetectionAnalyzer {
    fn analyze(&self, data: &ProcessedData) -> Result<AnalysisResult, AnalysisError> {
        let mut anomalies = Vec::new();
        
        for (metric_name, value) in &data.values {
            if let Some(anomaly) = self.detect_anomaly(metric_name, *value) {
                anomalies.push(anomaly);
            }
        }
        
        let insights = self.generate_insights(data);
        let predictions = Vec::new(); // 异常检测不产生预测
        
        Ok(AnalysisResult {
            id: Uuid::new_v4().to_string(),
            analysis_type: AnalysisType::AnomalyDetection,
            timestamp: Utc::now(),
            insights,
            anomalies,
            predictions,
            confidence: 0.85,
        })
    }
    
    fn get_analysis_type(&self) -> AnalysisType {
        AnalysisType::AnomalyDetection
    }
}

impl AnomalyDetectionAnalyzer {
    fn detect_anomaly(&self, metric_name: &str, value: f64) -> Option<Anomaly> {
        match self.detection_method {
            AnomalyDetectionMethod::Statistical => self.statistical_detection(metric_name, value),
            AnomalyDetectionMethod::IsolationForest => self.isolation_forest_detection(metric_name, value),
            AnomalyDetectionMethod::LocalOutlierFactor => self.lof_detection(metric_name, value),
            AnomalyDetectionMethod::OneClassSVM => self.one_class_svm_detection(metric_name, value),
        }
    }
    
    fn statistical_detection(&self, metric_name: &str, value: f64) -> Option<Anomaly> {
        // 简化的统计异常检测（基于3-sigma规则）
        let mean = 50.0; // 假设的历史均值
        let std_dev = 10.0; // 假设的历史标准差
        
        let z_score = (value - mean) / std_dev;
        
        if z_score.abs() > 3.0 {
            Some(Anomaly {
                id: Uuid::new_v4().to_string(),
                anomaly_type: AnomalyType::PointAnomaly,
                description: format!("{} 值异常: {:.2} (z-score: {:.2})", metric_name, value, z_score),
                value,
                expected_value: mean,
                deviation: z_score,
                severity: if z_score.abs() > 4.0 { Severity::Critical } else { Severity::Warning },
                timestamp: Utc::now(),
                confidence: 0.9,
            })
        } else {
            None
        }
    }
    
    fn isolation_forest_detection(&self, _metric_name: &str, _value: f64) -> Option<Anomaly> {
        // 简化的隔离森林检测
        None
    }
    
    fn lof_detection(&self, _metric_name: &str, _value: f64) -> Option<Anomaly> {
        // 简化的LOF检测
        None
    }
    
    fn one_class_svm_detection(&self, _metric_name: &str, _value: f64) -> Option<Anomaly> {
        // 简化的One-Class SVM检测
        None
    }
    
    fn generate_insights(&self, _data: &ProcessedData) -> Vec<Insight> {
        Vec::new()
    }
}

// 趋势分析器
pub struct TrendAnalysisAnalyzer {
    trend_window: Duration,
    min_data_points: usize,
}

impl DataAnalyzer for TrendAnalysisAnalyzer {
    fn analyze(&self, data: &ProcessedData) -> Result<AnalysisResult, AnalysisError> {
        let mut insights = Vec::new();
        
        for (metric_name, value) in &data.values {
            if let Some(trend_insight) = self.analyze_trend(metric_name, *value) {
                insights.push(trend_insight);
            }
        }
        
        Ok(AnalysisResult {
            id: Uuid::new_v4().to_string(),
            analysis_type: AnalysisType::TrendAnalysis,
            timestamp: Utc::now(),
            insights,
            anomalies: Vec::new(),
            predictions: Vec::new(),
            confidence: 0.8,
        })
    }
    
    fn get_analysis_type(&self) -> AnalysisType {
        AnalysisType::TrendAnalysis
    }
}

impl TrendAnalysisAnalyzer {
    fn analyze_trend(&self, metric_name: &str, value: f64) -> Option<Insight> {
        // 简化的趋势分析
        let previous_value = 45.0; // 假设的前一个值
        let change_rate = (value - previous_value) / previous_value;
        
        if change_rate.abs() > 0.1 {
            let insight_type = if change_rate > 0.0 {
                InsightType::TrendUp
            } else {
                InsightType::TrendDown
            };
            
            Some(Insight {
                id: Uuid::new_v4().to_string(),
                insight_type,
                description: format!("{} 趋势变化: {:.1}%", metric_name, change_rate * 100.0),
                value: change_rate,
                threshold: Some(0.1),
                severity: if change_rate.abs() > 0.2 { Severity::Warning } else { Severity::Info },
                timestamp: Utc::now(),
            })
        } else {
            None
        }
    }
}
```

### 5. 数据分析API

```rust
#[derive(Deserialize)]
pub struct AnalysisRequest {
    pub device_id: String,
    pub metrics: Vec<String>,
    pub analysis_type: AnalysisType,
    pub time_range: TimeRange,
    pub aggregation_window: Option<Duration>,
}

#[derive(Serialize)]
pub struct AnalysisResponse {
    pub analysis_id: String,
    pub results: AnalysisResult,
    pub processing_time: Duration,
    pub data_points_processed: u64,
}

// 数据分析API路由
pub fn analysis_routes() -> Router {
    Router::new()
        .route("/analyze", post(analyze_data))
        .route("/aggregate", post(aggregate_data))
        .route("/trends", get(get_trends))
        .route("/anomalies", get(get_anomalies))
        .route("/predictions", get(get_predictions))
        .route("/insights", get(get_insights))
}

async fn analyze_data(
    Json(request): Json<AnalysisRequest>,
    State(analysis_engine): State<Arc<DataAnalysisEngine>>,
) -> Result<Json<AnalysisResponse>, StatusCode> {
    let start_time = Instant::now();
    
    // 获取数据点
    let data_points = get_sensor_data(&request.device_id, &request.time_range).await
        .map_err(|_| StatusCode::INTERNAL_SERVER_ERROR)?;
    
    // 执行分析
    let results = analysis_engine.analyze_data(&data_points, &request.analysis_type).await
        .map_err(|_| StatusCode::INTERNAL_SERVER_ERROR)?;
    
    let processing_time = start_time.elapsed();
    
    Ok(Json(AnalysisResponse {
        analysis_id: Uuid::new_v4().to_string(),
        results,
        processing_time,
        data_points_processed: data_points.len() as u64,
    }))
}

async fn aggregate_data(
    Json(request): Json<AggregationRequest>,
    State(analysis_engine): State<Arc<DataAnalysisEngine>>,
) -> Result<Json<Vec<AggregatedData>>, StatusCode> {
    let aggregated_data = analysis_engine.aggregate_data(&request).await
        .map_err(|_| StatusCode::INTERNAL_SERVER_ERROR)?;
    
    Ok(Json(aggregated_data))
}
```

## 使用示例

### 1. 实时数据分析

```rust
#[tokio::main]
async fn main() {
    let analysis_engine = Arc::new(DataAnalysisEngine::new());
    
    // 注册数据处理器
    analysis_engine.register_processor(Box::new(DataCleaningProcessor {
        name: "data_cleaner".to_string(),
        quality_threshold: 0.8,
    })).await;
    
    analysis_engine.register_processor(Box::new(DataNormalizationProcessor {
        name: "normalizer".to_string(),
        normalization_method: NormalizationMethod::ZScore,
    })).await;
    
    // 注册分析器
    analysis_engine.register_analyzer(Box::new(AnomalyDetectionAnalyzer {
        detection_method: AnomalyDetectionMethod::Statistical,
        threshold: 3.0,
    })).await;
    
    analysis_engine.register_analyzer(Box::new(TrendAnalysisAnalyzer {
        trend_window: Duration::from_hours(1),
        min_data_points: 10,
    })).await;
    
    // 实时数据分析
    let sensor_data = SensorData {
        device_id: "device-001".to_string(),
        timestamp: Utc::now(),
        values: HashMap::from([
            ("temperature".to_string(), "25.5".to_string()),
            ("humidity".to_string(), "60.0".to_string()),
        ]),
    };
    
    let analysis_result = analysis_engine.analyze_realtime(&sensor_data).await.unwrap();
    println!("分析结果: {:?}", analysis_result);
}
```

### 2. 历史数据分析

```rust
// 历史数据分析
async fn analyze_historical_data(analysis_engine: Arc<DataAnalysisEngine>) {
    let request = AnalysisRequest {
        device_id: "device-001".to_string(),
        metrics: vec!["temperature".to_string(), "humidity".to_string()],
        analysis_type: AnalysisType::TrendAnalysis,
        time_range: TimeRange {
            start: Utc::now() - Duration::from_hours(24),
            end: Utc::now(),
        },
        aggregation_window: Some(Duration::from_hours(1)),
    };
    
    let response = analysis_engine.analyze_historical(&request).await.unwrap();
    
    println!("历史分析结果:");
    println!("处理时间: {:?}", response.processing_time);
    println!("处理数据点: {}", response.data_points_processed);
    println!("洞察数量: {}", response.results.insights.len());
    println!("异常数量: {}", response.results.anomalies.len());
}
```

### 3. 数据聚合分析

```rust
// 数据聚合分析
async fn aggregate_data_analysis(analysis_engine: Arc<DataAnalysisEngine>) {
    let aggregator = TimeWindowAggregator {
        aggregation_type: AggregationType::Average,
        window_size: Duration::from_hours(1),
    };
    
    let data_points = get_sensor_data_for_period("device-001", Duration::from_hours(24)).await;
    
    let aggregated_data = aggregator.aggregate(&data_points, TimeWindow {
        start: Utc::now() - Duration::from_hours(24),
        end: Utc::now(),
    }).unwrap();
    
    println!("聚合数据:");
    for (metric_name, metric) in &aggregated_data.metrics {
        println!("{}: 平均值={:.2}, 最大值={:.2}, 最小值={:.2}", 
                 metric_name, metric.mean, metric.max, metric.min);
    }
}
```

## 核心特性

1. **多处理器支持**: 数据清洗、标准化、转换等
2. **灵活聚合**: 时间窗口、统计聚合、自定义聚合
3. **异常检测**: 统计方法、机器学习方法
4. **趋势分析**: 趋势识别、变化率计算
5. **实时分析**: 流式数据处理
6. **历史分析**: 批量数据处理
7. **可扩展架构**: 插件式处理器和分析器
8. **API接口**: RESTful API支持

这个物联网数据分析实现提供了完整的数据处理、聚合和分析能力，支持实时和历史数据分析。
