# 网络优化策略实现

## 概述

IoT系统网络优化包含连接池管理、智能负载均衡、流量控制、网络拥塞控制等关键技术。

## 核心实现

### 1. 连接池管理

```rust
use std::collections::{HashMap, VecDeque};
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use tokio::net::TcpStream;
use tokio::time::timeout;

pub struct ConnectionPool {
    pools: Arc<Mutex<HashMap<String, Pool>>>,
    config: ConnectionPoolConfig,
}

#[derive(Debug, Clone)]
pub struct ConnectionPoolConfig {
    pub max_connections_per_host: usize,
    pub min_idle_connections: usize,
    pub max_idle_time: Duration,
    pub connection_timeout: Duration,
    pub keep_alive_timeout: Duration,
    pub enable_tcp_nodelay: bool,
}

impl Default for ConnectionPoolConfig {
    fn default() -> Self {
        Self {
            max_connections_per_host: 50,
            min_idle_connections: 5,
            max_idle_time: Duration::from_secs(60),
            connection_timeout: Duration::from_secs(10),
            keep_alive_timeout: Duration::from_secs(300),
            enable_tcp_nodelay: true,
        }
    }
}

#[derive(Debug)]
struct Pool {
    idle_connections: VecDeque<PooledConnection>,
    active_connections: usize,
    total_connections: usize,
    last_cleanup: Instant,
}

#[derive(Debug)]
struct PooledConnection {
    stream: TcpStream,
    created_at: Instant,
    last_used: Instant,
    use_count: u32,
}

impl ConnectionPool {
    pub fn new(config: ConnectionPoolConfig) -> Self {
        Self {
            pools: Arc::new(Mutex::new(HashMap::new())),
            config,
        }
    }

    pub async fn get_connection(&self, host: &str, port: u16) -> Result<ManagedConnection, NetworkError> {
        let key = format!("{}:{}", host, port);
        
        // 尝试从池中获取连接
        if let Some(conn) = self.try_get_pooled_connection(&key).await? {
            return Ok(conn);
        }

        // 创建新连接
        self.create_new_connection(host, port, key).await
    }

    async fn try_get_pooled_connection(&self, key: &str) -> Result<Option<ManagedConnection>, NetworkError> {
        let mut pools = self.pools.lock().unwrap();
        
        if let Some(pool) = pools.get_mut(key) {
            while let Some(mut pooled_conn) = pool.idle_connections.pop_front() {
                // 检查连接是否仍然有效
                if self.is_connection_valid(&pooled_conn).await {
                    pool.active_connections += 1;
                    pooled_conn.last_used = Instant::now();
                    pooled_conn.use_count += 1;

                    return Ok(Some(ManagedConnection {
                        stream: pooled_conn.stream,
                        pool: Arc::clone(&self.pools),
                        pool_key: key.to_string(),
                        created_at: pooled_conn.created_at,
                        use_count: pooled_conn.use_count,
                    }));
                }
            }
        }

        Ok(None)
    }

    async fn create_new_connection(&self, host: &str, port: u16, key: String) -> Result<ManagedConnection, NetworkError> {
        // 检查连接数限制
        {
            let pools = self.pools.lock().unwrap();
            if let Some(pool) = pools.get(&key) {
                if pool.total_connections >= self.config.max_connections_per_host {
                    return Err(NetworkError::TooManyConnections);
                }
            }
        }

        // 创建TCP连接
        let stream = timeout(
            self.config.connection_timeout,
            TcpStream::connect((host, port))
        ).await
        .map_err(|_| NetworkError::ConnectionTimeout)?
        .map_err(NetworkError::IoError)?;

        // 配置TCP选项
        if self.config.enable_tcp_nodelay {
            stream.set_nodelay(true).map_err(NetworkError::IoError)?;
        }

        // 更新池统计
        {
            let mut pools = self.pools.lock().unwrap();
            let pool = pools.entry(key.clone()).or_insert_with(|| Pool {
                idle_connections: VecDeque::new(),
                active_connections: 0,
                total_connections: 0,
                last_cleanup: Instant::now(),
            });

            pool.active_connections += 1;
            pool.total_connections += 1;
        }

        Ok(ManagedConnection {
            stream,
            pool: Arc::clone(&self.pools),
            pool_key: key,
            created_at: Instant::now(),
            use_count: 1,
        })
    }

    async fn is_connection_valid(&self, conn: &PooledConnection) -> bool {
        // 检查连接年龄
        if conn.last_used.elapsed() > self.config.max_idle_time {
            return false;
        }

        // 简单的连接可用性检查
        // 实际实现中可能需要发送ping或进行更复杂的检查
        true
    }

    pub async fn cleanup_idle_connections(&self) {
        let mut pools = self.pools.lock().unwrap();
        let now = Instant::now();

        for pool in pools.values_mut() {
            if now.duration_since(pool.last_cleanup) < Duration::from_secs(30) {
                continue;
            }

            pool.idle_connections.retain(|conn| {
                conn.last_used.elapsed() <= self.config.max_idle_time
            });

            // 保持最小空闲连接数
            while pool.idle_connections.len() < self.config.min_idle_connections {
                // 这里应该异步创建连接，简化实现
                break;
            }

            pool.last_cleanup = now;
        }
    }

    pub fn get_pool_stats(&self) -> HashMap<String, PoolStats> {
        let pools = self.pools.lock().unwrap();
        
        pools.iter().map(|(key, pool)| {
            let stats = PoolStats {
                idle_connections: pool.idle_connections.len(),
                active_connections: pool.active_connections,
                total_connections: pool.total_connections,
            };
            (key.clone(), stats)
        }).collect()
    }
}

pub struct ManagedConnection {
    stream: TcpStream,
    pool: Arc<Mutex<HashMap<String, Pool>>>,
    pool_key: String,
    created_at: Instant,
    use_count: u32,
}

impl ManagedConnection {
    pub fn stream(&mut self) -> &mut TcpStream {
        &mut self.stream
    }

    pub fn age(&self) -> Duration {
        self.created_at.elapsed()
    }

    pub fn use_count(&self) -> u32 {
        self.use_count
    }
}

impl Drop for ManagedConnection {
    fn drop(&mut self) {
        let mut pools = self.pool.lock().unwrap();
        
        if let Some(pool) = pools.get_mut(&self.pool_key) {
            pool.active_connections = pool.active_connections.saturating_sub(1);
            
            // 决定是否将连接返回池中
            if self.should_return_to_pool() {
                let pooled_conn = PooledConnection {
                    stream: unsafe { std::ptr::read(&self.stream) },
                    created_at: self.created_at,
                    last_used: Instant::now(),
                    use_count: self.use_count,
                };
                
                pool.idle_connections.push_back(pooled_conn);
            } else {
                pool.total_connections = pool.total_connections.saturating_sub(1);
            }
        }
        
        // 防止stream的drop被调用
        std::mem::forget(std::mem::replace(&mut self.stream, unsafe { std::mem::zeroed() }));
    }
}

impl ManagedConnection {
    fn should_return_to_pool(&self) -> bool {
        // 基于连接年龄、使用次数等决定是否回池
        self.age() < Duration::from_secs(300) && self.use_count < 100
    }
}

#[derive(Debug, Clone)]
pub struct PoolStats {
    pub idle_connections: usize,
    pub active_connections: usize,
    pub total_connections: usize,
}
```

### 2. 智能负载均衡

```rust
use std::collections::HashMap;
use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};
use std::sync::{Arc, RwLock};
use std::time::{Duration, Instant};

#[derive(Debug, Clone)]
pub struct Server {
    pub id: String,
    pub host: String,
    pub port: u16,
    pub weight: u32,
    pub health_check_url: Option<String>,
}

#[derive(Debug, Clone)]
pub struct ServerStats {
    pub request_count: u64,
    pub error_count: u64,
    pub avg_response_time: Duration,
    pub last_health_check: Option<Instant>,
    pub is_healthy: bool,
    pub load: f64,
}

pub struct LoadBalancer {
    servers: Arc<RwLock<Vec<Server>>>,
    stats: Arc<RwLock<HashMap<String, ServerStats>>>,
    algorithm: LoadBalancingAlgorithm,
    health_checker: Arc<HealthChecker>,
    round_robin_index: AtomicUsize,
}

#[derive(Debug, Clone)]
pub enum LoadBalancingAlgorithm {
    RoundRobin,
    WeightedRoundRobin,
    LeastConnections,
    LeastResponseTime,
    IPHash,
    Adaptive,
}

impl LoadBalancer {
    pub fn new(algorithm: LoadBalancingAlgorithm) -> Self {
        Self {
            servers: Arc::new(RwLock::new(Vec::new())),
            stats: Arc::new(RwLock::new(HashMap::new())),
            algorithm,
            health_checker: Arc::new(HealthChecker::new()),
            round_robin_index: AtomicUsize::new(0),
        }
    }

    pub fn add_server(&self, server: Server) {
        let mut servers = self.servers.write().unwrap();
        let mut stats = self.stats.write().unwrap();
        
        stats.insert(server.id.clone(), ServerStats {
            request_count: 0,
            error_count: 0,
            avg_response_time: Duration::default(),
            last_health_check: None,
            is_healthy: true,
            load: 0.0,
        });
        
        servers.push(server);
    }

    pub async fn select_server(&self, client_ip: Option<&str>) -> Option<Server> {
        let servers = self.servers.read().unwrap();
        let healthy_servers: Vec<_> = servers.iter()
            .filter(|server| self.is_server_healthy(&server.id))
            .cloned()
            .collect();

        if healthy_servers.is_empty() {
            return None;
        }

        match self.algorithm {
            LoadBalancingAlgorithm::RoundRobin => {
                self.round_robin_select(&healthy_servers)
            }
            LoadBalancingAlgorithm::WeightedRoundRobin => {
                self.weighted_round_robin_select(&healthy_servers)
            }
            LoadBalancingAlgorithm::LeastConnections => {
                self.least_connections_select(&healthy_servers)
            }
            LoadBalancingAlgorithm::LeastResponseTime => {
                self.least_response_time_select(&healthy_servers)
            }
            LoadBalancingAlgorithm::IPHash => {
                self.ip_hash_select(&healthy_servers, client_ip)
            }
            LoadBalancingAlgorithm::Adaptive => {
                self.adaptive_select(&healthy_servers)
            }
        }
    }

    fn round_robin_select(&self, servers: &[Server]) -> Option<Server> {
        if servers.is_empty() {
            return None;
        }

        let index = self.round_robin_index.fetch_add(1, Ordering::Relaxed) % servers.len();
        Some(servers[index].clone())
    }

    fn weighted_round_robin_select(&self, servers: &[Server]) -> Option<Server> {
        let total_weight: u32 = servers.iter().map(|s| s.weight).sum();
        if total_weight == 0 {
            return self.round_robin_select(servers);
        }

        let mut random_weight = rand::random::<u32>() % total_weight;
        
        for server in servers {
            if random_weight < server.weight {
                return Some(server.clone());
            }
            random_weight -= server.weight;
        }

        servers.first().cloned()
    }

    fn least_connections_select(&self, servers: &[Server]) -> Option<Server> {
        let stats = self.stats.read().unwrap();
        
        servers.iter()
            .min_by_key(|server| {
                stats.get(&server.id)
                    .map(|s| s.load)
                    .unwrap_or(0.0) as u64
            })
            .cloned()
    }

    fn least_response_time_select(&self, servers: &[Server]) -> Option<Server> {
        let stats = self.stats.read().unwrap();
        
        servers.iter()
            .min_by_key(|server| {
                stats.get(&server.id)
                    .map(|s| s.avg_response_time)
                    .unwrap_or(Duration::default())
            })
            .cloned()
    }

    fn ip_hash_select(&self, servers: &[Server], client_ip: Option<&str>) -> Option<Server> {
        if let Some(ip) = client_ip {
            use std::collections::hash_map::DefaultHasher;
            use std::hash::{Hash, Hasher};

            let mut hasher = DefaultHasher::new();
            ip.hash(&mut hasher);
            let hash = hasher.finish();
            
            let index = (hash as usize) % servers.len();
            Some(servers[index].clone())
        } else {
            self.round_robin_select(servers)
        }
    }

    fn adaptive_select(&self, servers: &[Server]) -> Option<Server> {
        let stats = self.stats.read().unwrap();
        
        // 综合考虑响应时间、错误率和负载
        servers.iter()
            .min_by_key(|server| {
                if let Some(stat) = stats.get(&server.id) {
                    let error_rate = if stat.request_count > 0 {
                        stat.error_count as f64 / stat.request_count as f64
                    } else {
                        0.0
                    };
                    
                    let score = stat.avg_response_time.as_millis() as f64 * 
                               (1.0 + error_rate * 10.0) * 
                               (1.0 + stat.load);
                    
                    score as u64
                } else {
                    0
                }
            })
            .cloned()
    }

    fn is_server_healthy(&self, server_id: &str) -> bool {
        let stats = self.stats.read().unwrap();
        stats.get(server_id)
            .map(|s| s.is_healthy)
            .unwrap_or(false)
    }

    pub fn record_request(&self, server_id: &str, response_time: Duration, success: bool) {
        let mut stats = self.stats.write().unwrap();
        
        if let Some(stat) = stats.get_mut(server_id) {
            stat.request_count += 1;
            
            if !success {
                stat.error_count += 1;
            }

            // 更新平均响应时间（指数移动平均）
            let alpha = 0.1;
            let new_avg = stat.avg_response_time.as_secs_f64() * (1.0 - alpha) + 
                         response_time.as_secs_f64() * alpha;
            stat.avg_response_time = Duration::from_secs_f64(new_avg);
        }
    }

    pub async fn start_health_checking(&self) {
        let health_checker = Arc::clone(&self.health_checker);
        let servers = Arc::clone(&self.servers);
        let stats = Arc::clone(&self.stats);

        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(30));
            
            loop {
                interval.tick().await;
                
                let server_list = servers.read().unwrap().clone();
                
                for server in server_list {
                    let is_healthy = health_checker.check_health(&server).await;
                    
                    let mut stats_guard = stats.write().unwrap();
                    if let Some(stat) = stats_guard.get_mut(&server.id) {
                        stat.is_healthy = is_healthy;
                        stat.last_health_check = Some(Instant::now());
                    }
                }
            }
        });
    }
}

pub struct HealthChecker {
    client: reqwest::Client,
}

impl HealthChecker {
    pub fn new() -> Self {
        Self {
            client: reqwest::Client::builder()
                .timeout(Duration::from_secs(5))
                .build()
                .unwrap(),
        }
    }

    pub async fn check_health(&self, server: &Server) -> bool {
        if let Some(ref health_url) = server.health_check_url {
            match self.client.get(health_url).send().await {
                Ok(response) => response.status().is_success(),
                Err(_) => false,
            }
        } else {
            // 简单的TCP连接检查
            tokio::net::TcpStream::connect((server.host.as_str(), server.port))
                .await
                .is_ok()
        }
    }
}
```

### 3. 流量控制

```rust
use std::sync::atomic::{AtomicU64, Ordering};
use std::time::{Duration, Instant};
use tokio::sync::Semaphore;

pub struct TrafficController {
    rate_limiter: TokenBucketLimiter,
    connection_limiter: Arc<Semaphore>,
    bandwidth_monitor: BandwidthMonitor,
    congestion_controller: CongestionController,
}

impl TrafficController {
    pub fn new(config: TrafficControlConfig) -> Self {
        Self {
            rate_limiter: TokenBucketLimiter::new(
                config.max_requests_per_second,
                config.burst_capacity,
            ),
            connection_limiter: Arc::new(Semaphore::new(config.max_concurrent_connections)),
            bandwidth_monitor: BandwidthMonitor::new(),
            congestion_controller: CongestionController::new(config.congestion_config),
        }
    }

    pub async fn acquire_request_permit(&self) -> Result<RequestPermit, TrafficControlError> {
        // 检查请求频率限制
        if !self.rate_limiter.try_acquire().await {
            return Err(TrafficControlError::RateLimitExceeded);
        }

        // 检查并发连接限制
        let connection_permit = self.connection_limiter
            .try_acquire()
            .map_err(|_| TrafficControlError::ConnectionLimitExceeded)?;

        // 检查拥塞控制
        if self.congestion_controller.should_throttle().await {
            return Err(TrafficControlError::NetworkCongested);
        }

        Ok(RequestPermit {
            _connection_permit: connection_permit,
            start_time: Instant::now(),
            bandwidth_monitor: Arc::new(self.bandwidth_monitor.clone()),
        })
    }

    pub async fn record_bandwidth_usage(&self, bytes: u64) {
        self.bandwidth_monitor.record_bytes(bytes).await;
    }

    pub fn get_traffic_stats(&self) -> TrafficStats {
        TrafficStats {
            current_connections: self.connection_limiter.available_permits(),
            requests_per_second: self.rate_limiter.current_rate(),
            bandwidth_usage: self.bandwidth_monitor.current_usage(),
            congestion_level: self.congestion_controller.congestion_level(),
        }
    }
}

pub struct RequestPermit {
    _connection_permit: tokio::sync::SemaphorePermit<'static>,
    start_time: Instant,
    bandwidth_monitor: Arc<BandwidthMonitor>,
}

impl Drop for RequestPermit {
    fn drop(&mut self) {
        let duration = self.start_time.elapsed();
        // 记录请求处理时间等统计信息
    }
}

pub struct TokenBucketLimiter {
    capacity: u64,
    tokens: AtomicU64,
    refill_rate: u64,
    last_refill: AtomicU64,
}

impl TokenBucketLimiter {
    pub fn new(refill_rate: u64, capacity: u64) -> Self {
        Self {
            capacity,
            tokens: AtomicU64::new(capacity),
            refill_rate,
            last_refill: AtomicU64::new(
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap()
                    .as_millis() as u64
            ),
        }
    }

    pub async fn try_acquire(&self) -> bool {
        self.refill_tokens();
        
        let current_tokens = self.tokens.load(Ordering::Acquire);
        if current_tokens > 0 {
            match self.tokens.compare_exchange_weak(
                current_tokens,
                current_tokens - 1,
                Ordering::Release,
                Ordering::Relaxed,
            ) {
                Ok(_) => true,
                Err(_) => self.try_acquire().await, // 重试
            }
        } else {
            false
        }
    }

    fn refill_tokens(&self) {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_millis() as u64;

        let last = self.last_refill.load(Ordering::Acquire);
        let elapsed = now.saturating_sub(last);

        if elapsed >= 1000 { // 至少1秒
            let tokens_to_add = (elapsed / 1000) * self.refill_rate;
            
            if self.last_refill.compare_exchange_weak(
                last, now, Ordering::Release, Ordering::Relaxed
            ).is_ok() {
                let current = self.tokens.load(Ordering::Acquire);
                let new_tokens = std::cmp::min(current + tokens_to_add, self.capacity);
                self.tokens.store(new_tokens, Ordering::Release);
            }
        }
    }

    pub fn current_rate(&self) -> f64 {
        self.tokens.load(Ordering::Relaxed) as f64
    }
}

#[derive(Clone)]
pub struct BandwidthMonitor {
    bytes_transferred: Arc<AtomicU64>,
    last_measurement: Arc<AtomicU64>,
    current_bandwidth: Arc<AtomicU64>,
}

impl BandwidthMonitor {
    pub fn new() -> Self {
        Self {
            bytes_transferred: Arc::new(AtomicU64::new(0)),
            last_measurement: Arc::new(AtomicU64::new(
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap()
                    .as_millis() as u64
            )),
            current_bandwidth: Arc::new(AtomicU64::new(0)),
        }
    }

    pub async fn record_bytes(&self, bytes: u64) {
        self.bytes_transferred.fetch_add(bytes, Ordering::Relaxed);
        self.update_bandwidth();
    }

    fn update_bandwidth(&self) {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_millis() as u64;

        let last = self.last_measurement.load(Ordering::Acquire);
        let elapsed = now.saturating_sub(last);

        if elapsed >= 1000 { // 每秒更新一次
            if self.last_measurement.compare_exchange_weak(
                last, now, Ordering::Release, Ordering::Relaxed
            ).is_ok() {
                let bytes = self.bytes_transferred.swap(0, Ordering::Relaxed);
                let bandwidth = (bytes * 1000) / elapsed; // bytes per second
                self.current_bandwidth.store(bandwidth, Ordering::Release);
            }
        }
    }

    pub fn current_usage(&self) -> u64 {
        self.current_bandwidth.load(Ordering::Relaxed)
    }
}

pub struct CongestionController {
    config: CongestionControlConfig,
    rtt_measurements: Arc<std::sync::Mutex<VecDeque<Duration>>>,
    packet_loss_rate: Arc<AtomicU64>,
    congestion_window: Arc<AtomicU64>,
}

#[derive(Debug, Clone)]
pub struct CongestionControlConfig {
    pub initial_window: u64,
    pub max_window: u64,
    pub rtt_threshold: Duration,
    pub loss_threshold: f64,
    pub slow_start_threshold: u64,
}

impl CongestionController {
    pub fn new(config: CongestionControlConfig) -> Self {
        Self {
            config,
            rtt_measurements: Arc::new(std::sync::Mutex::new(VecDeque::new())),
            packet_loss_rate: Arc::new(AtomicU64::new(0)),
            congestion_window: Arc::new(AtomicU64::new(config.initial_window)),
        }
    }

    pub async fn should_throttle(&self) -> bool {
        let current_rtt = self.average_rtt();
        let loss_rate = self.current_loss_rate();

        current_rtt > self.config.rtt_threshold || 
        loss_rate > self.config.loss_threshold
    }

    pub fn record_rtt(&self, rtt: Duration) {
        let mut measurements = self.rtt_measurements.lock().unwrap();
        measurements.push_back(rtt);
        
        if measurements.len() > 100 {
            measurements.pop_front();
        }

        // 更新拥塞窗口
        self.update_congestion_window(rtt);
    }

    fn average_rtt(&self) -> Duration {
        let measurements = self.rtt_measurements.lock().unwrap();
        if measurements.is_empty() {
            Duration::default()
        } else {
            let total: Duration = measurements.iter().sum();
            total / measurements.len() as u32
        }
    }

    fn current_loss_rate(&self) -> f64 {
        let loss_count = self.packet_loss_rate.load(Ordering::Relaxed);
        loss_count as f64 / 1000.0 // 简化计算
    }

    fn update_congestion_window(&self, rtt: Duration) {
        let current_window = self.congestion_window.load(Ordering::Relaxed);
        
        if rtt < self.config.rtt_threshold {
            // 慢启动或拥塞避免
            if current_window < self.config.slow_start_threshold {
                // 慢启动：指数增长
                let new_window = std::cmp::min(current_window * 2, self.config.max_window);
                self.congestion_window.store(new_window, Ordering::Relaxed);
            } else {
                // 拥塞避免：线性增长
                let new_window = std::cmp::min(current_window + 1, self.config.max_window);
                self.congestion_window.store(new_window, Ordering::Relaxed);
            }
        } else {
            // 拥塞检测：减半窗口
            let new_window = std::cmp::max(current_window / 2, 1);
            self.congestion_window.store(new_window, Ordering::Relaxed);
        }
    }

    pub fn congestion_level(&self) -> f64 {
        let window_ratio = self.congestion_window.load(Ordering::Relaxed) as f64 / 
                          self.config.max_window as f64;
        1.0 - window_ratio
    }
}

#[derive(Debug, Clone)]
pub struct TrafficControlConfig {
    pub max_requests_per_second: u64,
    pub burst_capacity: u64,
    pub max_concurrent_connections: usize,
    pub congestion_config: CongestionControlConfig,
}

#[derive(Debug, Clone)]
pub struct TrafficStats {
    pub current_connections: usize,
    pub requests_per_second: f64,
    pub bandwidth_usage: u64,
    pub congestion_level: f64,
}

#[derive(Debug, thiserror::Error)]
pub enum TrafficControlError {
    #[error("Rate limit exceeded")]
    RateLimitExceeded,
    #[error("Connection limit exceeded")]
    ConnectionLimitExceeded,
    #[error("Network congested")]
    NetworkCongested,
}

#[derive(Debug, thiserror::Error)]
pub enum NetworkError {
    #[error("Connection timeout")]
    ConnectionTimeout,
    #[error("Too many connections")]
    TooManyConnections,
    #[error("IO error: {0}")]
    IoError(std::io::Error),
}
```

## 配置管理

```toml
[network]
enable_connection_pooling = true
enable_load_balancing = true
enable_traffic_control = true

[connection_pool]
max_connections_per_host = 50
min_idle_connections = 5
max_idle_time_seconds = 60
connection_timeout_seconds = 10
keep_alive_timeout_seconds = 300
enable_tcp_nodelay = true

[load_balancing]
algorithm = "adaptive"  # round_robin, weighted_round_robin, least_connections, etc.
health_check_interval_seconds = 30
health_check_timeout_seconds = 5

[traffic_control]
max_requests_per_second = 1000
burst_capacity = 2000
max_concurrent_connections = 500

[congestion_control]
initial_window = 10
max_window = 1000
rtt_threshold_ms = 100
loss_threshold = 0.01
slow_start_threshold = 100
```

## 性能测试

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_connection_pool() {
        let config = ConnectionPoolConfig::default();
        let pool = ConnectionPool::new(config);

        // 测试连接获取
        let conn = pool.get_connection("localhost", 8080).await;
        assert!(conn.is_ok());
    }

    #[tokio::test]
    async fn test_load_balancer() {
        let lb = LoadBalancer::new(LoadBalancingAlgorithm::RoundRobin);
        
        lb.add_server(Server {
            id: "server1".to_string(),
            host: "localhost".to_string(),
            port: 8080,
            weight: 1,
            health_check_url: None,
        });

        let server = lb.select_server(None).await;
        assert!(server.is_some());
    }

    #[tokio::test]
    async fn test_traffic_controller() {
        let config = TrafficControlConfig {
            max_requests_per_second: 100,
            burst_capacity: 200,
            max_concurrent_connections: 50,
            congestion_config: CongestionControlConfig {
                initial_window: 10,
                max_window: 100,
                rtt_threshold: Duration::from_millis(100),
                loss_threshold: 0.01,
                slow_start_threshold: 50,
            },
        };

        let controller = TrafficController::new(config);
        
        let permit = controller.acquire_request_permit().await;
        assert!(permit.is_ok());
    }
}
```

## 总结

本网络优化策略实现提供了：

- 智能连接池管理和复用
- 多种负载均衡算法
- 全面的流量控制机制
- 网络拥塞检测和控制
- 完整的性能监控和统计
