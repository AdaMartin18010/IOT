# 实时数据流处理实现

## 概述

IoT系统实时数据流处理引擎支持大规模设备数据的实时采集、处理和分析，提供低延迟的数据处理能力。

## 核心架构

```text
实时数据流处理系统
├── 数据采集模块
│   ├── 设备数据采集
│   ├── 协议适配器
│   └── 数据缓冲
├── 流处理引擎
│   ├── 数据过滤
│   ├── 数据转换
│   ├── 数据聚合
│   └── 窗口计算
├── 存储管理模块
│   ├── 时序数据库
│   ├── 实时缓存
│   └── 批量存储
└── 分发通知模块
    ├── 实时推送
    ├── 告警通知
    └── 数据订阅
```

## 核心实现

### 1. 数据流处理引擎

```rust
use std::collections::{HashMap, VecDeque};
use std::sync::{Arc, RwLock};
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use serde::{Serialize, Deserialize};
use tokio::sync::{mpsc, broadcast};
use tokio::time::{interval, Instant};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataPoint {
    pub device_id: String,
    pub sensor_type: String,
    pub value: f64,
    pub timestamp: u64,
    pub metadata: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessedData {
    pub source: String,
    pub data_type: String,
    pub value: DataValue,
    pub timestamp: u64,
    pub tags: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum DataValue {
    Number(f64),
    Text(String),
    Boolean(bool),
}

pub struct StreamProcessor {
    input_channel: mpsc::Receiver<DataPoint>,
    output_channel: broadcast::Sender<ProcessedData>,
    processors: Vec<Box<dyn DataProcessor>>,
    windows: Arc<RwLock<HashMap<String, TimeWindow>>>,
    config: ProcessorConfig,
}

#[derive(Debug, Clone)]
pub struct ProcessorConfig {
    pub buffer_size: usize,
    pub window_size: Duration,
    pub processing_interval: Duration,
}

impl StreamProcessor {
    pub fn new(
        input_channel: mpsc::Receiver<DataPoint>,
        output_channel: broadcast::Sender<ProcessedData>,
        config: ProcessorConfig,
    ) -> Self {
        Self {
            input_channel,
            output_channel,
            processors: Vec::new(),
            windows: Arc::new(RwLock::new(HashMap::new())),
            config,
        }
    }

    pub fn add_processor(&mut self, processor: Box<dyn DataProcessor>) {
        self.processors.push(processor);
    }

    pub async fn start_processing(&mut self) {
        let mut buffer = VecDeque::with_capacity(self.config.buffer_size);
        let mut interval = interval(self.config.processing_interval);

        loop {
            tokio::select! {
                data_point = self.input_channel.recv() => {
                    if let Some(data) = data_point {
                        buffer.push_back(data);
                        
                        if buffer.len() >= self.config.buffer_size {
                            self.process_batch(&mut buffer).await;
                        }
                    }
                }
                
                _ = interval.tick() => {
                    if !buffer.is_empty() {
                        self.process_batch(&mut buffer).await;
                    }
                }
            }
        }
    }

    async fn process_batch(&self, buffer: &mut VecDeque<DataPoint>) {
        let batch: Vec<DataPoint> = buffer.drain(..).collect();
        
        for data_point in batch {
            self.update_time_window(&data_point).await;
            
            for processor in &self.processors {
                if let Some(processed) = processor.process(&data_point).await {
                    let _ = self.output_channel.send(processed);
                }
            }
        }
    }

    async fn update_time_window(&self, data_point: &DataPoint) {
        let mut windows = self.windows.write().unwrap();
        let window_key = format!("{}_{}", data_point.device_id, data_point.sensor_type);
        
        let window = windows.entry(window_key).or_insert_with(|| {
            TimeWindow::new(self.config.window_size)
        });
        
        window.add_data_point(data_point.clone());
    }
}

pub trait DataProcessor: Send + Sync {
    async fn process(&self, data_point: &DataPoint) -> Option<ProcessedData>;
    fn name(&self) -> &str;
}
```

### 2. 时间窗口管理

```rust
#[derive(Debug)]
pub struct TimeWindow {
    window_size: Duration,
    data_points: VecDeque<DataPoint>,
    last_cleanup: Instant,
}

impl TimeWindow {
    pub fn new(window_size: Duration) -> Self {
        Self {
            window_size,
            data_points: VecDeque::new(),
            last_cleanup: Instant::now(),
        }
    }

    pub fn add_data_point(&mut self, data_point: DataPoint) {
        self.data_points.push_back(data_point);
        
        if self.last_cleanup.elapsed() > Duration::from_secs(10) {
            self.cleanup_expired_data();
            self.last_cleanup = Instant::now();
        }
    }

    pub fn get_data(&self) -> Vec<DataPoint> {
        self.data_points.iter().cloned().collect()
    }

    pub fn calculate_average(&self) -> Option<f64> {
        if self.data_points.is_empty() {
            return None;
        }

        let sum: f64 = self.data_points.iter().map(|dp| dp.value).sum();
        Some(sum / self.data_points.len() as f64)
    }

    pub fn calculate_min_max(&self) -> Option<(f64, f64)> {
        if self.data_points.is_empty() {
            return None;
        }

        let values: Vec<f64> = self.data_points.iter().map(|dp| dp.value).collect();
        let min = values.iter().fold(f64::INFINITY, |a, &b| a.min(b));
        let max = values.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
        
        Some((min, max))
    }

    fn cleanup_expired_data(&mut self) {
        let now = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_secs();
        
        let cutoff_time = now - self.window_size.as_secs();
        
        self.data_points.retain(|dp| dp.timestamp >= cutoff_time);
    }
}
```

### 3. 数据过滤器

```rust
pub struct DataFilter {
    filter_rules: Vec<FilterRule>,
}

#[derive(Debug, Clone)]
pub struct FilterRule {
    pub field: String,
    pub operator: FilterOperator,
    pub value: f64,
}

#[derive(Debug, Clone)]
pub enum FilterOperator {
    GreaterThan,
    LessThan,
    Equal,
    NotEqual,
}

impl DataFilter {
    pub fn new(filter_rules: Vec<FilterRule>) -> Self {
        Self { filter_rules }
    }
}

impl DataProcessor for DataFilter {
    async fn process(&self, data_point: &DataPoint) -> Option<ProcessedData> {
        for rule in &self.filter_rules {
            if !self.apply_rule(rule, data_point) {
                return None;
            }
        }

        Some(ProcessedData {
            source: data_point.device_id.clone(),
            data_type: data_point.sensor_type.clone(),
            value: DataValue::Number(data_point.value),
            timestamp: data_point.timestamp,
            tags: data_point.metadata.clone(),
        })
    }

    fn name(&self) -> &str {
        "data_filter"
    }
}

impl DataFilter {
    fn apply_rule(&self, rule: &FilterRule, data_point: &DataPoint) -> bool {
        match &rule.operator {
            FilterOperator::GreaterThan => data_point.value > rule.value,
            FilterOperator::LessThan => data_point.value < rule.value,
            FilterOperator::Equal => (data_point.value - rule.value).abs() < f64::EPSILON,
            FilterOperator::NotEqual => (data_point.value - rule.value).abs() >= f64::EPSILON,
        }
    }
}
```

### 4. 数据聚合器

```rust
pub struct DataAggregator {
    aggregation_type: AggregationType,
    window_size: Duration,
    windows: Arc<RwLock<HashMap<String, AggregationWindow>>>,
}

#[derive(Debug, Clone)]
pub enum AggregationType {
    Average,
    Sum,
    Count,
    Min,
    Max,
}

#[derive(Debug)]
struct AggregationWindow {
    values: VecDeque<f64>,
    timestamps: VecDeque<u64>,
    window_size: Duration,
}

impl AggregationWindow {
    fn new(window_size: Duration) -> Self {
        Self {
            values: VecDeque::new(),
            timestamps: VecDeque::new(),
            window_size,
        }
    }

    fn add_value(&mut self, value: f64, timestamp: u64) {
        self.values.push_back(value);
        self.timestamps.push_back(timestamp);
        
        let cutoff_time = timestamp - self.window_size.as_secs();
        while let Some(&front_timestamp) = self.timestamps.front() {
            if front_timestamp < cutoff_time {
                self.timestamps.pop_front();
                self.values.pop_front();
            } else {
                break;
            }
        }
    }

    fn calculate(&self, agg_type: &AggregationType) -> Option<f64> {
        if self.values.is_empty() {
            return None;
        }

        match agg_type {
            AggregationType::Average => {
                let sum: f64 = self.values.iter().sum();
                Some(sum / self.values.len() as f64)
            }
            AggregationType::Sum => Some(self.values.iter().sum()),
            AggregationType::Count => Some(self.values.len() as f64),
            AggregationType::Min => self.values.iter().fold(Some(f64::INFINITY), |acc, &x| {
                Some(acc.unwrap_or(f64::INFINITY).min(x))
            }),
            AggregationType::Max => self.values.iter().fold(Some(f64::NEG_INFINITY), |acc, &x| {
                Some(acc.unwrap_or(f64::NEG_INFINITY).max(x))
            }),
        }
    }
}

impl DataAggregator {
    pub fn new(aggregation_type: AggregationType, window_size: Duration) -> Self {
        Self {
            aggregation_type,
            window_size,
            windows: Arc::new(RwLock::new(HashMap::new())),
        }
    }
}

impl DataProcessor for DataAggregator {
    async fn process(&self, data_point: &DataPoint) -> Option<ProcessedData> {
        let window_key = format!("{}_{}", data_point.device_id, data_point.sensor_type);
        
        let aggregated_value = {
            let mut windows = self.windows.write().unwrap();
            let window = windows.entry(window_key.clone()).or_insert_with(|| {
                AggregationWindow::new(self.window_size)
            });
            
            window.add_value(data_point.value, data_point.timestamp);
            window.calculate(&self.aggregation_type)
        };

        aggregated_value.map(|value| ProcessedData {
            source: data_point.device_id.clone(),
            data_type: format!("{}_{:?}", data_point.sensor_type, self.aggregation_type),
            value: DataValue::Number(value),
            timestamp: data_point.timestamp,
            tags: data_point.metadata.clone(),
        })
    }

    fn name(&self) -> &str {
        "data_aggregator"
    }
}
```

### 5. 数据采集器

```rust
pub struct DataCollector {
    output_channel: mpsc::Sender<DataPoint>,
    device_configs: HashMap<String, DeviceConfig>,
}

#[derive(Debug, Clone)]
pub struct DeviceConfig {
    pub device_id: String,
    pub protocol: String,
    pub endpoint: String,
    pub sampling_interval: Duration,
    pub sensors: Vec<SensorConfig>,
}

#[derive(Debug, Clone)]
pub struct SensorConfig {
    pub sensor_type: String,
    pub unit: String,
    pub scaling_factor: f64,
}

impl DataCollector {
    pub fn new(output_channel: mpsc::Sender<DataPoint>) -> Self {
        Self {
            output_channel,
            device_configs: HashMap::new(),
        }
    }

    pub fn add_device(&mut self, config: DeviceConfig) {
        self.device_configs.insert(config.device_id.clone(), config);
    }

    pub async fn start_collection(&self) {
        for config in self.device_configs.values() {
            let config = config.clone();
            let sender = self.output_channel.clone();
            
            tokio::spawn(async move {
                let mut interval = interval(config.sampling_interval);
                
                loop {
                    interval.tick().await;
                    
                    for sensor in &config.sensors {
                        match Self::collect_sensor_data(&config, sensor).await {
                            Ok(data_point) => {
                                let _ = sender.send(data_point).await;
                            }
                            Err(e) => {
                                eprintln!("Failed to collect data from {}: {:?}", config.device_id, e);
                            }
                        }
                    }
                }
            });
        }
    }

    async fn collect_sensor_data(
        device_config: &DeviceConfig,
        sensor_config: &SensorConfig,
    ) -> Result<DataPoint, CollectionError> {
        let raw_value = Self::simulate_sensor_reading(sensor_config).await?;
        let scaled_value = raw_value * sensor_config.scaling_factor;

        let mut metadata = HashMap::new();
        metadata.insert("unit".to_string(), sensor_config.unit.clone());
        metadata.insert("protocol".to_string(), device_config.protocol.clone());

        Ok(DataPoint {
            device_id: device_config.device_id.clone(),
            sensor_type: sensor_config.sensor_type.clone(),
            value: scaled_value,
            timestamp: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_secs(),
            metadata,
        })
    }

    async fn simulate_sensor_reading(sensor_config: &SensorConfig) -> Result<f64, CollectionError> {
        use rand::Rng;
        
        let mut rng = rand::thread_rng();
        
        let base_value = match sensor_config.sensor_type.as_str() {
            "temperature" => rng.gen_range(18.0..35.0),
            "humidity" => rng.gen_range(30.0..80.0),
            "pressure" => rng.gen_range(990.0..1030.0),
            _ => rng.gen_range(0.0..100.0),
        };

        Ok(base_value)
    }
}

#[derive(Debug, thiserror::Error)]
pub enum CollectionError {
    #[error("Network error: {0}")]
    NetworkError(String),
    #[error("Protocol error: {0}")]
    ProtocolError(String),
}
```

## 配置管理

```toml
[stream_processor]
buffer_size = 1000
window_size_seconds = 300
processing_interval_ms = 100

[data_collection]
default_sampling_interval_seconds = 30
max_concurrent_collections = 100

[aggregation]
default_window_size_seconds = 60
supported_types = ["average", "sum", "count", "min", "max"]
```

## 测试框架

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_data_filter() {
        let filter_rules = vec![
            FilterRule {
                field: "value".to_string(),
                operator: FilterOperator::GreaterThan,
                value: 10.0,
            }
        ];
        
        let filter = DataFilter::new(filter_rules);
        
        let data_point = DataPoint {
            device_id: "device1".to_string(),
            sensor_type: "temperature".to_string(),
            value: 15.0,
            timestamp: 1234567890,
            metadata: HashMap::new(),
        };
        
        let result = filter.process(&data_point).await;
        assert!(result.is_some());
    }

    #[test]
    fn test_time_window() {
        let mut window = TimeWindow::new(Duration::from_secs(60));
        
        let data_point = DataPoint {
            device_id: "device1".to_string(),
            sensor_type: "temperature".to_string(),
            value: 25.0,
            timestamp: 1234567890,
            metadata: HashMap::new(),
        };
        
        window.add_data_point(data_point);
        
        let average = window.calculate_average();
        assert_eq!(average, Some(25.0));
    }
}
```

## 部署配置

### Docker

```dockerfile
FROM rust:1.70-alpine AS builder
WORKDIR /app
COPY . .
RUN apk add --no-cache openssl-dev
RUN cargo build --release

FROM alpine:latest
RUN apk --no-cache add ca-certificates
WORKDIR /root/
COPY --from=builder /app/target/release/stream_processor ./
COPY config/stream_processor.toml ./config/
EXPOSE 8080
CMD ["./stream_processor"]
```

## 总结

本实时数据流处理实现提供了完整的数据采集、处理和分析能力，支持多种数据处理器、时间窗口管理，确保IoT系统能够高效处理大规模实时数据流。
