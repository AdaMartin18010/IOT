# 可解释AI方法与测试验证

## 1. LIME解释器实现

### 1.1 LIME核心算法

```python
import numpy as np
import lime
import lime.lime_tabular
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

class SemanticLIMEExplainer:
    def __init__(self, semantic_model, feature_names, training_data):
        self.semantic_model = semantic_model
        self.feature_names = feature_names
        self.training_data = training_data
        self.scaler = StandardScaler()
        self.scaler.fit(training_data)
        
        self.explainer = lime.lime_tabular.LimeTabularExplainer(
            training_data=self.training_data,
            feature_names=feature_names,
            class_names=['non_conflict', 'conflict'],
            mode='classification',
            kernel_width=25
        )
    
    def explain_prediction(self, instance, num_features=10, num_samples=5000):
        # 数据标准化
        normalized_instance = self.scaler.transform([instance])
        
        # 生成解释
        explanation = self.explainer.explain_instance(
            normalized_instance[0],
            self.semantic_model.predict_proba,
            num_features=num_features,
            num_samples=num_samples
        )
        
        # 提取关键特征
        key_features = explanation.as_list()
        
        # 生成语义解释
        semantic_explanation = self.generate_semantic_explanation(key_features, instance)
        
        # 计算解释质量指标
        explanation_quality = self.assess_explanation_quality(explanation, instance)
        
        return {
            'lime_explanation': explanation,
            'key_features': key_features,
            'semantic_explanation': semantic_explanation,
            'confidence_score': explanation.score,
            'explanation_quality': explanation_quality,
            'local_fidelity': self.calculate_local_fidelity(explanation, instance)
        }
    
    def generate_semantic_explanation(self, key_features, instance):
        explanation_parts = []
        
        for feature, weight in key_features:
            if weight > 0:
                explanation_parts.append(
                    f"特征 '{feature}' 支持冲突预测 (权重: {weight:.3f})"
                )
            else:
                explanation_parts.append(
                    f"特征 '{feature}' 反对冲突预测 (权重: {weight:.3f})"
                )
        
        return " | ".join(explanation_parts)
    
    def assess_explanation_quality(self, explanation, instance):
        # 计算解释的稳定性
        stability_score = self.calculate_stability(explanation, instance)
        
        # 计算解释的完整性
        completeness_score = self.calculate_completeness(explanation, instance)
        
        # 计算解释的一致性
        consistency_score = self.calculate_consistency(explanation, instance)
        
        return {
            'stability': stability_score,
            'completeness': completeness_score,
            'consistency': consistency_score,
            'overall_quality': (stability_score + completeness_score + consistency_score) / 3
        }
    
    def calculate_local_fidelity(self, explanation, instance):
        # 计算局部保真度
        local_prediction = explanation.local_pred
        actual_prediction = self.semantic_model.predict_proba([instance])[0]
        
        fidelity = 1 - np.abs(local_prediction - actual_prediction).mean()
        return fidelity
```

### 1.2 LIME稳定性测试

```python
class LIMEStabilityTester:
    def __init__(self, lime_explainer):
        self.lime_explainer = lime_explainer
    
    def test_explanation_stability(self, instance, num_runs=10):
        explanations = []
        
        for _ in range(num_runs):
            explanation = self.lime_explainer.explain_prediction(instance)
            explanations.append(explanation)
        
        # 计算特征权重的一致性
        feature_consistency = self.calculate_feature_consistency(explanations)
        
        # 计算解释顺序的一致性
        order_consistency = self.calculate_order_consistency(explanations)
        
        # 计算置信度的一致性
        confidence_consistency = self.calculate_confidence_consistency(explanations)
        
        return {
            'feature_consistency': feature_consistency,
            'order_consistency': order_consistency,
            'confidence_consistency': confidence_consistency,
            'overall_stability': (feature_consistency + order_consistency + confidence_consistency) / 3
        }
    
    def calculate_feature_consistency(self, explanations):
        # 提取所有解释中的特征
        all_features = set()
        for exp in explanations:
            all_features.update([f[0] for f in exp['key_features']])
        
        # 计算特征出现频率的一致性
        feature_frequencies = {}
        for feature in all_features:
            count = sum(1 for exp in explanations 
                       if any(f[0] == feature for f in exp['key_features']))
            feature_frequencies[feature] = count / len(explanations)
        
        # 计算一致性分数
        consistency_score = np.mean(list(feature_frequencies.values()))
        return consistency_score
```

## 2. SHAP解释器实现

### 2.1 SHAP核心算法

```python
import shap
import numpy as np
from sklearn.ensemble import RandomForestClassifier

class SemanticSHAPExplainer:
    def __init__(self, semantic_model, background_data):
        self.semantic_model = semantic_model
        self.background_data = background_data
        self.explainer = shap.TreeExplainer(semantic_model)
        
    def explain_prediction(self, instance):
        # SHAP值计算
        shap_values = self.explainer.shap_values(instance)
        
        # 特征重要性排序
        feature_importance = self.rank_features_by_importance(shap_values, self.semantic_model.feature_names)
        
        # 生成解释文本
        explanation_text = self.generate_explanation_text(feature_importance, instance)
        
        # 计算解释质量
        explanation_quality = self.assess_shap_explanation_quality(shap_values, instance)
        
        return {
            'shap_values': shap_values,
            'feature_importance': feature_importance,
            'explanation_text': explanation_text,
            'base_value': self.explainer.expected_value,
            'explanation_quality': explanation_quality
        }
    
    def explain_multiple_predictions(self, instances):
        # 批量解释
        all_shap_values = []
        all_explanations = []
        
        for instance in instances:
            explanation = self.explain_prediction(instance)
            all_shap_values.append(explanation['shap_values'])
            all_explanations.append(explanation['explanation_text'])
        
        return {
            'shap_values': np.array(all_shap_values),
            'explanations': all_explanations,
            'summary_plot_data': self.generate_summary_plot_data(all_shap_values)
        }
    
    def generate_explanation_text(self, feature_importance, instance):
        explanation_parts = []
        
        for feature, importance in feature_importance[:5]:  # 前5个重要特征
            if importance > 0:
                explanation_parts.append(
                    f"'{feature}' 对冲突预测有正面贡献 (+{importance:.3f})"
                )
            else:
                explanation_parts.append(
                    f"'{feature}' 对冲突预测有负面贡献 ({importance:.3f})"
                )
        
        return " | ".join(explanation_parts)
    
    def assess_shap_explanation_quality(self, shap_values, instance):
        # 计算SHAP解释的完整性
        completeness = self.calculate_shap_completeness(shap_values)
        
        # 计算SHAP解释的稳定性
        stability = self.calculate_shap_stability(shap_values)
        
        # 计算SHAP解释的准确性
        accuracy = self.calculate_shap_accuracy(shap_values, instance)
        
        return {
            'completeness': completeness,
            'stability': stability,
            'accuracy': accuracy,
            'overall_quality': (completeness + stability + accuracy) / 3
        }
```

### 2.2 SHAP解释质量评估

```python
class SHAPQualityAssessor:
    def __init__(self, shap_explainer):
        self.shap_explainer = shap_explainer
    
    def assess_explanation_quality(self, instances, num_samples=100):
        quality_metrics = []
        
        for instance in instances:
            explanation = self.shap_explainer.explain_prediction(instance)
            quality = explanation['explanation_quality']
            quality_metrics.append(quality)
        
        # 计算平均质量指标
        avg_completeness = np.mean([q['completeness'] for q in quality_metrics])
        avg_stability = np.mean([q['stability'] for q in quality_metrics])
        avg_accuracy = np.mean([q['accuracy'] for q in quality_metrics])
        avg_overall = np.mean([q['overall_quality'] for q in quality_metrics])
        
        return {
            'average_completeness': avg_completeness,
            'average_stability': avg_stability,
            'average_accuracy': avg_accuracy,
            'average_overall_quality': avg_overall,
            'quality_distribution': {
                'completeness': [q['completeness'] for q in quality_metrics],
                'stability': [q['stability'] for q in quality_metrics],
                'accuracy': [q['accuracy'] for q in quality_metrics],
                'overall': [q['overall_quality'] for q in quality_metrics]
            }
        }
```

## 3. 可解释性测试框架

### 3.1 测试用例生成

```python
class ExplainabilityTestFramework:
    def __init__(self, lime_explainer, shap_explainer):
        self.lime_explainer = lime_explainer
        self.shap_explainer = shap_explainer
    
    def generate_test_cases(self, num_cases=100):
        test_cases = []
        
        # 生成边界测试用例
        boundary_cases = self.generate_boundary_test_cases(num_cases // 4)
        test_cases.extend(boundary_cases)
        
        # 生成异常测试用例
        anomaly_cases = self.generate_anomaly_test_cases(num_cases // 4)
        test_cases.extend(anomaly_cases)
        
        # 生成正常测试用例
        normal_cases = self.generate_normal_test_cases(num_cases // 2)
        test_cases.extend(normal_cases)
        
        return test_cases
    
    def generate_boundary_test_cases(self, num_cases):
        boundary_cases = []
        
        # 生成特征边界值
        for feature in self.lime_explainer.feature_names:
            min_val = np.min(self.lime_explainer.training_data[:, 
                self.lime_explainer.feature_names.index(feature)])
            max_val = np.max(self.lime_explainer.training_data[:, 
                self.lime_explainer.feature_names.index(feature)])
            
            # 最小值边界
            boundary_case = np.zeros(len(self.lime_explainer.feature_names))
            boundary_case[self.lime_explainer.feature_names.index(feature)] = min_val
            boundary_cases.append(boundary_case)
            
            # 最大值边界
            boundary_case = np.zeros(len(self.lime_explainer.feature_names))
            boundary_case[self.lime_explainer.feature_names.index(feature)] = max_val
            boundary_cases.append(boundary_case)
        
        return boundary_cases[:num_cases]
    
    def generate_anomaly_test_cases(self, num_cases):
        anomaly_cases = []
        
        # 生成异常值
        for _ in range(num_cases):
            anomaly_case = np.random.normal(0, 3, len(self.lime_explainer.feature_names))
            anomaly_cases.append(anomaly_case)
        
        return anomaly_cases
    
    def generate_normal_test_cases(self, num_cases):
        # 从训练数据中随机采样
        indices = np.random.choice(len(self.lime_explainer.training_data), num_cases, replace=False)
        return self.lime_explainer.training_data[indices]
```

### 3.2 解释一致性测试

```python
class ExplanationConsistencyTester:
    def __init__(self, lime_explainer, shap_explainer):
        self.lime_explainer = lime_explainer
        self.shap_explainer = shap_explainer
    
    def test_explanation_consistency(self, test_cases):
        consistency_results = []
        
        for case in test_cases:
            # LIME解释
            lime_explanation = self.lime_explainer.explain_prediction(case)
            
            # SHAP解释
            shap_explanation = self.shap_explainer.explain_prediction(case)
            
            # 计算一致性
            consistency = self.calculate_explanation_consistency(lime_explanation, shap_explanation)
            consistency_results.append(consistency)
        
        return {
            'average_consistency': np.mean(consistency_results),
            'consistency_distribution': consistency_results,
            'consistency_std': np.std(consistency_results)
        }
    
    def calculate_explanation_consistency(self, lime_explanation, shap_explanation):
        # 提取特征重要性
        lime_features = {f[0]: abs(f[1]) for f in lime_explanation['key_features']}
        shap_features = {f[0]: abs(f[1]) for f in shap_explanation['feature_importance']}
        
        # 计算特征重要性的一致性
        all_features = set(lime_features.keys()) | set(shap_features.keys())
        
        consistency_scores = []
        for feature in all_features:
            lime_importance = lime_features.get(feature, 0)
            shap_importance = shap_features.get(feature, 0)
            
            if lime_importance > 0 and shap_importance > 0:
                # 计算相对重要性的一致性
                consistency = 1 - abs(lime_importance - shap_importance) / max(lime_importance, shap_importance)
                consistency_scores.append(consistency)
        
        return np.mean(consistency_scores) if consistency_scores else 0
```

## 4. 可视化与报告

### 4.1 解释可视化

```python
import matplotlib.pyplot as plt
import seaborn as sns

class ExplanationVisualizer:
    def __init__(self):
        self.colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    
    def visualize_lime_explanation(self, lime_explanation, save_path=None):
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # 特征重要性条形图
        features, weights = zip(*lime_explanation['key_features'])
        colors = ['green' if w > 0 else 'red' for w in weights]
        
        ax1.barh(features, weights, color=colors)
        ax1.set_title('LIME Feature Importance')
        ax1.set_xlabel('Weight')
        
        # 置信度分布
        ax2.pie([lime_explanation['confidence_score'], 
                 1 - lime_explanation['confidence_score']], 
                labels=['Confident', 'Uncertain'],
                colors=['lightblue', 'lightcoral'])
        ax2.set_title('Explanation Confidence')
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def visualize_shap_explanation(self, shap_explanation, save_path=None):
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # SHAP值瀑布图
        features = [f[0] for f in shap_explanation['feature_importance']]
        shap_values = [f[1] for f in shap_explanation['feature_importance']]
        
        ax1.barh(features, shap_values, color=self.colors[:len(features)])
        ax1.set_title('SHAP Feature Values')
        ax1.set_xlabel('SHAP Value')
        
        # 基础值vs预测值
        ax2.bar(['Base Value', 'Prediction'], 
                [shap_explanation['base_value'], 
                 shap_explanation['base_value'] + sum(shap_values)],
                color=['lightgray', 'lightblue'])
        ax2.set_title('Base Value vs Prediction')
        ax2.set_ylabel('Value')
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
```

### 4.2 解释质量报告

```python
class ExplanationQualityReporter:
    def __init__(self):
        self.report_template = """
# 可解释AI方法质量报告

## 测试概览
- 测试用例数量: {num_cases}
- 测试时间: {test_time}
- 平均解释质量: {avg_quality:.3f}

## LIME解释器性能
- 平均稳定性: {lime_stability:.3f}
- 平均完整性: {lime_completeness:.3f}
- 平均一致性: {lime_consistency:.3f}

## SHAP解释器性能
- 平均完整性: {shap_completeness:.3f}
- 平均稳定性: {shap_stability:.3f}
- 平均准确性: {shap_accuracy:.3f}

## 解释一致性
- 平均一致性: {avg_consistency:.3f}
- 一致性标准差: {consistency_std:.3f}

## 详细结果
{detailed_results}
"""
    
    def generate_report(self, test_results, save_path=None):
        report_content = self.report_template.format(
            num_cases=test_results['num_cases'],
            test_time=test_results['test_time'],
            avg_quality=test_results['average_quality'],
            lime_stability=test_results['lime_metrics']['stability'],
            lime_completeness=test_results['lime_metrics']['completeness'],
            lime_consistency=test_results['lime_metrics']['consistency'],
            shap_completeness=test_results['shap_metrics']['completeness'],
            shap_stability=test_results['shap_metrics']['stability'],
            shap_accuracy=test_results['shap_metrics']['accuracy'],
            avg_consistency=test_results['consistency']['average_consistency'],
            consistency_std=test_results['consistency']['consistency_std'],
            detailed_results=self.format_detailed_results(test_results)
        )
        
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
        
        return report_content
    
    def format_detailed_results(self, test_results):
        details = []
        
        for i, case_result in enumerate(test_results['case_results']):
            details.append(f"""
### 测试用例 {i+1}
- LIME质量: {case_result['lime_quality']:.3f}
- SHAP质量: {case_result['shap_quality']:.3f}
- 一致性: {case_result['consistency']:.3f}
""")
        
        return "\n".join(details)
```

## 5. 集成测试

```python
import pytest
import numpy as np

class TestExplainabilityMethods:
    def test_lime_explainer(self):
        # 模拟语义模型
        semantic_model = RandomForestClassifier()
        feature_names = [f'feature_{i}' for i in range(10)]
        training_data = np.random.rand(100, 10)
        
        lime_explainer = SemanticLIMEExplainer(semantic_model, feature_names, training_data)
        
        # 测试解释生成
        test_instance = np.random.rand(10)
        explanation = lime_explainer.explain_prediction(test_instance)
        
        assert 'key_features' in explanation
        assert 'semantic_explanation' in explanation
        assert 'confidence_score' in explanation
        assert len(explanation['key_features']) > 0
    
    def test_shap_explainer(self):
        # 模拟语义模型
        semantic_model = RandomForestClassifier()
        background_data = np.random.rand(100, 10)
        
        shap_explainer = SemanticSHAPExplainer(semantic_model, background_data)
        
        # 测试解释生成
        test_instance = np.random.rand(10)
        explanation = shap_explainer.explain_prediction(test_instance)
        
        assert 'shap_values' in explanation
        assert 'feature_importance' in explanation
        assert 'explanation_text' in explanation
        assert len(explanation['feature_importance']) > 0
    
    def test_explanation_consistency(self):
        # 测试LIME和SHAP解释的一致性
        lime_explainer = mock_lime_explainer()
        shap_explainer = mock_shap_explainer()
        
        consistency_tester = ExplanationConsistencyTester(lime_explainer, shap_explainer)
        test_cases = np.random.rand(10, 10)
        
        consistency_result = consistency_tester.test_explanation_consistency(test_cases)
        
        assert 'average_consistency' in consistency_result
        assert consistency_result['average_consistency'] >= 0
        assert consistency_result['average_consistency'] <= 1
```

这个文档提供了可解释AI方法与测试验证的完整实现，包括LIME和SHAP解释器、测试框架、可视化、质量评估等核心功能。
