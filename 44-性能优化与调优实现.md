# IoT系统性能优化与调优详细实现

## 1. 性能分析框架

### 1.1 性能指标体系
```
系统层指标 → 应用层指标 → 业务层指标
• CPU/内存    • 响应时间   • 设备吞吐量
• 网络I/O     • 并发数     • 消息处理率
• 磁盘I/O     • 错误率     • 语义转换率
```

### 1.2 性能监控架构
- **实时监控**: Prometheus + Grafana
- **链路追踪**: Jaeger + OpenTelemetry  
- **性能分析**: perf + flame graphs
- **压力测试**: JMeter + K6

## 2. 核心组件性能优化

### 2.1 网关性能优化
```rust
// src/performance/gateway_optimizer.rs
use tokio::sync::{RwLock, Semaphore};
use std::sync::Arc;
use dashmap::DashMap;
use lru::LruCache;

pub struct OptimizedGateway {
    // 连接池管理
    connection_pools: Arc<DashMap<String, ConnectionPool>>,
    
    // 消息缓存
    message_cache: Arc<RwLock<LruCache<String, CachedMessage>>>,
    
    // 并发控制
    concurrency_limiter: Arc<Semaphore>,
    
    // 性能指标
    metrics: Arc<PerformanceMetrics>,
    
    // 优化配置
    config: OptimizationConfig,
}

#[derive(Debug, Clone)]
pub struct OptimizationConfig {
    pub max_connections_per_protocol: usize,
    pub message_cache_size: usize,
    pub max_concurrent_requests: usize,
    pub batch_processing_size: usize,
    pub compression_enabled: bool,
    pub keep_alive_timeout: std::time::Duration,
}

#[derive(Debug, Clone)]
pub struct CachedMessage {
    pub content: Vec<u8>,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub access_count: u32,
}

impl OptimizedGateway {
    pub fn new(config: OptimizationConfig) -> Self {
        OptimizedGateway {
            connection_pools: Arc::new(DashMap::new()),
            message_cache: Arc::new(RwLock::new(LruCache::new(config.message_cache_size))),
            concurrency_limiter: Arc::new(Semaphore::new(config.max_concurrent_requests)),
            metrics: Arc::new(PerformanceMetrics::new()),
            config,
        }
    }
    
    pub async fn process_message_optimized(&self, message: IncomingMessage) -> Result<ProcessedMessage, GatewayError> {
        // 获取并发许可
        let _permit = self.concurrency_limiter.acquire().await?;
        
        let start_time = std::time::Instant::now();
        
        // 检查缓存
        if let Some(cached) = self.check_message_cache(&message.id).await {
            self.metrics.cache_hits.inc();
            return Ok(cached);
        }
        
        // 批量处理优化
        let result = if self.should_batch_process(&message) {
            self.batch_process_message(message).await?
        } else {
            self.single_process_message(message).await?
        };
        
        // 更新缓存
        self.update_message_cache(&result).await;
        
        // 记录性能指标
        let duration = start_time.elapsed();
        self.metrics.processing_time.observe(duration.as_secs_f64());
        self.metrics.messages_processed.inc();
        
        Ok(result)
    }
    
    async fn check_message_cache(&self, message_id: &str) -> Option<ProcessedMessage> {
        let cache = self.message_cache.read().await;
        cache.peek(message_id).map(|cached| {
            ProcessedMessage {
                id: message_id.to_string(),
                content: cached.content.clone(),
                processed_at: cached.timestamp,
            }
        })
    }
    
    async fn update_message_cache(&self, message: &ProcessedMessage) {
        let mut cache = self.message_cache.write().await;
        cache.put(message.id.clone(), CachedMessage {
            content: message.content.clone(),
            timestamp: message.processed_at,
            access_count: 1,
        });
    }
    
    fn should_batch_process(&self, message: &IncomingMessage) -> bool {
        message.size < self.config.batch_processing_size &&
        message.priority == MessagePriority::Normal
    }
    
    async fn batch_process_message(&self, message: IncomingMessage) -> Result<ProcessedMessage, GatewayError> {
        // 实现批量处理逻辑
        // 收集相似消息一起处理以提高效率
        
        let processed = ProcessedMessage {
            id: message.id,
            content: self.optimize_message_content(message.payload)?,
            processed_at: chrono::Utc::now(),
        };
        
        Ok(processed)
    }
    
    async fn single_process_message(&self, message: IncomingMessage) -> Result<ProcessedMessage, GatewayError> {
        // 单独处理高优先级或大消息
        let processed = ProcessedMessage {
            id: message.id,
            content: self.optimize_message_content(message.payload)?,
            processed_at: chrono::Utc::now(),
        };
        
        Ok(processed)
    }
    
    fn optimize_message_content(&self, payload: Vec<u8>) -> Result<Vec<u8>, GatewayError> {
        if self.config.compression_enabled && payload.len() > 1024 {
            // 压缩大消息
            self.compress_payload(payload)
        } else {
            Ok(payload)
        }
    }
    
    fn compress_payload(&self, payload: Vec<u8>) -> Result<Vec<u8>, GatewayError> {
        use flate2::write::GzEncoder;
        use flate2::Compression;
        use std::io::Write;
        
        let mut encoder = GzEncoder::new(Vec::new(), Compression::default());
        encoder.write_all(&payload)?;
        Ok(encoder.finish()?)
    }
}

// 连接池优化
pub struct ConnectionPool {
    protocol: String,
    active_connections: Arc<DashMap<String, Connection>>,
    max_connections: usize,
    connection_timeout: std::time::Duration,
}

impl ConnectionPool {
    pub fn new(protocol: String, max_connections: usize) -> Self {
        ConnectionPool {
            protocol,
            active_connections: Arc::new(DashMap::new()),
            max_connections,
            connection_timeout: std::time::Duration::from_secs(30),
        }
    }
    
    pub async fn get_connection(&self, endpoint: &str) -> Result<Connection, PoolError> {
        // 检查现有连接
        if let Some(conn) = self.active_connections.get(endpoint) {
            if conn.is_alive().await {
                return Ok(conn.clone());
            } else {
                self.active_connections.remove(endpoint);
            }
        }
        
        // 检查连接数限制
        if self.active_connections.len() >= self.max_connections {
            return Err(PoolError::MaxConnectionsReached);
        }
        
        // 创建新连接
        let conn = self.create_connection(endpoint).await?;
        self.active_connections.insert(endpoint.to_string(), conn.clone());
        
        Ok(conn)
    }
    
    async fn create_connection(&self, endpoint: &str) -> Result<Connection, PoolError> {
        match self.protocol.as_str() {
            "mqtt" => self.create_mqtt_connection(endpoint).await,
            "coap" => self.create_coap_connection(endpoint).await,
            "http" => self.create_http_connection(endpoint).await,
            _ => Err(PoolError::UnsupportedProtocol),
        }
    }
    
    async fn create_mqtt_connection(&self, endpoint: &str) -> Result<Connection, PoolError> {
        // MQTT连接优化
        let mut opts = rumqttc::MqttOptions::new("gateway", endpoint, 1883);
        opts.set_keep_alive(std::time::Duration::from_secs(60));
        opts.set_max_packet_size(1024 * 1024, 1024 * 1024); // 1MB
        opts.set_connection_timeout(self.connection_timeout);
        
        let (client, eventloop) = rumqttc::AsyncClient::new(opts, 100);
        
        // 启动事件循环
        tokio::spawn(async move {
            let mut eventloop = eventloop;
            loop {
                if let Err(e) = eventloop.poll().await {
                    eprintln!("MQTT连接错误: {}", e);
                    break;
                }
            }
        });
        
        Ok(Connection::Mqtt(client))
    }
    
    async fn create_coap_connection(&self, _endpoint: &str) -> Result<Connection, PoolError> {
        // CoAP连接实现
        Ok(Connection::CoAP(CoAPClient::new()))
    }
    
    async fn create_http_connection(&self, _endpoint: &str) -> Result<Connection, PoolError> {
        // HTTP连接池
        let client = reqwest::Client::builder()
            .timeout(self.connection_timeout)
            .pool_max_idle_per_host(10)
            .pool_idle_timeout(std::time::Duration::from_secs(30))
            .build()?;
        
        Ok(Connection::Http(client))
    }
}
```

### 2.2 语义引擎优化
```rust
// src/performance/semantic_optimizer.rs
use rayon::prelude::*;
use std::sync::Arc;
use tokio::sync::RwLock;

pub struct OptimizedSemanticEngine {
    // 推理缓存
    inference_cache: Arc<RwLock<InferenceCache>>,
    
    // 并行处理池
    thread_pool: rayon::ThreadPool,
    
    // 预编译规则
    compiled_rules: Arc<CompiledRuleSet>,
    
    // 性能配置
    config: SemanticOptimizationConfig,
}

#[derive(Debug, Clone)]
pub struct SemanticOptimizationConfig {
    pub cache_size: usize,
    pub parallel_inference_threshold: usize,
    pub rule_compilation_enabled: bool,
    pub incremental_reasoning: bool,
    pub memory_limit_mb: usize,
}

pub struct InferenceCache {
    cache: lru::LruCache<String, InferenceResult>,
    hit_count: u64,
    miss_count: u64,
}

impl OptimizedSemanticEngine {
    pub fn new(config: SemanticOptimizationConfig) -> Result<Self, SemanticError> {
        let thread_pool = rayon::ThreadPoolBuilder::new()
            .num_threads(num_cpus::get())
            .build()?;
        
        Ok(OptimizedSemanticEngine {
            inference_cache: Arc::new(RwLock::new(InferenceCache::new(config.cache_size))),
            thread_pool,
            compiled_rules: Arc::new(CompiledRuleSet::new()),
            config,
        })
    }
    
    pub async fn process_semantic_data_optimized(&self, data: SemanticData) -> Result<InferenceResult, SemanticError> {
        let cache_key = self.generate_cache_key(&data);
        
        // 检查缓存
        if let Some(cached_result) = self.check_cache(&cache_key).await {
            return Ok(cached_result);
        }
        
        // 选择处理策略
        let result = if data.complexity_score > self.config.parallel_inference_threshold {
            self.parallel_inference(data).await?
        } else {
            self.sequential_inference(data).await?
        };
        
        // 更新缓存
        self.update_cache(cache_key, &result).await;
        
        Ok(result)
    }
    
    async fn parallel_inference(&self, data: SemanticData) -> Result<InferenceResult, SemanticError> {
        let rules = self.compiled_rules.clone();
        let data_chunks = self.partition_data(data);
        
        // 并行处理数据块
        let partial_results: Vec<PartialResult> = data_chunks
            .into_par_iter()
            .map(|chunk| {
                self.process_data_chunk(chunk, &rules)
            })
            .collect::<Result<Vec<_>, _>>()?;
        
        // 合并结果
        self.merge_partial_results(partial_results)
    }
    
    async fn sequential_inference(&self, data: SemanticData) -> Result<InferenceResult, SemanticError> {
        // 使用预编译规则进行快速推理
        self.compiled_rules.apply_to_data(data)
    }
    
    fn partition_data(&self, data: SemanticData) -> Vec<SemanticDataChunk> {
        // 智能分区策略
        let chunk_size = (data.entities.len() / num_cpus::get()).max(1);
        
        data.entities
            .chunks(chunk_size)
            .enumerate()
            .map(|(index, entities)| SemanticDataChunk {
                id: index,
                entities: entities.to_vec(),
                context: data.context.clone(),
            })
            .collect()
    }
    
    fn process_data_chunk(&self, chunk: SemanticDataChunk, rules: &CompiledRuleSet) -> Result<PartialResult, SemanticError> {
        // 在工作线程中处理数据块
        let start = std::time::Instant::now();
        
        let inferences = rules.apply_to_chunk(&chunk)?;
        let processing_time = start.elapsed();
        
        Ok(PartialResult {
            chunk_id: chunk.id,
            inferences,
            processing_time,
        })
    }
    
    fn merge_partial_results(&self, partial_results: Vec<PartialResult>) -> Result<InferenceResult, SemanticError> {
        let mut merged_inferences = Vec::new();
        let mut total_processing_time = std::time::Duration::new(0, 0);
        
        for partial in partial_results {
            merged_inferences.extend(partial.inferences);
            total_processing_time += partial.processing_time;
        }
        
        // 去重和一致性检查
        merged_inferences = self.deduplicate_inferences(merged_inferences);
        
        Ok(InferenceResult {
            inferences: merged_inferences,
            processing_time: total_processing_time,
            confidence_score: self.calculate_confidence(&merged_inferences),
        })
    }
    
    fn deduplicate_inferences(&self, mut inferences: Vec<Inference>) -> Vec<Inference> {
        inferences.sort_by(|a, b| a.subject.cmp(&b.subject));
        inferences.dedup_by(|a, b| a.subject == b.subject && a.predicate == b.predicate);
        inferences
    }
    
    async fn check_cache(&self, cache_key: &str) -> Option<InferenceResult> {
        let mut cache = self.inference_cache.write().await;
        if let Some(result) = cache.cache.get(cache_key) {
            cache.hit_count += 1;
            Some(result.clone())
        } else {
            cache.miss_count += 1;
            None
        }
    }
    
    async fn update_cache(&self, cache_key: String, result: &InferenceResult) {
        let mut cache = self.inference_cache.write().await;
        cache.cache.put(cache_key, result.clone());
    }
    
    fn generate_cache_key(&self, data: &SemanticData) -> String {
        use std::hash::{Hash, Hasher};
        use std::collections::hash_map::DefaultHasher;
        
        let mut hasher = DefaultHasher::new();
        data.hash(&mut hasher);
        format!("semantic_{}", hasher.finish())
    }
    
    fn calculate_confidence(&self, inferences: &[Inference]) -> f64 {
        if inferences.is_empty() {
            return 0.0;
        }
        
        let total_confidence: f64 = inferences.iter().map(|i| i.confidence).sum();
        total_confidence / inferences.len() as f64
    }
}

// 规则编译优化
pub struct CompiledRuleSet {
    compiled_rules: Vec<CompiledRule>,
    rule_index: std::collections::HashMap<String, usize>,
}

impl CompiledRuleSet {
    pub fn new() -> Self {
        CompiledRuleSet {
            compiled_rules: Vec::new(),
            rule_index: std::collections::HashMap::new(),
        }
    }
    
    pub fn compile_rules(&mut self, rules: Vec<SemanticRule>) -> Result<(), CompilationError> {
        for (index, rule) in rules.into_iter().enumerate() {
            let compiled = self.compile_single_rule(rule)?;
            self.rule_index.insert(compiled.id.clone(), index);
            self.compiled_rules.push(compiled);
        }
        Ok(())
    }
    
    fn compile_single_rule(&self, rule: SemanticRule) -> Result<CompiledRule, CompilationError> {
        // 将规则编译为优化的字节码
        let bytecode = RuleCompiler::compile(&rule)?;
        
        Ok(CompiledRule {
            id: rule.id,
            bytecode,
            metadata: rule.metadata,
        })
    }
    
    pub fn apply_to_data(&self, data: SemanticData) -> Result<InferenceResult, SemanticError> {
        let mut inferences = Vec::new();
        let start = std::time::Instant::now();
        
        for compiled_rule in &self.compiled_rules {
            let rule_inferences = compiled_rule.execute(&data)?;
            inferences.extend(rule_inferences);
        }
        
        Ok(InferenceResult {
            inferences,
            processing_time: start.elapsed(),
            confidence_score: 0.9, // 示例值
        })
    }
    
    pub fn apply_to_chunk(&self, chunk: &SemanticDataChunk) -> Result<Vec<Inference>, SemanticError> {
        let mut inferences = Vec::new();
        
        for compiled_rule in &self.compiled_rules {
            let chunk_inferences = compiled_rule.execute_on_chunk(chunk)?;
            inferences.extend(chunk_inferences);
        }
        
        Ok(inferences)
    }
}
```

## 3. 数据库性能优化

### 3.1 查询优化
```sql
-- 创建性能优化索引
CREATE INDEX CONCURRENTLY idx_device_data_timestamp 
ON device_data (timestamp DESC, device_id);

CREATE INDEX CONCURRENTLY idx_device_data_type_status 
ON device_data (data_type, status) 
WHERE status = 'active';

-- 分区表优化
CREATE TABLE device_data_partitioned (
    id BIGSERIAL,
    device_id UUID NOT NULL,
    timestamp TIMESTAMPTZ NOT NULL,
    data_type VARCHAR(50) NOT NULL,
    payload JSONB NOT NULL,
    status VARCHAR(20) DEFAULT 'active'
) PARTITION BY RANGE (timestamp);

-- 创建月度分区
CREATE TABLE device_data_y2024m01 PARTITION OF device_data_partitioned
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE device_data_y2024m02 PARTITION OF device_data_partitioned
FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- 优化查询语句
-- 使用准备语句
PREPARE device_data_query AS
SELECT device_id, timestamp, payload
FROM device_data_partitioned
WHERE timestamp >= $1 AND timestamp < $2
  AND device_id = ANY($3)
ORDER BY timestamp DESC
LIMIT $4;

-- 使用物化视图
CREATE MATERIALIZED VIEW device_summary_hourly AS
SELECT 
    device_id,
    date_trunc('hour', timestamp) as hour,
    data_type,
    count(*) as message_count,
    avg((payload->>'value')::numeric) as avg_value,
    max((payload->>'value')::numeric) as max_value,
    min((payload->>'value')::numeric) as min_value
FROM device_data_partitioned
WHERE timestamp >= now() - interval '7 days'
GROUP BY device_id, hour, data_type;

-- 创建刷新索引
CREATE UNIQUE INDEX ON device_summary_hourly (device_id, hour, data_type);

-- 自动刷新物化视图
CREATE OR REPLACE FUNCTION refresh_device_summary()
RETURNS void AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY device_summary_hourly;
END;
$$ LANGUAGE plpgsql;

-- 创建定时任务
SELECT cron.schedule('refresh-device-summary', '0 * * * *', 'SELECT refresh_device_summary();');
```

### 3.2 连接池优化
```rust
// src/performance/db_optimizer.rs
use sqlx::{Pool, Postgres, postgres::PgPoolOptions};
use std::time::Duration;

pub struct OptimizedDatabaseManager {
    read_pool: Pool<Postgres>,
    write_pool: Pool<Postgres>,
    config: DatabaseOptimizationConfig,
}

#[derive(Debug, Clone)]
pub struct DatabaseOptimizationConfig {
    pub max_connections: u32,
    pub min_connections: u32,
    pub connection_timeout: Duration,
    pub idle_timeout: Duration,
    pub max_lifetime: Duration,
    pub statement_cache_capacity: usize,
}

impl OptimizedDatabaseManager {
    pub async fn new(
        read_url: &str,
        write_url: &str,
        config: DatabaseOptimizationConfig,
    ) -> Result<Self, sqlx::Error> {
        // 读连接池配置
        let read_pool = PgPoolOptions::new()
            .max_connections(config.max_connections)
            .min_connections(config.min_connections)
            .acquire_timeout(config.connection_timeout)
            .idle_timeout(Some(config.idle_timeout))
            .max_lifetime(Some(config.max_lifetime))
            .connect(read_url)
            .await?;
        
        // 写连接池配置
        let write_pool = PgPoolOptions::new()
            .max_connections(config.max_connections / 2) // 写连接数较少
            .min_connections(config.min_connections / 2)
            .acquire_timeout(config.connection_timeout)
            .idle_timeout(Some(config.idle_timeout))
            .max_lifetime(Some(config.max_lifetime))
            .connect(write_url)
            .await?;
        
        Ok(OptimizedDatabaseManager {
            read_pool,
            write_pool,
            config,
        })
    }
    
    pub async fn execute_optimized_query<T>(&self, query: OptimizedQuery<T>) -> Result<Vec<T>, DatabaseError>
    where
        T: for<'r> sqlx::FromRow<'r, sqlx::postgres::PgRow> + Send + Unpin,
    {
        match query.query_type {
            QueryType::Read => self.execute_read_query(query).await,
            QueryType::Write => self.execute_write_query(query).await,
        }
    }
    
    async fn execute_read_query<T>(&self, query: OptimizedQuery<T>) -> Result<Vec<T>, DatabaseError>
    where
        T: for<'r> sqlx::FromRow<'r, sqlx::postgres::PgRow> + Send + Unpin,
    {
        let start = std::time::Instant::now();
        
        // 使用预编译语句
        let prepared = sqlx::query_as::<_, T>(&query.sql);
        
        let mut query_builder = prepared;
        for param in query.parameters {
            query_builder = query_builder.bind(param);
        }
        
        let result = query_builder
            .fetch_all(&self.read_pool)
            .await?;
        
        let duration = start.elapsed();
        
        // 记录查询性能
        self.record_query_performance(&query.name, duration, result.len()).await;
        
        Ok(result)
    }
    
    async fn execute_write_query<T>(&self, query: OptimizedQuery<T>) -> Result<Vec<T>, DatabaseError>
    where
        T: for<'r> sqlx::FromRow<'r, sqlx::postgres::PgRow> + Send + Unpin,
    {
        let start = std::time::Instant::now();
        
        // 使用事务确保一致性
        let mut tx = self.write_pool.begin().await?;
        
        let prepared = sqlx::query_as::<_, T>(&query.sql);
        let mut query_builder = prepared;
        
        for param in query.parameters {
            query_builder = query_builder.bind(param);
        }
        
        let result = query_builder
            .fetch_all(&mut *tx)
            .await?;
        
        tx.commit().await?;
        
        let duration = start.elapsed();
        self.record_query_performance(&query.name, duration, result.len()).await;
        
        Ok(result)
    }
    
    pub async fn execute_batch_insert<T>(&self, data: Vec<T>) -> Result<u64, DatabaseError>
    where
        T: BatchInsertable,
    {
        if data.is_empty() {
            return Ok(0);
        }
        
        let batch_size = 1000;
        let mut total_inserted = 0u64;
        
        for chunk in data.chunks(batch_size) {
            let inserted = self.insert_batch_chunk(chunk).await?;
            total_inserted += inserted;
        }
        
        Ok(total_inserted)
    }
    
    async fn insert_batch_chunk<T>(&self, chunk: &[T]) -> Result<u64, DatabaseError>
    where
        T: BatchInsertable,
    {
        let mut tx = self.write_pool.begin().await?;
        
        // 构建批量插入SQL
        let sql = T::build_batch_insert_sql(chunk.len());
        let mut query = sqlx::query(&sql);
        
        for item in chunk {
            query = item.bind_parameters(query);
        }
        
        let result = query.execute(&mut *tx).await?;
        tx.commit().await?;
        
        Ok(result.rows_affected())
    }
    
    async fn record_query_performance(&self, query_name: &str, duration: Duration, row_count: usize) {
        // 记录查询性能指标
        println!("查询 {} 执行时间: {:?}, 返回行数: {}", query_name, duration, row_count);
    }
}

pub trait BatchInsertable {
    fn build_batch_insert_sql(count: usize) -> String;
    fn bind_parameters(self, query: sqlx::query::Query<'_, sqlx::Postgres, sqlx::postgres::PgArguments>) -> sqlx::query::Query<'_, sqlx::Postgres, sqlx::postgres::PgArguments>;
}
```

## 4. 缓存优化策略

### 4.1 多级缓存架构
```rust
// src/performance/cache_optimizer.rs
use std::sync::Arc;
use tokio::sync::RwLock;

pub struct MultiLevelCache {
    // L1缓存：内存中的LRU缓存
    l1_cache: Arc<RwLock<lru::LruCache<String, CacheItem>>>,
    
    // L2缓存：Redis分布式缓存
    l2_cache: Arc<redis::Client>,
    
    // L3缓存：数据库查询结果缓存
    l3_cache: Arc<DatabaseCache>,
    
    config: CacheOptimizationConfig,
}

#[derive(Debug, Clone)]
pub struct CacheOptimizationConfig {
    pub l1_cache_size: usize,
    pub l1_ttl: Duration,
    pub l2_ttl: Duration,
    pub l3_ttl: Duration,
    pub compression_threshold: usize,
    pub prefetch_enabled: bool,
}

#[derive(Debug, Clone)]
pub struct CacheItem {
    pub data: Vec<u8>,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub access_count: u32,
    pub size: usize,
}

impl MultiLevelCache {
    pub fn new(redis_client: redis::Client, db_cache: DatabaseCache, config: CacheOptimizationConfig) -> Self {
        MultiLevelCache {
            l1_cache: Arc::new(RwLock::new(lru::LruCache::new(config.l1_cache_size))),
            l2_cache: Arc::new(redis_client),
            l3_cache: Arc::new(db_cache),
            config,
        }
    }
    
    pub async fn get<T>(&self, key: &str) -> Result<Option<T>, CacheError>
    where
        T: serde::de::DeserializeOwned + Clone,
    {
        // L1缓存查找
        if let Some(item) = self.get_from_l1(key).await {
            self.record_cache_hit("L1").await;
            return Ok(Some(self.deserialize_item(&item)?));
        }
        
        // L2缓存查找
        if let Some(item) = self.get_from_l2(key).await? {
            self.record_cache_hit("L2").await;
            // 回填L1缓存
            self.put_to_l1(key, &item).await;
            return Ok(Some(self.deserialize_item(&item)?));
        }
        
        // L3缓存查找
        if let Some(item) = self.get_from_l3(key).await? {
            self.record_cache_hit("L3").await;
            // 回填L2和L1缓存
            self.put_to_l2(key, &item).await?;
            self.put_to_l1(key, &item).await;
            return Ok(Some(self.deserialize_item(&item)?));
        }
        
        self.record_cache_miss().await;
        Ok(None)
    }
    
    pub async fn put<T>(&self, key: &str, value: &T) -> Result<(), CacheError>
    where
        T: serde::Serialize,
    {
        let item = self.serialize_item(value)?;
        
        // 写入所有缓存层级
        self.put_to_l1(key, &item).await;
        self.put_to_l2(key, &item).await?;
        self.put_to_l3(key, &item).await?;
        
        // 预取相关数据
        if self.config.prefetch_enabled {
            self.prefetch_related_data(key).await?;
        }
        
        Ok(())
    }
    
    async fn get_from_l1(&self, key: &str) -> Option<CacheItem> {
        let mut cache = self.l1_cache.write().await;
        cache.get_mut(key).map(|item| {
            item.access_count += 1;
            item.clone()
        })
    }
    
    async fn put_to_l1(&self, key: &str, item: &CacheItem) {
        let mut cache = self.l1_cache.write().await;
        cache.put(key.to_string(), item.clone());
    }
    
    async fn get_from_l2(&self, key: &str) -> Result<Option<CacheItem>, CacheError> {
        let mut conn = self.l2_cache.get_async_connection().await?;
        
        let data: Option<Vec<u8>> = redis::cmd("GET")
            .arg(key)
            .query_async(&mut conn)
            .await?;
        
        if let Some(data) = data {
            let item = self.decompress_if_needed(data)?;
            Ok(Some(CacheItem {
                data: item,
                timestamp: chrono::Utc::now(),
                access_count: 1,
                size: item.len(),
            }))
        } else {
            Ok(None)
        }
    }
    
    async fn put_to_l2(&self, key: &str, item: &CacheItem) -> Result<(), CacheError> {
        let mut conn = self.l2_cache.get_async_connection().await?;
        
        let data = self.compress_if_needed(&item.data)?;
        
        redis::cmd("SETEX")
            .arg(key)
            .arg(self.config.l2_ttl.as_secs())
            .arg(data)
            .query_async(&mut conn)
            .await?;
        
        Ok(())
    }
    
    async fn get_from_l3(&self, key: &str) -> Result<Option<CacheItem>, CacheError> {
        self.l3_cache.get(key).await
    }
    
    async fn put_to_l3(&self, key: &str, item: &CacheItem) -> Result<(), CacheError> {
        self.l3_cache.put(key, item).await
    }
    
    fn serialize_item<T>(&self, value: &T) -> Result<CacheItem, CacheError>
    where
        T: serde::Serialize,
    {
        let data = bincode::serialize(value)?;
        Ok(CacheItem {
            size: data.len(),
            data,
            timestamp: chrono::Utc::now(),
            access_count: 0,
        })
    }
    
    fn deserialize_item<T>(&self, item: &CacheItem) -> Result<T, CacheError>
    where
        T: serde::de::DeserializeOwned,
    {
        Ok(bincode::deserialize(&item.data)?)
    }
    
    fn compress_if_needed(&self, data: &[u8]) -> Result<Vec<u8>, CacheError> {
        if data.len() > self.config.compression_threshold {
            use flate2::write::GzEncoder;
            use flate2::Compression;
            use std::io::Write;
            
            let mut encoder = GzEncoder::new(Vec::new(), Compression::default());
            encoder.write_all(data)?;
            Ok(encoder.finish()?)
        } else {
            Ok(data.to_vec())
        }
    }
    
    fn decompress_if_needed(&self, data: Vec<u8>) -> Result<Vec<u8>, CacheError> {
        // 检查是否为压缩数据（简单的魔数检查）
        if data.len() > 2 && data[0] == 0x1f && data[1] == 0x8b {
            use flate2::read::GzDecoder;
            use std::io::Read;
            
            let mut decoder = GzDecoder::new(&data[..]);
            let mut decompressed = Vec::new();
            decoder.read_to_end(&mut decompressed)?;
            Ok(decompressed)
        } else {
            Ok(data)
        }
    }
    
    async fn prefetch_related_data(&self, key: &str) -> Result<(), CacheError> {
        // 基于访问模式预取相关数据
        let related_keys = self.predict_related_keys(key).await;
        
        for related_key in related_keys {
            if self.get_from_l1(&related_key).await.is_none() {
                // 异步预取
                let cache = self.clone();
                let key = related_key.clone();
                tokio::spawn(async move {
                    let _ = cache.get::<serde_json::Value>(&key).await;
                });
            }
        }
        
        Ok(())
    }
    
    async fn predict_related_keys(&self, _key: &str) -> Vec<String> {
        // 实现基于机器学习的预测算法
        // 这里是简化版本
        vec![]
    }
    
    async fn record_cache_hit(&self, level: &str) {
        println!("缓存命中: {}", level);
    }
    
    async fn record_cache_miss(&self) {
        println!("缓存未命中");
    }
}
```

## 5. 性能测试和基准测试

### 5.1 压力测试配置
```javascript
// tests/performance/load-test.js
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate } from 'k6/metrics';

// 自定义指标
const errorRate = new Rate('errors');

export const options = {
  stages: [
    { duration: '2m', target: 100 }, // 启动阶段
    { duration: '5m', target: 100 }, // 稳定阶段
    { duration: '2m', target: 200 }, // 增压阶段
    { duration: '5m', target: 200 }, // 高负载稳定
    { duration: '2m', target: 300 }, // 峰值测试
    { duration: '3m', target: 300 }, // 峰值稳定
    { duration: '2m', target: 0 },   // 降压阶段
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'], // 95%请求在500ms内
    http_req_failed: ['rate<0.1'],    // 错误率小于10%
    errors: ['rate<0.1'],
  },
};

export default function () {
  // 模拟设备数据上报
  const deviceData = {
    device_id: `device_${Math.floor(Math.random() * 1000)}`,
    timestamp: new Date().toISOString(),
    data_type: 'sensor_reading',
    payload: {
      temperature: 20 + Math.random() * 15,
      humidity: 40 + Math.random() * 30,
      pressure: 1000 + Math.random() * 50,
    },
  };

  const response = http.post('http://iot-gateway/api/v1/data', JSON.stringify(deviceData), {
    headers: { 'Content-Type': 'application/json' },
  });

  const success = check(response, {
    'status is 200': (r) => r.status === 200,
    'response time < 500ms': (r) => r.timings.duration < 500,
  });

  errorRate.add(!success);

  sleep(1);
}

// 语义查询测试
export function semanticQueryTest() {
  const query = {
    query_type: 'semantic_inference',
    entities: [
      { type: 'Device', id: `device_${Math.floor(Math.random() * 100)}` },
      { type: 'Sensor', id: `sensor_${Math.floor(Math.random() * 50)}` },
    ],
    context: {
      location: 'building_a',
      time_range: {
        start: new Date(Date.now() - 3600000).toISOString(),
        end: new Date().toISOString(),
      },
    },
  };

  const response = http.post('http://semantic-engine/api/v1/infer', JSON.stringify(query), {
    headers: { 'Content-Type': 'application/json' },
  });

  check(response, {
    'semantic query status is 200': (r) => r.status === 200,
    'semantic response time < 2s': (r) => r.timings.duration < 2000,
  });
}
```

### 5.2 性能监控脚本
```bash
#!/bin/bash
# scripts/performance-monitor.sh

set -e

NAMESPACE="iot-system"
DURATION="300s"  # 5分钟监控

echo "开始性能监控..."

# 创建性能监控目录
mkdir -p performance-reports/$(date +%Y%m%d_%H%M%S)
REPORT_DIR="performance-reports/$(date +%Y%m%d_%H%M%S)"

# 监控CPU和内存使用
echo "监控系统资源使用..."
kubectl top nodes > "$REPORT_DIR/node-resources.txt"
kubectl top pods -n $NAMESPACE > "$REPORT_DIR/pod-resources.txt"

# 收集应用指标
echo "收集应用性能指标..."
curl -s "http://prometheus:9090/api/v1/query_range?query=rate(iot_message_total[5m])&start=$(date -d '5 minutes ago' -u +%s)&end=$(date -u +%s)&step=30s" > "$REPORT_DIR/message-rate.json"

curl -s "http://prometheus:9090/api/v1/query_range?query=iot_response_time_seconds&start=$(date -d '5 minutes ago' -u +%s)&end=$(date -u +%s)&step=30s" > "$REPORT_DIR/response-time.json"

# 数据库性能监控
echo "监控数据库性能..."
kubectl exec -n $NAMESPACE postgres-cluster-1 -- psql -U iot_user -d iot_db -c "
SELECT 
    schemaname,
    tablename,
    attname,
    n_distinct,
    correlation
FROM pg_stats 
WHERE schemaname = 'public' 
ORDER BY tablename, attname;
" > "$REPORT_DIR/db-stats.txt"

# 网络性能监控
echo "监控网络性能..."
kubectl exec -n $NAMESPACE deployment/iot-gateway -- ss -tuln > "$REPORT_DIR/network-connections.txt"

# 生成性能报告
echo "生成性能报告..."
cat > "$REPORT_DIR/performance-summary.md" << EOF
# IoT系统性能监控报告

## 监控时间
- 开始时间: $(date -d '5 minutes ago')
- 结束时间: $(date)
- 监控时长: 5分钟

## 系统资源使用

### 节点资源
\`\`\`
$(cat "$REPORT_DIR/node-resources.txt")
\`\`\`

### Pod资源
\`\`\`
$(cat "$REPORT_DIR/pod-resources.txt")
\`\`\`

## 应用性能指标

### 消息处理速率
- 数据文件: message-rate.json

### 响应时间分布
- 数据文件: response-time.json

## 数据库性能
\`\`\`
$(head -20 "$REPORT_DIR/db-stats.txt")
\`\`\`

## 网络连接状态
\`\`\`
$(head -20 "$REPORT_DIR/network-connections.txt")
\`\`\`

## 优化建议

1. 根据CPU使用率调整Pod副本数
2. 监控内存泄漏情况
3. 优化数据库查询性能
4. 检查网络连接池配置

EOF

echo "性能监控完成，报告保存在: $REPORT_DIR"
```

这个性能优化与调优实现提供了全面的性能分析和优化策略，包括应用层优化、数据库优化、缓存策略、性能测试等，确保IoT系统在高负载下的稳定运行。 